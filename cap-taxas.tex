%% ------------------------------------------------------------------------- %%
\chapter{Probabilidades de transição}
\label{cap:taxas}

Nesse capítulo nos concentraremos em problemas ligados às
probabilidades de transição do Processo K. Primeiramente trataremos de
problemas relacionados à continuidade das mesmas, para depois
calcularmos suas derivadas na origem. Isso nos permitirá calcular a
medida invariante do processo.  Por fim exibiremos o gerador
infinitesimal no caso homogêneo e fecharemos o capítulo estudando a
dimensão de Hausdorff do tempo em que o Processo K passa no infinito.


%% ------------------------------------------------------------------------- %%

\section{Continuidade das probabilidas de transição}
\label{sec:continuidade}

Para deixar a nossa notação menos carregada, vamos introduzir a
seguinte definição para $x, y \in \Nzb$ e $t > 0$:
\begin{displaymath}
  p_{xy} (t) := P(X^x(t) = y).
\end{displaymath}
Denotaremos ainda por $P(t)$ a ``matriz'' cujas entradas sejam $p_{xy}(t)$.

Nessa seção vamos estudar a continuidade dessas funções. Começaremos
com uma caracterização básica das mesmas.

\begin{proposicao}
  \label{prop:transicao-mensuravel}
  Para todo $x, y, z \in \Nzb$ e $t, s > 0$ vale que:
  \begin{enumerate}
  \item $p_{xy}(t) \geq 0$
  \item $\displaystyle \sum_{y \in \Nzb} p_{xy}(t) = 1$
  \item $\displaystyle p_{xy}(t+s) = \sum_{z\in\Nzb}p_{xz}(t)p_{zy}(s)$
  \item $p_{xy}(\bullet)$ é uma função Lebesgue mensurável.
  \end{enumerate}
\end{proposicao}

\begin{proof}
  Os três primeiros itens são consequência direta da definição de
  $p_{x y}$ e da propriedade de Markov do processo. Para o quarto,
  consideremos $X^{x, (n)}$ como na Seção \ref{sec:aproximacoes} e
  definiremos $p^n_{x y}(t) := P(X^{x, (n)}(t) = y)$. Observemos que
  $p^n_{xy}$ é Lebesgue mensurável porque $X^{x, (n)}$ é um processo
  markoviano de saltos.

  Fixado $x \in \Nzb$ e $y \in \Nz$, notemos que $f_y(z) := \ind\{ z =
  y \}$ é uma função contínua e todo $t > 0$ fixado é \qc um ponto de
  continuidade de $X^x$. Assim podemos usar o Teorema
  \ref{teo:convergencia}, junto com o Teorema da Convergência
  Dominada, para concluir que $p_{x y}^n(t) \xrightarrow{n\to\infty}
  p_{x y}(t)$.

  Assim, como limite de funções mensuráveis também é uma função
  mensurável, teremos que para todo $x \in \Nzb$ e $y \in \Nz$, $p_{x
    y}$ é uma função mensurável.

  Por fim, temos que $p_{x \infty} (t) = 1 - \sum_{y \in \Nz} p_{x y}
  (t)$. Assim $p_{x \infty}$ é limite de funções mensuráveis, e
  portanto também mensurável.
\end{proof}

\begin{proposicao}
  \label{prop:transicao-continua}
  Para todo $x, y \in \Nzb$, $p_{x y}(\bullet)$ é uma função contínua
  em $(0, \infty)$.
\end{proposicao} 

\begin{proof}
  Esse é um dos itens do Teorema 1 da primeira seção da parte II
  (pg. 120) de \cite{chung:67}.
\end{proof}

\begin{proposicao}
  \label{prop:continuidade-facil}
  Para $x \in \Nz$,
  \begin{equation}
    \lim_{t \searrow 0}p_{xx}(t) = 1.   
  \end{equation}
\end{proposicao}
\begin{proof}
  Como $x \in \Nz$, temos que:
  \begin{displaymath}
    p_{xx}(t) \geq P( \gamma_x T^x_0 > t) = e^{-\frac{t}{\gamma_x}}
    \xrightarrow{t \searrow 0} 1.
    \qedhere
  \end{displaymath}
\end{proof}

% \begin{lema}
%   \label{lema:deriv_gamma}
%   Vale que $ \frac{\Gamma^{\infty}(t)}{t} \xrightarrow{t \searrow 0}
%   c$ em probabilidade.
% \end{lema}
% \begin{proof}
%   Fixando um $t > 0$, teremos que:
%   \begin{equation}
%     \label{eq:gamma_tt}
%     \frac{\Gamma^{\infty}(t)}{t} = 
%     c + \frac{1}{t} \sum_{x \in \Nz} \sum_{i = 1}^{N^x(t)}
%     \gamma_x T^x_i
%   \end{equation}

%   Agora podemos calcular a transformada de Laplace da segunda parte
%   dessa expressão. Fixado um $u > 0$, teremos que:
%   \begin{equation}
%     \label{eq:lap_vaizero}
%     \E \left[ \exp\left\{
%       -u \frac{1}{t} \sum_{x \in \Nz} \sum_{i=1}^{N^x(t)} \gamma_x T^x_i
%     \right\} \right] =
%     \exp \left\{
%       - u \sum_{x \in \Nz}  \frac{\lambda_x \gamma_x}{1 +
%         \frac{u \gamma_x}{t}}
%     \right\} %\xrightarrow{t \searrow 0} 1.
%   \end{equation}

%   Essa função é monótona em $t$ e cada termo da soma
%   vai a zero quando $t \searrow 0$.  Assim o teorema da convergência
%   monótona nos diz que \eqref{eq:lap_vaizero} converge à $1$ quanto $t
%   \searrow 0$, para qualquer $u \geq 0$ fixado.

%   Como a função constante igual a $1$ é a transformada de Laplace de
%   uma variável aleatória que vale $0$ sempre, temos que a expressão
%   \eqref{eq:gamma_tt} converge em probabilidade para $c$ quando $t
%   \searrow 0$.
% \end{proof}

% \begin{lema}
%   \label{lema:deriv_inv_gamma}
%   Seja $\Xi$  a função inversa de $\Gamma^\infty$.
%   Vale que $ \frac{\Xi(t)}{t} \xrightarrow{t \searrow 0}
%   \frac{1}{c}$ em probabilidade.
% \end{lema}
% \begin{proof}

%   Fixe $t > 0$ e $\epsilon > 0$. Com probabilidade $1$, temos que $t$
%   é um ponto de continuidade de $\Gamma^\infty$. Assim existe $s > 0$
%   tal que $\Gamma$

% \end{proof}

\begin{proposicao}
  \label{prop:continuidade}
  Caso $c > 0$, vale que:
  \begin{equation}
    \label{eq:continuidade-infinito}
    \lim_{t \searrow 0}p_{\infty \infty}(t) = 1.    
  \end{equation}
\end{proposicao}

\begin{proof}
  Consideremos a função:
  \begin{displaymath}
    \theta(t) = \int_0^t \ind \{ X^\infty (s) = \infty \} ds.
  \end{displaymath}.
  
  Pela construção do processo, temos que $\theta(t) \geq c
  \Xi(t)$, onde $\Xi(t)$ é a inversa de $\Gamma^\infty(t)$.

  Como $\Gamma^\infty(t)$ é \qc finito para todo $t$, então $\Xi(t) >
  0$ \qc. Portanto $\E [\theta(t)] > 0$. Usando o teorema de Fubini
  obtemos que:
  \begin{displaymath}
    \E[\theta(t)] =  \int_0^t p_{\infty \infty} (s) ds
  \end{displaymath}

  Assim obtemos que o conjunto dos $t > 0$ onde $p_{\infty \infty}(t)
  > 0$ tem medida de Lebesgue positiva. Em particular existe um $s >
  0$ tal que $p_{\infty \infty}(s) > 0$. Fixemos tal $s$.

  Tomemos uma sequência arbitrária $(t_n)$, onde $t_n \searrow 0$ e
  $p_{\infty \infty} (t_n)$ convirja para um número real $u$ quanto $n
  \to \infty$.  Se mostrarmos que $u = 1$, seguirá que o limite dado
  em \eqref{eq:continuidade-infinito} existe e vale $1$.


  Usando as proposições \ref{prop:transicao-mensuravel} e
  \ref{prop:transicao-continua}, obteremos que:
  \begin{displaymath}
    p_{\infty\infty}(s) = \lim_{n \to \infty} p_{\infty \infty}(s+t_n) = \lim_{n \to
      \infty} \sum_{x \in \Nzb} p_{\infty x}(s) p_{x \infty} (t_n)
  \end{displaymath}

  Pela Proposição \ref{prop:continuidade-facil}, se $x \in \Nz$ então
  $p_{x \infty}(t_n) \to 0$ quando $n \to \infty$. Cada termo
  dessa soma é menor ou igual à $p_{\infty x} (s)$, que é somável em
  $x$. Assim usando o Teorema da Convergência Dominada, concluímos
  que:
  \begin{displaymath}
    p_{\infty\infty}(s) = p_{\infty \infty}(s)u.
  \end{displaymath}

  Já que $p_{\infty \infty}(s) > 0$, concluímos que $u = 1$.
\end{proof}

\begin{proposicao}
  \label{prop:naocontinuidade}
  Caso $c = 0$, para todo $x \in \Nzb$ e $t > 0$, temos que $p_{x
    \infty} (t) = 0$.
\end{proposicao}
\begin{proof}
  Essa demonstração é uma adaptação direta do Lema \emph{3.15} de
  \cite{fontes:08}.

  Para $m \in \Nzb$  definimos:
  \begin{align*}
    \theta_m(t) := \int_0^t \ind \left\{ X^\infty_0(s) \geq m \right\} ds
  \end{align*}

  Estamos interessados em estimar $\theta(t) :=
  \theta_\infty(t)$. Para isso notemos que $\theta_m(t)$ é decrescente em
  $m$. Dessa forma, se $\Xi$ for a função inversa de $\Gamma^\infty$, teremos
  que:
  \begin{align*}
    \theta_\infty(t) \leq \theta_m(t) = \sum_{x \geq m}
    \sum_{i=1}^{\Xi(t)} \gamma_x T^x_i.
  \end{align*}


  Como $\theta_m(t) \leq t$, essa série em $x$ converge. Assim temos
  que $\lim_{m\to\infty} \theta_m(t) = 0$ \qc e, portanto, $\theta(t) =
  0$ \qc. Dessa forma também vale que $\E[\theta(t)] = 0$. Agora
  usando o teorema de Fubini, teremos:
  \begin{align*}
    0 &= \E\left[ \int_0^t \ind \left\{ X^\infty_0(s) = \infty
      \right\} ds \right]\\
    &= \int_0^t P \left\{ X^\infty_0(s) = \infty
    \right\} ds\\
    &= \int_0^t p_{\infty \infty} (s) ds
  \end{align*}

  Dessa forma concluímos que $p_{\infty \infty} (t) = 0$ para Lebesgue
  quase todo $t$. A continuidade de $p_{\infty \infty}$ nos diz que
  $p_{\infty \infty}(t) = 0$ para todo $t > 0$.

  Agora tomemos um $x \in \Nz$ arbitrário. Condicionando no valor de
  $\gamma_x T_0$ e usando a Proposição \ref{prop:reinicia-infinito}
  teremos que:
  \begin{align*}
    p_{x \infty} (t) = \int_0^t p_{\infty \infty} (t-s)
    \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds.\\
  \end{align*}

  Portanto para todo $t > 0$ vale que $p_{x \infty} (t) = 0$.

  % Assim, usando a propriedade de Markov do processo, teremos que para
  % todo $t> 0$ e $s \in (0, t)$:
  % \begin{align*}
  %   p_{\infty \infty} (t) &= \sum_{x \in \Nzb} p_{\infty x}(s) p_{x
  %     \infty} (t-s)\\
  %   &= p_{\infty \infty} (s) p_{\infty \infty} (t-s).
  % \end{align*}

  % Agora suponha por absurdo que exista um $t > 0$ tal que
  % $p_{\infty \infty} (t) > 0$. Então teríamos que $p_{\infty
  %   \infty}(s) > 0$ para todo $s \in (0, t)$, conjunto esse que tem
  % medida de Lebesgue positiva.
\end{proof}


%% ------------------------------------------------------------------------- %%

\section{Matriz Q}
\label{sec:matrizq}

Nessa seção vamos calcular a matriz de taxas de transição do 
Processo K. Vamos definí-la pelo limite:
\begin{displaymath}
  Q = \lim_{t \searrow 0} \frac{P(t) - I}{t}, 
\end{displaymath}
onde $I$ é a matriz identidade.

Vamos mostrar que, para o caso $c > 0$, essa matriz vale:
\begin{displaymath}
  Q = \left(
    \begin{array}{ccccc}
      -\frac{1}{\gamma_1} & 0 & 0 & \cdots & \frac{1}{\gamma_1}\\
      0 & -\frac{1}{\gamma_2} & 0 & \cdots & \frac{1}{\gamma_2}\\
      0 & 0 & -\frac{1}{\gamma_3} & \cdots & \frac{1}{\gamma_3}\\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      \frac{\lambda_1}{c} & \frac{\lambda_2}{c} &
      \frac{\lambda_3}{c} & \cdots & -\infty\\
    \end{array}
  \right).
\end{displaymath}

Enquanto que no caso $c=0$, teremos:
\begin{displaymath}
  Q = \left(
    \begin{array}{ccccc}
      -\frac{1}{\gamma_1} & 0 & 0 & \cdots & 0\\
      0 & -\frac{1}{\gamma_2} & 0 & \cdots & 0\\
      0 & 0 & -\frac{1}{\gamma_3} & \cdots & 0\\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      \infty & \infty & \infty & \cdots & -\infty\\
    \end{array}
  \right).
\end{displaymath}

\begin{proposicao}
  \label{prop:taxa-x-y}
  Sejam $x, y \in \Nz$, $x \neq y$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{xy}(t)}{t} = 0.
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align*}
    \frac{p_{x y} (t)}{t}
    &= \frac{1}{t}\int_{0}^{t} P( X^x(t) = y |
    \gamma_x T_0 = s) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{1}{t} \int_{0}^{t} P( X^\infty(t-s) = y ) \frac{1}{\gamma_x}
    e^{-\frac{s}{\gamma_x}} ds \\
    &\leq \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(t-s) ds \\
    &= \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(s) ds.
  \end{align*}

  Como $p_{\infty y} (t) \to 0$ quando $t \searrow 0$, então a
  quantidade acima também converge à zero.
\end{proof}

\begin{proposicao}
  \label{prop:taxa-x-x}
  Para $x \in \Nz$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{xx}(t) - 1}{t} = -\frac{1}{\gamma_x}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align*}
    p_{xx} (t)
    &= P( \gamma_x T_0^x > t) + 
    \int_{0}^{t} P( X^x(t) = y |
    \gamma_x T_0 = s) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &= e^{-\frac{t}{\gamma_x}} + 
    \int_{0}^{t} p_{\infty y}(t-s) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
  \end{align*}
  Fazendo contas análogas às da Proposição \ref{prop:taxa-x-y},
  podemos mostrar que o segundo termo dessa soma, dividido por $t$,
  vai para zero quando $t \searrow 0$. Tratando o primeiro termo
  agora, temos que:
  \begin{displaymath}
    \frac{e^{-\frac{t}{\gamma_x}} - 1}{t} \xrightarrow{t \searrow 0}
    -\frac{1}{\gamma_x}.
  \end{displaymath}

  Dessa forma:
  \begin{displaymath}
     \lim_{t \searrow 0} \frac{p_{xx} (t) - 1}{t} = -\frac{1}{\gamma_x}.
  \end{displaymath}
  
\end{proof}

\begin{proposicao}
  Para $x \in \Nz$, vale que:
  \label{prop:taxa-inf-x}
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{\infty x}(t)}{t} = \begin{cases}
      \frac{\lambda_x}{c} & \textrm{ se } c > 0 \\
      \infty & \textrm{ se } c = 0 \\
    \end{cases}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align}
    \frac{p_{\infty x}(t)}{t} &= \frac{1}{t} P \left( \bigcup_{i =
        1}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right) \notag \\
    &= \frac{P \left( \Gamma^\infty (\sigma^x_1 -) \leq t <
      \Gamma^\infty(\sigma^x_1) \right)}{t} +
    \frac{1}{t} P \left( \bigcup_{i =
        2}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right) \notag \notag \\
    \label{erros_taxa_inf}
    &= \frac{P \left( \Gamma^\infty (\sigma^x_1 -) \leq t \right)}{t} -
    \frac{P \left( \Gamma^\infty (\sigma^x_1) \leq t \right)}{t} +
    \frac{1}{t} P \left( \bigcup_{i =
        2}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right)
  \end{align}

  Agora vamos mostrar que o segundo termo dessa soma vai a zero quando
  $t \searrow 0$.
  \begin{align*}
    \frac{P (\Gamma^\infty (\sigma^x_1) \leq t)}{t}
    &= \frac{1}{t} P \left(
      \gamma_x T^x_1 + 
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i +
      c\sigma^x_1
      \leq t
    \right) \\
    &\leq \frac{1}{t} P \left(
      \gamma_x T^x_1 + 
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t
    \right)\\
    &= \frac{1}{t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t - s
      \middle\vert \gamma_x T^x_1 = s
    \right) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &\leq \frac{1}{\gamma_x t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t - s
    \right) ds\\
    &= \frac{1}{\gamma_x t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq s
    \right) ds\\
    % &\xrightarrow{t\searrow0} \frac{1}{\gamma_x} P \left(
    %   \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
    %   = 0 \right) = 0
  \end{align*}

  Quando $s \to 0$, a probabilidade dentro da integral converge para
  zero pela Proposição \ref{prop:gamma-dist-continua}. Portanto o
  segundo termo de \eqref{erros_taxa_inf} converge à zero.

  Agora notemos que, como $\Gamma^\infty$ é não decrescente e $\sigma^x_1
  < \sigma^x_i$ para $i \geq 2$, então $\Gamma^\infty(\sigma^x_1) \leq
  \Gamma^\infty(\sigma^x_i -)$ e, assim, o evento no terceiro termo de
  \eqref{erros_taxa_inf} está contido no evento do segundo
  termo. Como mostramos que o segundo termo vai a zero, teremos
  que o terceiro também irá.

  Resta calcular o limite do primeiro termo. Ao fazer isso
  estaremos também calculando o limite desejado.

  Faremos isso usando o Teorema Tauberiano, que relaciona o
  comportamento a função de distribuição de uma variável aleatória
  positiva perto do zero com o comportamento de sua transformada de
  Laplace no infinito.  Seguiremos o enunciado do
  Teorema\emph{XIII.5.1} de \cite{fellerv2}.

  O primeiro passo é calcular a transformada da Laplace de
  $\Gamma^\infty(\sigma^x_1-)$. Para um $u \geq 0$ ela vale:
  \begin{align*}
    \phi (u) := \E \left[ e^{-u \Gamma^\infty (\sigma^x_1-)}  \right] =
    \lambda_x \left( \lambda_x + uc + u \sum_{y \neq x}
      \frac{\lambda_x \gamma_x}{1 + u\gamma_x}  \right)^{-1}.
  \end{align*}

  Por enquanto vamos nos concentrar no caso $c > 0$. Para $u, v > 0$
  teremos que:
  \begin{align*}
    \frac{\phi(uv)}{\phi (u)} &= \frac{\lambda_x + uc + \sum_{y \neq
        x} \frac{u \lambda_x\gamma_x}{1 + u \gamma_x}} {\lambda_x + u
      v c + \sum_{y \neq x} \frac{u v
        \lambda_x\gamma_x}{1 + u v \gamma_x}} \\
    &= \frac{\frac{\lambda_x}{u} + c + \sum_{y \neq x}
      \frac{\lambda_x\gamma_x}{1 + u \gamma_x}} {\frac{\lambda_x}{u} +
      v c + \sum_{y \neq x} \frac{v
        \lambda_x\gamma_x}{1 + u v \gamma_x}}. \\
  \end{align*}

  Notemos que quando $u$ vai ao infinito, os termos de cada uma das
  somas vai a zero monotonamente com $u$; assim usando o Teorema da
  Convergência Monótona, teremos que:
  \begin{align*}
      \lim_{u \to \infty} \frac{\phi(uv)}{\phi (u)} &= \frac{1}{v}.
  \end{align*}

  Com isso verificamos a condição do Teorema Tauberiano, de onde
  obtemos que:
  \begin{align*}
    \lim_{t \searrow 0} \frac{P( \Gamma^\infty(\sigma^x_1-) \leq
      t)}{\phi(\frac{1}{t})} = 1
  \end{align*}

  Se mostrarmos que $\frac{\phi(\frac{1}{t})}{t}$ converge para
  $\frac{\lambda_x}{c}$ quanto $t \searrow 0$, teremos mostrado o
  nosso resultado.

  \begin{align*}
    \frac{\phi(\frac{1}{t})}{t} &= \frac{1}{t} \lambda_x \left(
      \lambda_x + \frac{c}{t} + \sum_{y \neq x} \frac{1}{t}
      \frac{\lambda_x \gamma_x}{1 + \frac{\gamma_x}{t}} \right)^{-1} \\
    &= \lambda_x \left( t\lambda_x + c + \sum_{y \neq x}
      \frac{\lambda_x \gamma_x}{1 + \frac{\gamma_x}{t}} \right)^{-1}.
  \end{align*}

  Novamente cada termo da soma vai a zero monotonamente quanto $t
  \searrow 0$, assim pelo Teorema da Convergência Monótona, teremos que
  $\lim_{t \searrow 0} \frac{\phi(\frac{1}{t})}{t} =
  \frac{\lambda_x}{c}$.

  Para tratar o caso $c = 0$ observemos que nossa construção do
  Processo K permite que acoplemos várias versões do processo, com $c$
  diferentes em um mesmo espaço de probabilidade. Basta usar os mesmos
  processos de Poisson e variáveis exponenciais para todos eles. Assim
  vamos colocar um índice $c$ em $\Gamma^y_c$ para denotar com qual
  valor de $c$ estamos trabalhando.

  Notemos que, para todo $y \in \Nzb$, $\Gamma^y_c$ é crescente com
  $c$ e, portanto, $P ( \Gamma^\infty_c(\sigma^x_1-) \leq t)$ é
  monótona em $c$.  Dessa forma teremos que, para todo $c > 0$:
  \begin{align*}
    \liminf_{t \searrow 0} \frac{P ( \Gamma^\infty_0(\sigma^x_1-) \leq
      t)}{t} &\geq \liminf_{t \searrow 0} \frac{P (
      \Gamma^\infty_c(\sigma^x_1-) \leq t)}{t}
    = \frac{\lambda_x}{c}.
  \end{align*}

  Tomando $c > 0$ cada vez menores, concluiremos que $\frac{P (
    \Gamma^\infty_0(\sigma^x_1-))}{t} \xrightarrow{t \searrow 0}
  \infty$.
\end{proof}

\begin{proposicao}
  \label{prop:taxa-inf-inf}
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{\infty \infty}(t) - 1}{t} = -\infty.
  \end{displaymath}
\end{proposicao}
\begin{proof}
  O caso $c = 0$ é trivial em vista da Proposição
  \ref{prop:naocontinuidade}. Assim vamos supor que $c > 0$.
  Para todo $n \in \Nz$, teremos que:

  \begin{equation}
    \label{eq:taxa-inf-inf}
    \frac{p_{\infty \infty} (t) - 1}{t} = - \sum_{x \in \Nz}
    \frac{p_{\infty x} (t)}{t} \leq - \sum_{x = 1}^n \frac{p_{\infty
        x} (t)}{t}.
  \end{equation}
  
  Usando a Proposição \ref{prop:taxa-inf-x}, obtemos que a parte da
  direita converge para $-\sum_{x = 1}^n \frac{\lambda_x}{c}$,
  quantidade essa que vai para $-\infty$ quando $n \to \infty$. Como a
  desigualdade \eqref{eq:taxa-inf-inf} vale para todo $n$, concluímos que:
  \begin{align*}
    \lim_{t \searrow 0}\frac{p_{\infty \infty} (t) - 1}{t} &= -
    \infty.
    \qedhere
  \end{align*}
\end{proof}

\begin{proposicao}
  \label{prop:taxa-x-inf}
  Para $x \in \Nz$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{x \infty}(t)}{t} = \begin{cases}
      \frac{1}{\gamma_x} & \textrm{ se } c > 0 \\
      0 & \textrm{ se } c = 0 . \\
    \end{cases}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  Novamente o caso $c = 0$ é trivial em vista da Proposição
  \ref{prop:naocontinuidade}. Assim vamos supor que $c > 0$.
  \begin{align*}
    \frac{p_{x \infty}}{t}
    &= \frac{1}{t} \int_0^t P(X^x (t) = \infty | \gamma_x
    T_0 = s) \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{1}{t} \int_0^t p_{\infty \infty} (t-s)
    \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{e^{-t}}{\gamma_x} \frac{1}{t} \int_0^t p_{\infty \infty} (s)
    e^{\frac{s}{\gamma_x}} ds .
  \end{align*}

  A Proposição \ref{prop:continuidade} nos diz que $p_{\infty \infty}
  (t) \xrightarrow{t \searrow 0} 1$, de onde concluímos que:
   \begin{displaymath}
    \frac{p_{x \infty}(t)}{t} \xrightarrow{t \searrow 0} 
    \frac{1}{\gamma_x}.
    \qedhere
  \end{displaymath}
\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Propriedade de Feller}
\label{sec:prop-feller}


Uma observação interessante que fizemos sobre Processos K não
homogêneos é que eles podem não ser de Feller.  Depois de calcular as
taxas de transição, finalmente temos ferramentas para especificar
explicitamente quando isso acontece.

Dizemos que um processo markoviano é de Feller se seu semigrupo
$\Psi_t$ apresentado na Definição \ref{def:semigrupo} levar funções
contínuas em funções contínuas.

\begin{proposicao}
  São equivalentes:
  \begin{enumerate}
  \item $\displaystyle \lim_{x \to \infty} \gamma_x = 0$
  \item O Processo K é um processo de Feller.
  \end{enumerate}
\end{proposicao}

\begin{proof}

  Primeiramente vamos mostrar que se $\gamma_x \to 0$ quando $x \to
  \infty$, então o Processo K é de Feller.

  Fixemos um $t > 0$ e uma $f: \Nzb \to \R$ contínua. Queremos mostrar
  que $\Psi_t f$ é uma função contínua, isso é, que $\Psi_t f(y) \to
  \Psi f (\infty)$ quando $y \to \infty$.

  Notemos que:
  \begin{displaymath}
    \Psi_t f(\infty) - \Psi_t f (y) =
    \E \left[
      f(X^\infty(t)) - f(X^y(t))
    \right].
  \end{displaymath}

  Como a quantidade dentro da esperança é limitada por $2 \Vert f
  \Vert$, então, se mostrarmos que ela vai quase certamente à zero,
  seguirá pelo Teorema da Convergência Dominada que a esperança também
  irá a zero.

  Como $\gamma_y \to 0$, então $\gamma_y T_0 \to 0$ \qc. Assim, tomando
  $y$ suficientemente grande, teremos que $\gamma_y T_0 < t$. Portanto
  usando a Proposição \ref{prop:reinicia-infinito}, teremos que:
  \begin{displaymath}
    f(X^\infty(t)) - f(X^y(t)) = 
    f(X^\infty(t)) - f(X^\infty(t-\gamma_y T_0)).
  \end{displaymath}

  O Corolário \ref{cor:continuidades-processo} nos garante que $t$ é
  quase certamente um ponto de continuidade do processo, assim
  $X^\infty(t-\gamma_y T_0) \xrightarrow{y\to\infty} X^\infty(t)$ \qc
  e, como $f$ é contínua, concluímos que a quantidade acima converge a
  zero \qc.

  Agora vamos mostrar que se $\gamma_x$ não converge para zero quando
  $x \to \infty$, então o processo não é de Feller.

  Se $\gamma_x$ não convergir a zero, então existe um $\delta > 0$ e
  uma sequência crescente $(y_n)_n$ tal que $\gamma_{y_n} > \delta$ para
  todo $n$.

  Por absurdo, vamos supor que o processo seja de Feller. Assim
  teremos que para todo $y \in \Nz$ e $t > 0$ vale que $p_{x y} (t)
  \to p_{\infty y} (t)$ quando $x \to \infty$. Usando a Proposição
  \ref{prop:reinicia-infinito} teremos que:
  \begin{displaymath}
    p_{y_n y}(t) = \int_0^t \frac{1}{\gamma_{y_n}}
    e^{-\frac{s}{\gamma_{y_n}}} p_{\infty y} (t-s) ds
    \leq \frac{1}{\delta} \int_0^t p_{\infty y} (t-s) d s.
  \end{displaymath}

  Tomando limite $n \to \infty$ obteremos:
  \begin{align*}
     p_{\infty y}(t)
     &\leq \frac{1}{\delta} \int_0^t p_{\infty y}(t - s) d s\\
     &  =  \frac{1}{\delta} \int_0^t p_{\infty y}(s) d s .
  \end{align*}

  Como  $p_{\infty y}(t) \to 0$ quando $t \searrow 0$, concluiremos que:
  \begin{displaymath}
    \frac{p_{\infty y}(t)}{t} \xrightarrow{t \searrow 0} 0,
  \end{displaymath}
  o que contradiz a Proposição \ref{prop:taxa-inf-x}.
\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Medida invariante}
\label{sec:invariante}

O objetivo dessa seção é mostrar que a seguinte distribuição de
probabilidade é uma medida invariante para o Processo K:

\begin{equation}
  \label{eq:invariante}
  \pi(x) := \begin{cases}
    \frac{\lambda_x \gamma_x}{c + \sum_{y \in \Nz} \lambda_y \gamma_y}
    & \textrm{ se } x \in \Nz \\
    \frac{c}{c + \sum_{y \in \Nz} \lambda_y \gamma_y}
    & \textrm{ se } x = \infty \\
  \end{cases}
\end{equation}

\begin{proposicao}
  \label{prop:existe-unica-invariante}
  O Processo K tem uma única medida invariante.
\end{proposicao}
\begin{proof}
  Fixemos um número real $h > 0$ e consideremos uma cadeia de Markov em
  tempo discreto $(Y^{y, h}_n)_{n \geq 0}$ dada por $Y^{y, h}_n =
  X^y(n h)$.

  Fixado um estado $y \in \Nz$, definamos $M = \min \{ i \geq 1:
  \gamma_y T_i^y > h \}$. $M$ é uma variável aleatória com
  distribuição geométrica com probabilidade de sucesso dada por
  $e^{-\frac{h}{\gamma_y}}$. Observemos que $X_t^y = y$ para todo $t
  \in [\Gamma^y(\sigma_M^y-), \Gamma^y(\sigma_M^x-) + \gamma_y T_M^y)$
  e, como $\gamma_y T_M^y > h$, existe um múltiplo de $h$ nesse
  intervalo. Assim se $n h$ for esse múltiplo, vai valer que $Y_n^{y,
    h} = y$. Dessa maneira a esperança de tempo de retorno à um estado
  em $(Y_n)$ é finita.

  Para todo $x \in \Nzb$ e $y \in \Nz$ vale que $P (Y^{x, h}_1 = y) >
  0$.  Temos ainda que $P (Y^{x, h}_1 = \infty ) > 0$ se $c > 0$,
  enquanto que $P (Y^{x, h}_1 = \infty) = 0$ para $c = 0$. Assim a
  cadeia é irredutível no caso $c > 0$ e, no caso $c = 0$, temos que
  $\Nz$ é uma classe de comunicação fechada, enquanto que $\infty$ é
  um estado transiente.


  Em todo caso, para cada escolha de $h$, existe uma única medida de
  probabilidade $\mu_h$ tal que para todo $n \geq 1$:
  \begin{displaymath}
    \mu_h (y) = \sum_{x \in \Nzb} \mu_h(x) P ( Y_n^ {x, h} = y) =
    \sum_{x \in \Nzb} \mu_h(x) P ( X^x (nh) = y)
  \end{displaymath}

  Fixando um inteiro $m \geq 1$ e um número real $h > 0$, temos que:
  \begin{align*}
    \mu_h (y) &= \sum_{x \in \Nzb} \mu_h(x) P ( Y_{m}^ {x, h} = y)\\
    &= \sum_{x \in \Nzb} \mu_h(x) P ( X^x (m h) = y) \\
    &= \sum_{x \in \Nzb} \mu_h(x) P ( Y_{1}^ {x, m h} = y).
  \end{align*}

  Portanto $\mu_h$ é uma medida invariante para $(Y^{\bullet, mh}_n)_{n
    \geq 0}$ e, como ela é única, teremos que $\mu_h = \mu_{m h}$.

  Assim concluímos que $\mu_{h} = \mu_1$ para todo $h > 0$ racional.
  Fixado um $h > 0$ irracional, tome uma sequência de números
  racionais $(h_n)$ que convirjam para $h$. Teremos que:
  \begin{align*}
    \mu_1(y) &= \sum_{x \in \Nzb} \mu_1(x) P ( Y_{1}^ {x, h_n} = y)\\
    &= \sum_{x \in \Nzb} \mu_1(x) P ( X^x(h_n) = y)\\
    &= \sum_{x \in \Nzb} \mu_1(x) p_{x y} (h_n).    
  \end{align*}
  
  Cada termo dessa soma é dominado por $\mu_1(x)$, que tem soma $1$.
  Assim usando a Proposição \ref{prop:transicao-continua} e o Teorema da
  Convergência Dominada, concluímos que:
  \begin{align*}
    \mu_1 (y) &= \sum_{x \in \Nzb} \mu_1(x) p_{x y} (h)\\
    &= \sum_{x \in \Nzb} \mu_1(x) P(Y_1^{x, h} = y).
  \end{align*}

  Dessa forma $\mu_1$ é uma probabilidade invariante para
  $(Y_n^{\bullet, h})_n $ e, como a probabilidade invariante é única,
  concluímos que $\mu_1 = \mu_h$ para todo $h > 0$.
\end{proof}


\begin{proposicao}
  A probabilidade $\pi$ definida em \eqref{eq:invariante} é a medida
  invariante do Processo K, caso $c > 0$.
\end{proposicao}
\begin{proof}
  Seja $\mu$ a única probabilidade invariante do nosso processo que
  encontramos na Proposição \ref{prop:existe-unica-invariante}. Para
  todo $t > 0$, vale que:
  \begin{gather*}
    \frac{\mu(y)}{t} = \sum_{x \in \Nzb} \mu(x) \frac{p_{x y}
      (t)}{t}\\
  \end{gather*}

  Rearranjando os termos, teremos que para todo $y \in \Nz$:
  \begin{displaymath}
    0 = \mu(\infty) \frac{p_{\infty y}(t)}{t} + 
    \mu(y) \frac{p_{y y}(t) - 1}{t} + 
    \sum_{x \in \Nz \setminus \{y\}} \mu(x) \frac{p_{x
        y}(t)}{t}.
  \end{displaymath}

  As Proposições \ref{prop:taxa-x-x} e \ref{prop:taxa-inf-x} nos
  fornecem os limites dos dois primeiros termos, enquanto que a
  Proposição \ref{prop:taxa-x-y} nos garante que $\frac{p_{x
      y}(t)}{t}$ converge à zero quando $t \searrow 0$ para cada $x
  \in \Nz \setminus \{y\}$.

  Levando em conta que $p_{x y} (t) \leq P(\Gamma^x (\sigma_1^y -)
  \leq t) \leq P(\Gamma^\infty (\sigma_1^y -) \leq t)$, quantidade
  essa que não depende de $x$. Na demonstração da Proposição
  \ref{prop:taxa-inf-x} mostramos que $\frac{1}{t}P(\Gamma^\infty
  (\sigma_1^y -) \leq t)$ converge para $\frac{\lambda_y}{c}$ quanto
  $t \searrow 0$. Portanto, para $t$ suficientemente pequeno, teremos
  que existe uma constante $H$ que não depende de $x$, tal que
  $\frac{p_{x y}(t)}{t} \leq H$ para todo $x \in \Nz\setminus \{y\}$.

  Como $\mu(x)$ é somável em $x$, o Teorema da Convergência Dominada
  nos garante que:
  \begin{displaymath}
    \sum_{x \in \Nz \setminus \{y\}} \mu(x)
    \frac{p_{x y}(t)}{t} \xrightarrow{t \searrow 0} 0.
  \end{displaymath}

  Assim obtemos que, para cada $y \in \Nz$:
  \begin{gather*}
    0 = \mu(\infty) \frac{\lambda_y}{c} - \mu(y) \frac{1}{\gamma_y}.
  \end{gather*}

  Adicionando a restrição de que $\sum_{x\in\Nzb}\mu(x) = 1$,
  obteremos um sistema de equações cuja uma única solução é a $\pi$
  dada em \eqref{eq:invariante}.
\end{proof}

\begin{lema}
  \label{lema:c_continuo}
  Para todo $t > 0$ e $y \in \Nzb$, vale que:
  \begin{displaymath}
    X_c^y(t) \xrightarrow[\qc]{c\searrow 0} X_0^y(t)
  \end{displaymath}
\end{lema}
\begin{proof}
  Observe que $\Gamma^y_c (t) - \Gamma^y_0(t) = ct \xrightarrow{c
    \searrow 0} 0$.

  Pela Proposição \ref{prop:naocontinuidade}, temos que $P(X^y_0(t) =
  \infty) = 0$.

  Ainda sabemos que $\Gamma^y_0(\sigma^x_i-)$ é uma variável aleatória
  contínua. Assim $P( \Gamma^y_0(\sigma^x_i-) = t) = 0$.

  Dessa forma vamos fixar uma realização do processo onde $X^y_0(t) =
  x < \infty$ e $\Gamma^y_0(\sigma^x_i-) \neq t$ para todo $x \in \Nz$
  e $i \geq 1$. Tais realizações têm probabilidade $1$.


  Agora há duas possibilidades. A primeira é que $t < \gamma_y
  T_0$. Nesse caso teremos que para qualquer $c \geq 0$, vale que
  $X^y_c = y$. Assim a convergência é trivial.

  A segunda possibilidade é que $t \geq \gamma_y T_0$. Nesse caso,
  existe um $i \geq 1$ tal que $\Gamma^y_0(\sigma^x_i-) < t <
  \Gamma^y_0(\sigma^x_i)$. Portanto para $c > 0$ pequeno o suficiente,
  teremos que $\Gamma^y_c(\sigma^x_i-) < t < \Gamma^y_c(\sigma^x_i)$ e
  assim $X_c^y(t) = x$.
\end{proof}

\begin{proposicao}
  A probabilidade $\pi$ definida em \eqref{eq:invariante} é a medida
  invariante do processo K, caso $c = 0$.
\end{proposicao}
\begin{proof}

  Vamos denotar a probabilidade definida em \eqref{eq:invariante} por
  $\pi_c$

  Mostramos na Proposição \ref{prop:naocontinuidade} que para todo $y
  \in \Nzb$, $P(X^y(t) = \infty) = 0$. Assim é trivial verificar que
  $0 = \pi_0(\infty) = \sum_{y\in\Nzb} \pi_0(y) P(X^y(t) =
  \infty)$.

  A partir de agora fixemos um $x \in \Nz$ arbitrário. Vamos mostrar que:
  \begin{align*}
    \pi_0(x) &= \sum_{y \in \Nzb} \pi_0 (y) P(X^y_0 (t) = x).
  \end{align*}

  Observemos que $f_x(y) = \ind\{y = x\}$ é uma função contínua e
  limitada. Dessa forma, pelo Lema \ref{lema:c_continuo}, obtemos que
  $f_x(X^y_c(t)) \xrightarrow{c\searrow 0} f_x(X^y_0(t))$ \qc para
  todo $y\in\Nzb$.


  Assim aplicando o Teorema da Convergência Dominada concluímos que:
  \begin{displaymath}
    P(X^y_c (t) = x) = \E [f_x(X^y_c(t))]
    \xrightarrow{c\searrow0}
    \E [f_x(X^y_0(t))] = P(X^y_0(t) = x).
  \end{displaymath}

  Como $\pi_c$ é invariante para $(X^y_c)$, teremos que para todo $x
  \in \Nz$ e $t \geq 0$:
  \begin{align*}
    \pi_c(x) &= \sum_{y \in \Nzb} \pi_c (y) P(X^y_c (t) = x).
  \end{align*}
  Observando \eqref{eq:invariante}, notamos que $\pi_c(y)
  \xrightarrow{c\searrow 0} \pi_0(y)$ para todo $y \in \Nzb$ e que
  $\pi_c(y) \leq \pi_0(y)$ para todo $y \in \Nz$ e $c > 0$.  Dessa
  forma podemos tomar o limite $c\searrow0$ e usar o Teorema da
  Convergência Dominada para obter que:
  \begin{align*}
    \pi_0(x) &= \sum_{y \in \Nzb} \pi_0 (y) P(X^y_0 (t) = x).
  \end{align*}

  De onde concluímos que $\pi_0$ é uma medida invariante para o
  Processo K no caso $c = 0$.
\end{proof}


%% ------------------------------------------------------------------------- %%

\section{Gerador infinitesimal}
\label{sec:gerador}


O gerador infinitesimal é muitas vezes usado no estudo de processos
estocásticos para garantir a existência do processo
estudado. Descrevem-se as taxas de transição por meio de uma expressão
e argumenta-se, geralmente usando o teorema de Hille-Yosida, que
existe um processo com essa expressão como gerador.

Nós estamos indo na direção oposta. Já temos uma construção do nosso
processo e queremos saber como é o seu gerador.

Por simplicidade, vamos nos limitar ao caso homogêneo, isso é,
$\lambda_x = 1$ para todo $x \in \Nz$.

O exemplo K1 de \cite{kendall:56} é o nosso Processo K homogêneo, com
$c = 1$ e $\gamma_x = 1/\alpha_{x+1}$. Nesse artigo eles calcularam o
gerador infinitesimal, $\AAA$, de tal processo. Seu domínio é o
conjunto das funções $f: \Nzb \to \R$ tal que $\lim_{x \to \infty}
\frac{f(x) - f(\infty)}{\gamma_x}$ existe e vale $\sum_{x \in \Nz}
[f(\infty) - f(x)]$, série essa que tem que convergir
absolutamente. Em funções $f$ desse domínio, o gerador vale:
\begin{align*}
  \AAA f (x) = \begin{cases}
    \displaystyle
    \frac{f(\infty) - f(x)}{\gamma_x} & \text{se } x \in \Nz\\
    \displaystyle
    \sum_{y\in \Nz} [f(y) - f(\infty)] & \text{se } x = \infty.
  \end{cases}
\end{align*}

Nós vamos tratar apenas do caso $c = 0$.

O primeiro passo será definir a classe de funções sobre a qual
definiremos o gerador. Essa classe tem que ser pequena o suficiente
para que o gerador esteja bem definido, mas rica o suficiente para que
ele caracterize o semigrupo. A classe adotada será:

\begin{equation}
  \label{eq:classe-gerador}
  \DDD = \left\{ f \in \CC: \sum_{x\in \Nz} |f(x)-f(\infty)| < \infty,
    \:
    \sum_{x\in \Nz} \left( f(x)-f(\infty)\right) = 0, \:
    \lim_{x \to \infty} \frac{f(x) - f(\infty)}{\gamma_x} \textrm{ existe}
  \right\},
\end{equation}
onde $\CC$ é o conjunto das funções de $\Nzb$ em $\R$ contínuas.

\begin{definicao}
  \label{def:gerador}
  O gerador infinitesimal $\AAA$ é uma função $\AAA: \DDD \to \CC$,
  definida através do limite:
  \begin{equation}
    \label{eq:def-gerador}
    \AAA f = \lim_{t \searrow 0} \frac{\Psi_t f - f}{t},
  \end{equation}
  onde $\Psi$ é o semigrupo de transição apresentado na Definição
  \ref{def:semigrupo}.
\end{definicao}

Agora vamos mostrar nas proposições \ref{prop:gerador-x} e
\ref{prop:gerador-inf} que esse limite de fato existe e calcular o seu
valor.

\begin{proposicao}
  \label{prop:gerador-x}
  Para $f \in \DDD$ e $x \in \Nz$, vale que:
  \begin{equation}
    \label{eq:gerador-x}
    \lim_{t \searrow 0} \frac{\Psi_t f(x) - f(x)}{t} = \frac{f(\infty)
    - f(x)}{\gamma_x}
  \end{equation}
\end{proposicao}

\begin{proof}
  Esse resultado vale para toda $f \in \CC$. Não facilitará em nada
  nos restringir à $\DDD$. Assim vamos fixar uma $f \in \CC$ e $x \in \Nz$
  arbitrários.

  Para um $\epsilon > 0$ fixado arbitrariamente vamos mostrar que:
  \begin{align}
    \label{eq:gerador-x-sup}
    \limsup_{t \searrow 0} \frac{\Psi_t f (x) - f(x)}{t} &\leq
    \frac{f(\infty) - f(x) + \epsilon}{\gamma_x}\\
    \label{eq:gerador-x-inf}
    \liminf_{t \searrow 0} \frac{\Psi_t f (x) - f(x)}{t} &\geq
    \frac{f(\infty) - f(x) - \epsilon}{\gamma_x}, 
  \end{align}
  de onde conclui-se \eqref{eq:gerador-x} imediatamente.

  Agora observemos que:
  \begin{align*}
    \frac{\Psi_t f(x) - f(x)}{t}
    &= \sum_{y \in \Nzb} \left[ f(y) - f(x) \right] \frac{p_{x y}(t)}{t}\\
    &=
    \sum_{y \in \Nzb\setminus\{x\}} \left[ f(y) - f(x) \right] \frac{p_{x y}(t)}{t}.
  \end{align*}


  Como $f \in \CC$, podemos tomar um $m \in \Nz$ tal que $m > x$ e
  $f(y) < f(\infty) + \epsilon$ para $y > m$. Com base na Proposição
  \ref{prop:taxa-x-y} concluímos que cada termo da série acima
  converge para zero quanto $t \searrow 0$. Portanto:
  \begin{align*}
    \limsup_{t \searrow 0}
    \frac{\Psi_t f(x) - f(x)}{t}
    &= \limsup_{t \searrow 0}
    \sum_{y > m} \left[ f(y) - f(x) \right] \frac{p_{x y}(t)}{t}\\
    &\leq \left[ f(\infty) - f(x) + \epsilon \right] \limsup_{t \searrow 0} 
    \sum_{y > m} \frac{p_{x y}(t)}{t}\\
    &= \left[ f(\infty) - f(x) + \epsilon \right] \limsup_{t \searrow 0}
    \frac{1 - P\left( X^x(t) \leq m \right)}{t}\\
    &= \left[ f(\infty) - f(x) + \epsilon \right] \limsup_{t \searrow 0}
    \left[
      \frac{1 - p_{x x}(t)}{t}
      - \sum_{\substack{y \leq m \\ y\neq x}} \frac{p_{x y}(t)}{t}
    \right]\\
  \end{align*}

  Agora usando a Proposição \ref{prop:taxa-x-x}, verificamos que o
  primeiro termo da expressão acima converge para
  $\frac{1}{\gamma_x}$, enquanto que o segundo converge para zero pela
  Proposição \ref{prop:taxa-x-y}. Dessa forma concluímos
  \eqref{eq:gerador-x-sup}. A expressão \eqref{eq:gerador-x-inf} pode
  ser demonstrada de forma análoga.
\end{proof}


\begin{proposicao}
  \label{prop:gerador-inf}
  Para $f \in \DDD$, vale que:
  \begin{equation}
    \label{eq:gerador-inf}
    \lim_{t \searrow 0} \frac{\Psi_t f(\infty) - f(\infty)}{t} =
    \lim_{x \to \infty} \frac{f(\infty) - f(x)}{\gamma_x}
  \end{equation}
\end{proposicao}
\begin{proof}
  Fixemos uma $h \in \DDD$ arbitrária. Vamos denotar por $\bar{h}(x) =
  h(x) - h(\infty)$, além disso vamos definir $L = \lim_{x \to \infty}
  -\bar{h}(x)/\gamma_x$, limite que existe pela definição de
  $\DDD$. Sem perda de generalidade vamos supor que $L \geq 0$.

  Por toda essa demonstração, vamos supor que $\gamma_1 \geq \gamma_2
  \geq \gamma_x$ para todo $x > 2$. Não perdemos generalidade ao fazer
  essa suposição porque basta trocar $1$ e $2$ no nosso argumento pelos
  estados que têm $\gamma$ máximo. Tais estados existem porque $\gamma_x
  \to 0$ quando $x \to \infty$.

  Outra notação que usaremos será:
  \begin{align*}
    \Gamma^{<x>}(t) &= \sum_{z\in\Nz\setminus\{x\}} \sum_{i =
      1}^{N^z(t)} \gamma_y T^z_i \\
    \Gamma^{<x, y>}(t) &= \sum_{z\in\Nz\setminus\{x,y\}} \sum_{i =
      1}^{N^z(t)} \gamma_y T^z_i .
  \end{align*}


  Para um $\epsilon > 0$ fixado arbitrariamente, vamos mostrar que:
  \begin{align}
    \label{eq:gerador-inf-sup}
    \limsup_{t \searrow 0} \frac{\Psi_t h(\infty) - h(\infty)}{t} 
    &\leq L + \epsilon\\
    \label{eq:gerador-inf-inf}
    \liminf_{t \searrow 0} \frac{\Psi_t h(\infty) - h(\infty)}{t} 
    &\geq L - \epsilon,
  \end{align}
  de onde concluiremos \eqref{eq:gerador-inf} imediatamente.

  Agora vamos começar a fazer as contas de fato.
  \begin{align*}
    \Psi_t h (\infty) - h(\infty) &=
    \sum_{x \in \Nz} \bar{h}(x) P\left( X^\infty(t) = x \right)\\
    &= 
    \sum_{x \in \Nz} \bar{h}(x) P\left( X^\infty(t) = x \text{ pela } 1^{a}
        \text{ vez}\right)\\
    &+ \sum_{x \in \Nz} \bar{h}(x) P\left( X^\infty(t) = x \text{ não pela
      } 1^a \text{ vez}\right)
  \end{align*}

  O Lema \ref{lema:gerador-erro} nos diz que o segundo termo, dividido
  por $t$ converge para zero quando $t \searrow 0$, assim podemos nos
  concentrar apenas nas primeiras visitas. Usando o fato de que
  $\sum_{x\in\Nz}\bar{h}(x) = 0$, podemos escrever o primeiro termo da
  expressão acima como:
  \begin{align}
    \notag
    & \sum_{x \in \Nz} \bar{h}(x) \left[
      P\left( X^\infty(t) = x \text{ pela } 1^a \text{ vez} \right) - 
      P\left( X^\infty(t) = 1 \text{ pela } 1^a \text{ vez} \right)
    \right] \\
    \label{eq:gerador-inf-1}
    &= \sum_{x > 1} \bar{h}(x) \left[
      P\left( X^\infty(t) = x \text{ pela } 1^a \text{ vez} \right) - 
      P\left( X^\infty(t) = 1 \text{ pela } 1^a \text{ vez} \right)
    \right].
  \end{align}
  

  Agora podemos escrever:
  \begin{align*}
    P\left( X^\infty(t) = x \text{ pela } 1^a \text{ vez} \right) &=
    P\left(
      X^\infty(t) = x \text{ pela } 1^a \text{ vez},
      \sigma^x_1 < \sigma^1_1
    \right) \\ &+
    P\left(
      X^\infty(t) = x \text{ pela } 1^a \text{ vez},
      \sigma^x_1 > \sigma^1_1
    \right),
  \end{align*}
  e com base nisso reescrever \eqref{eq:gerador-inf-1} como:
  \begin{gather}
    \label{eq:gerador-inf-2}
    \sum_{x > 1} \bar{h}(x) \left[
      P\left(
        X^\infty(t) = x \text{ pela } 1^a \text{ vez},
        \sigma^x_1 < \sigma^1_1
      \right) - 
      P\left(
        X^\infty(t) = 1 \text{ pela } 1^a \text{ vez},
        \sigma^1_1 < \sigma^x_1
      \right)
    \right]
    + \epsilon_t
  \end{gather}

  Para mostrar que $\frac{\epsilon_t}{t} \to 0$ quando $t
  \searrow 0$, note que:
  \begin{align}
    \left| \frac{\epsilon_t}{t} \right|
    \label{eq:gerador-inf-visita-1-antes}
    & \leq  \frac{1}{t} \sum_{x > 1}|\bar{h}(x)|
    P\left(
      X^\infty(t) = x \text{ pela } 1^a \text{ vez},
      \sigma^x_1 > \sigma^1_1
    \right)\\
    \label{eq:gerador-inf-visita-x-antes}    
    &+ \frac{1}{t} \sum_{x > 1}|\bar{h}(x)|
    P\left(
      X^\infty(t) = 1 \text{ pela } 1^a \text{ vez},
      \sigma^1_1 > \sigma^x_1
    \right)
  \end{align}

  Para dominar \eqref{eq:gerador-inf-visita-1-antes}, note que
  $\Gamma^{<1, x>}(\sigma_1^x)$ domina estocasticamente $\Gamma^{<1,
    2>}(\sigma_1^2)$. Dessa forma:
  \begin{align*}
    \frac{1}{t} P\left(
      X^\infty(t) = x \text{ pela } 1^a \text{ vez},
      \sigma^x_1 > \sigma^1_1
    \right)
    & \leq \frac{1}{t} P(\Gamma^{<1, x>}(\sigma^x_1) \leq t, \gamma_1 T^1_1 \leq
    t)\\
    &\leq P(\Gamma^{<1, 2>}(\sigma^2_1) \leq t) \frac{1 -
      e^{-t/\gamma_1}}{t}\\
    &\leq \frac{P(\Gamma^{<1, 2>}(\sigma^2_1) \leq t)}{\gamma_1}.
  \end{align*}

  Essa última quantidade não depende de $x$ e vai a zero quando $t
  \searrow 0$. Dessa forma concluímos que
  \eqref{eq:gerador-inf-visita-1-antes} converge a zero quando $t
  \searrow 0$.    

  Para controlar \eqref{eq:gerador-inf-visita-x-antes}, notemos que se
  $X(t) = 1$ e $\sigma_1^1 > \sigma_1^x$ então o processo passou por $1$
  e por $x$ até o instante $t$. Como $\gamma_1 \geq \gamma_x$, usando
  a propriedade de falta de memória da exponencial nos $\sigma$'s,
  obtemos que a probabilidade de termos passado por $1$ e por $x$ é
  menor ou igual à probabilidade do processo ter passado por $x$ pelo
  menos duas vezes. Dessa forma, usando o Lema
  \ref{lema:gerador-erro}, concluímos que
  \eqref{eq:gerador-inf-visita-1-antes} converge à zero quando $t
  \searrow 0$.

  
  Voltando a trabalhar com \eqref{eq:gerador-inf-2}, notemos que,
  condicionado em $\{\sigma^x_1 < \sigma^1_1\}$, $\sigma^x_1$ tem
  distribuição exponencial de média $1/2$. Ainda vale que $\Gamma^{<1,
    x>}(\bullet)$ é independente de $\{\sigma^x_1,
  \sigma^1_1\}$. Assim se $S$ for uma v.a. exponencial de média $1/2$
  independente de todo o processo, teremos que a distribuição de
  $\Gamma^{<1, x>}(\sigma_1^x)$ condicionada em $\sigma^x_1 <
  \sigma_1^1$ será igual à distribuição de $\Gamma^{<1, x>}(S)$.


  Notemos que $X^\infty(t) = x$ pela primeira vez se e somente se
  $\Gamma^{<x>}(\sigma^x_1) \leq t < \Gamma^{<x>}(\sigma^x_1) +
  \gamma_x T^x_1$.

  Levando em conta todas as considerações anteriores, se denotarmos
  por $f_x$ a densidade de $\Gamma^{<1, x>}(S)$ e por $F_x$ a sua
  função de distribuição, podemos escrever \eqref{eq:gerador-inf-2}, a
  menos do $\epsilon_t$, por:
  \begin{align}
     \frac{1}{2} &\sum_{x > 1} \bar{h}(x) \int_0^t f_x (s) \left(
      e^{-\frac{t-s}{\gamma_x}} - e^{-\frac{t-s}{\gamma_1}}
    \right) d s \notag\\
    \label{eq:gerador-inf-3}
    = &- \frac{1}{2} \sum_{x > 1} \bar{h}(x) \int_0^t f_x (s) \left(
      1 - e^{-\frac{t-s}{\gamma_x}}
    \right) d s\\
    \label{eq:gerador-inf-4}
    &+ \frac{1}{2} \sum_{x > 1} \bar{h}(x) \int_0^t f_x (s) \left(
      1 - e^{-\frac{t-s}{\gamma_1}}
    \right) d s
  \end{align}

  Para controlar \eqref{eq:gerador-inf-4}, podemos escrever:
  \begin{align*}
    \left| \frac{1}{t} \sum_{x > 1} \bar{h}(x) \int_0^t f_x (s) \left(
        1 - e^{-\frac{t-s}{\gamma_1}}
      \right) d s \right| &\leq
  \sum_{x > 1} |\bar{h}(x)| \int_0^t f_x (s) \frac{1}{t}\left(
    1 - e^{-\frac{t-s}{\gamma_1}}
  \right) d s \\
  & \leq \sum_{x > 1} |\bar{h}(x)| \frac{F_x(t)}{\gamma_1} d s  .
  \end{align*}
  Observemos que cada termo dessa série converge para zero quando $t
  \searrow 0$, enquanto que eles são dominados por $|\bar{h}(x)| /
  \gamma_1$, que é somável. Assim, usando o Teorema da Convergência
  Dominada, concluímos que \eqref{eq:gerador-inf-4} dividido por
  $t$ converge para zero quando $t \searrow 0$.

  Fazendo uma integração por partes, obtemos que
  \eqref{eq:gerador-inf-3} é igual à:
  \begin{align*}
    - \frac{1}{2} \sum_{x > 1} \frac{\bar{h}(x)}{\gamma_x} \left[
      \int_0^t F_x (s) e^{-\frac{t-s}{\gamma_x}} ds
    \right].
  \end{align*}

  Como $F_x(t) \to 0$ quanto $t \searrow 0$, temos que cada termo da
  série acima dividido por $t$ converge para zero quando $t \searrow
  0$. Assim podemos nos preocupar apenas com a cauda da série. Para
  isso tomemos um $m > 2$ tal que $-\frac{\bar{h}(x)}{\gamma_x} < L +
  \epsilon$ sempre que $x > m$.

  Como $\gamma_2\geq \gamma_x$, então $\Gamma^{<1, x>}$ domina
  estocasticamente $\Gamma^{<1, 2>}$. Dessa forma $F_x(t) \leq
  F_2(t)$.

  Com todas essas considerações, podemos escrever:
  \begin{align*}
    \limsup_{t \searrow 0} \frac{\Psi_t h(\infty) - h(\infty)}{t}
    &\leq \left( L + \epsilon \right) \limsup_{t \searrow
      0}\frac{1}{t} \int_0^t \frac{F_2(s)}{2} \sum_{x >
      m}e^{-\frac{t-s}{\gamma_x}} ds\\
    &= \left( L + \epsilon \right) \limsup_{t \searrow
      0}\frac{1}{t} \int_0^t \frac{F_2(s)}{2} \left[
      2 + \sum_{x > 2} e^{-\frac{t-s}{\gamma_x}}
    \right] ds\\
    &- \left( L + \epsilon \right) \limsup_{t \searrow
      0}\frac{1}{t} \int_0^t \frac{F_2(s)}{2} \left[
      2 + \sum_{x=3}^m e^{-\frac{t-s}{\gamma_x}}
    \right] ds\\
  \end{align*}

  O segundo termo dessa soma vale zero já que $F_2(s) \to 0$ quando $s
  \searrow 0$ e a soma dentro da integral é finita. Resta calcular o
  primeiro termo. Para isso vamos definir:
  \begin{align*}
    \Phi(t) &:= \int_0^t \frac{F_2(s)}{2} \left[
      2 + \sum_{x > 2} e^{-\frac{t-s}{\gamma_x}}
    \right] ds.
  \end{align*}

  Vamos mostrar que $\Phi(t) = t$. Faremos isso calculando a
  transformada de Laplace de $\Phi$ e a identificando com a da
  identidade.

  Notemos que $\Phi$ é a convolução de duas outras funções; assim
  vamos calcular a transformada de Laplace delas. Para um $\beta > 0$
  fixado, usando integração por partes, podemos calcular a
  transformada de Laplace da primeira por:
  \begin{align*}
    \int_0^{\infty} e^{-\beta t} \frac{F_2(t)}{2} dt 
    &= \frac{1}{2 \beta} \int_0^\infty e^{-\beta t}  f_2(t) dt\\
    &= \frac{1}{2 \beta}
    \E \left[ \exp \{-\beta \Gamma^{<1,2>} (S)\}  \right]\\
    &= \frac{1}{\beta}
    \left( 2 + \sum_{x > 2}
      \frac{\beta\gamma_x}{1 + \beta \gamma_x}  \right)^{-1}.
  \end{align*}

  Enquanto que a segunda irá valer:
  \begin{align*}
    \int_0^{\infty} e^{-\beta t} \left[
      2 + \sum_{x > 2} e^{-\frac{t}{\gamma_x}}
    \right]  dt 
    &= \frac{2}{\beta} + \sum_{x > 2} \int_0^\infty \exp \left\{
      - \left(\beta + \frac{1}{\gamma_x} \right) t \right\} dt\\
    &= \frac{2}{\beta} + \sum_{x > 2} \frac{\gamma_x}{1 +
      \beta\gamma_x}\\
    &= \frac{1}{\beta} \left(
      2 + \sum_{x > 2} \frac{\beta\gamma_x}{1 +
      \beta\gamma_x}
    \right)
  \end{align*}

  Multiplicando as duas, obteremos que:
  \begin{displaymath}
    \int_0^\infty e^{- \beta t} \Phi(t) dt = \frac{1}{\beta^2}.
  \end{displaymath}

  Agora identificamos essa transformada de Laplace com a da
  identidade, já que:
  \begin{displaymath}
    \int_0^\infty t e^{- \beta t}  d t = \frac{1}{\beta^2}.
  \end{displaymath}


  Dessa forma podemos concluir que:
  \begin{align*}
    \limsup_{t \searrow 0} \frac{\Psi_t h(\infty) - h(\infty)}{t}
    &\leq \left( L + \epsilon \right) \limsup_{t \searrow
      0}\frac{1}{t}  \Phi(t)\\
    &= L + \epsilon.
  \end{align*}


  Com isso mostramos \eqref{eq:gerador-inf-sup}. Podemos mostrar
  \eqref{eq:gerador-inf-inf} de forma análoga, mas em vez de usar que
  $\Gamma^{<1,x>}$ domina $\Gamma^{<1,2>}$, usaremos que
  $\Gamma^{<1>}$ domina $\Gamma^{<1, x>}$. Também teremos que inserir
  $x = 2$ na soma que define $\Phi$.
\end{proof}

\begin{lema}
  \label{lema:gerador-erro}
  Para $h \in \DDD$, vale que:
  \begin{equation}
    \label{eq:gerador-erro}
    \lim_{t \searrow 0} \frac{1}{t} \sum_{x\in\Nz} |h(x) - h(\infty)|
    P\left( X^\infty \text{passou mais que 1 vez por x até o
      instante } t \right) = 0.
  \end{equation}
\end{lema}
\begin{proof}
  Novamente vamos supor, sem perda de generalidade, que $\gamma_1 \geq
  \gamma_x$ para todo $x \in \Nz$. Vamos também denotar por $f(x) =
  |h(x) - h(\infty)|$.

  Usando o fato que $\Gamma^{<x>} (\sigma^x_2)$ domina
  estocasticamente $\Gamma^{<1>} (\sigma^1_2)$ e que $\Gamma^{<1>}$
  tem incrementos estacionários e independentes, podemos estimar:
  \begin{align*}
    P \left( \Gamma^{<x>} (\sigma^x_2) \leq t \right)
    &\leq P \left( \Gamma^{<1>} (\sigma^1_2) \leq t \right)\\
    &= P \left( \exp \left\{
        -\frac{\Gamma^{<1>}(\sigma^1_2)}{t}
      \right\}
        \geq e^{-1} \right)\\
    &\leq e \:
    \E \left[ \exp \left\{
        -\frac{\Gamma^{<1>}(\sigma^1_2)}{t}
      \right\} \right]\\
    &= e \: \left\{ \E \left[  -\frac{\Gamma^{<1>}(\sigma^1_1)}{t}
      \right] \right\}^2 \\
    &= e \: \left( 1 + \sum_{y > 1} \frac{\gamma_y}{t +
        \gamma_y}  \right)^{-2}\\
    &\leq e \: \left( \sum_{y \in \Nz} \frac{\gamma_y}{t +
        \gamma_y}  \right)^{-2}
  \end{align*}

  Observemos que o evento $\{ X^\infty $ passou mais que 1 vez por x
  até o instante $t\}$ está contido no evento $\{ \Gamma^{<x>}
  (\sigma^x_2) \leq t,\, \gamma_x T^x_1 \leq t\}$. Assim usando a
  estimativa acima e o fato que $\Gamma^{<x>} (\sigma^x_2)$ é
  independente de $T_1^x$, obtemos que:
  \begin{align*}
    \frac{1}{t} \sum_{x\in\Nz} f(x) P\left( X^\infty(t) = x \text{ não
        pela primeira vez}\right) 
    &\leq \mathrm{cte} \: \left( \sum_{y \in \Nz} \frac{\gamma_y}{t +
        \gamma_y}  \right)^{-2} \sum_{x \in \Nz} f(x) \frac{1 -
      e^{-t/\gamma_x}}{t} .
  \end{align*}

  Se denotarmos por $A_t = \{ x \in \Nz: \gamma_x > t \}$,
  podemos escrever a quantidade acima por:
  \begin{align}
    \label{eq:erro-gerador-partes}
    \left( 1 + \sum_{y > 1} \frac{\gamma_y}{t + \gamma_y} \right)^{-2}
    \sum_{x \in \Nz} f(x) \frac{1 - e^{-t/\gamma_x}}{t}
    &= \left( \sum_{y \in \Nz} \frac{\gamma_y}{t + \gamma_y} \right)^{-2}
    \sum_{x \in A_t} f(x) \frac{1 - e^{-t/\gamma_x}}{t}\\
    &+ \left( \sum_{y \in \Nz} \frac{\gamma_y}{t + \gamma_y} \right)^{-2}
    \sum_{x \not\in A_t} f(x) \frac{1 - e^{-t/\gamma_x}}{t} .\notag
  \end{align}

  Vamos mostrar que os dois termos convergem a zero quando $t
  \searrow 0$.

  Para o primeiro termo notemos que, para cada $t > 0$ fixado, $A_t$ é
  um conjunto finito mas a sua cardinalidade vai para o infinito
  quando $t \searrow 0$. Observemos também que, como $h \in \DDD$,
  então $f(x)/\gamma_x$ converge; assim essa quantidade é limitada por
  uma constante. Se denotarmos por $\#A_t$ a cardinalidade de $A_t$,
  poderemos escrever:
  \begin{align*}
    \left( \sum_{y \in \Nz} \frac{\gamma_y}{t + \gamma_y} \right)^{-2}
    \sum_{x \in A_t} f(x) \frac{1 - e^{-t/\gamma_x}}{t}
    &\leq
    \left( \sum_{y \in A_t} \frac{\gamma_y}{t + \gamma_y} \right)^{-2}
    \sum_{x \in A_t} \frac{f(x)}{\gamma_x}\\
    &\leq \mathrm{cte} \:
    \left( \sum_{y \in A_t} \frac{\gamma_y}{2 \gamma_y} \right)^{-2}
    \# A_t\\
    &= \mathrm{cte} \:
    \left( \frac{1}{2} \# A_t \right)^{-2}
    \# A_t\\
    &\leq \mathrm{cte} \: \frac{1}{\#A_t}
    \xrightarrow{t \searrow 0} 0.
  \end{align*}

  Com isso concluímos que o primeiro termo de
  \eqref{eq:erro-gerador-partes} converge a zero quando $t \searrow
  0$.

  Vamos tratar agora o segundo. Como $\frac{f(x)}{\gamma_x}$ converge,
  então essa sequência é limitada. Assim $f(x) \leq \mathrm{cte}\;
  \gamma_x$. Juntando isso ao fato de que $1 - e^{-t/\gamma_x} \leq 1$
  podemos escrever:
  \begin{align}
    \label{eq:erro-gerador-quase}
    \left( \sum_{y \in \Nz} \frac{\gamma_y}{t + \gamma_y} \right)^{-2}
    \sum_{x \not\in A_t} f(x) \frac{1 - e^{-t/\gamma_x}}{t}
    & \leq \mathrm{cte} \;
    \left( \sum_{y \in \Nz} \frac{\gamma_y}{t + \gamma_y} \right)^{-2}
    \sum_{x \not\in A_t} \frac{\gamma_x}{t}.
  \end{align}

  Agora vamos separar em 2 casos.  O primeiro ocorre quando $\sum_{x
    \not\in A_t} \frac{\gamma_x}{t}$ é limitado. Nesse caso é fácil
  ver que o denominador de \eqref{eq:erro-gerador-quase} diverge
  quando $t\searrow 0$. Dessa forma a expressão acima converge para zero
  quanto $t \searrow 0$.


  O segundo ocorre quando $\sum_{x \not\in A_t} \frac{\gamma_x}{t}$
  vai para o infinito quanto $t \searrow 0$.

  Quando $y \not\in A_t$, vale que $\gamma_y \leq t$ e,
  consequentemente, $\frac{\gamma_x}{t+\gamma_x} \geq \frac{\gamma_y}{2
    t}$. Portanto podemos dominar \eqref{eq:erro-gerador-quase} a
  menos da constante por:
  \begin{displaymath}
    \left(
      \frac{1}{2} \sum_{y \not\in A_t} \frac{\gamma_y}{t}
    \right)^{-2}
    \sum_{x \not\in A_t} \frac{\gamma_x}{t} \xrightarrow{t \searrow 0}
    0.\qedhere
  \end{displaymath}
\end{proof}

\begin{teorema}
  A função $\AAA$, definida nas proposições \ref{prop:gerador-x} e
  \ref{prop:gerador-inf} é um gerador infinitesimal que caracteriza
  o semigrupo $\Psi_t$.
\end{teorema}
\begin{proof}
  Seguindo \cite{liggett:85}, munindo $\CC$ da norma do supremo, vamos
  mostrar que:
  \begin{enumerate}
  \item $\AAA$ leva funções de $\DDD$ em $\CC$;
  \item $\DDD$ é denso em $\CC$.
  \item A imagem de $I - \AAA$ é densa em $\CC$, onde $I$ é a identidade.
  \end{enumerate}

  O item 1 é evidente de \eqref{eq:gerador-x} e
  \eqref{eq:gerador-inf}.

  Para o item 2, tomemos uma $g \in \CC$ e $\epsilon > 0$ arbitrários e
  vamos encontrar uma $f \in \DDD$ tal que $\Vert f - g \Vert <
  \epsilon$.

  Como $g$ é contínua, existe um $n_0$ tal que $|g(x) - g(\infty)| <
  \epsilon/2$ para $x > n_0$. Fixemos tal $n_0$. Vamos definir $f(x) =
  g(x)$ para $x \leq n_0$.

  Agora tomemos um $m$ tal que $ \frac{1}{m} |\sum_{x = 1}^{n_0} (g(x)
  - g(\infty))| < \epsilon/2$ e, para $n_0 < x \leq n_0 + m$,
  definamos $f(x) = g(\infty) -\frac{1}{m} \sum_{x = 1}^{n_0} (g(x) -
  g(\infty))$.

  Para $x > n_0 + m$ ou $x = \infty$, tomemos $f(x) = g(\infty)$.

  Como $f(x) = f(\infty)$ para $x > n_0 + m$, então $f$ é contínua,
  $\lim_{x \to \infty} \frac{f(x) - f(\infty)}{\gamma_x} = 0$ e 
  $\sum_{x \in \Nz} |f(x) - f(\infty)| < \infty$. Vale ainda que:
  \begin{align*}
    \sum_{x \in \Nz} (f(x) - f(\infty) )
    &= \sum_{x = 1}^{n_0} (f(x) - f(\infty)) + 
    \sum_{x = n_0 + 1}^m (f(x) - f(\infty)) \\
    &= \sum_{x = 1}^{n_0} (g(x) - g(\infty)) +
    m \left( - \frac{1}{m}  \sum_{x = 1}^{n_0} (g(x) - g(\infty))
    \right)\\
    &= 0.
  \end{align*}

  Portanto $f \in \DDD$. Ainda $\sup_{x \in \Nzb} |f(x) - g(x)| <
  \epsilon$ por construção. Assim concluímos que $\DDD$ é denso em
  $\CC$.

  Resta mostrar o item 3. Para isso vamos mostrar que a imagem de $I -
  \AAA$ é o conjunto das funções contínuas.

  Tomemos uma $g \in \CC$ e vamos encontrar uma $f \in \DDD$ tal que
  $f - \AAA f = g$.

  Observemos que $\sum_{x \in \Nz} \frac{\gamma_x}{1+\gamma_x} <
  \infty$ e $g$ é limitada. Assim a constante $L$ abaixo está bem
  definida:
  \begin{displaymath}
    L := -\left( \sum_{x \in \Nz} \frac{\gamma_x}{1+\gamma_x} \right)^{-1}
    \sum_{x \in \Nz} \frac{\gamma_x}{1 +
      \gamma_x} \left[ g(x) - g(\infty) \right].
  \end{displaymath}

  Com base nela, vamos definir:
  \begin{equation*}
    %\label{eq:inversa-imagem}
    f(x) = \begin{cases}
      g(\infty) - L & \textrm{ se } x = \infty\\
      \frac{1}{1+\gamma_x} \left[
        \gamma_x g(x) + g(\infty) - L
      \right] & \textrm{ caso contrário.}
    \end{cases}
  \end{equation*}

  Com base nessa definição, observamos que:
  \begin{align*}
    %\label{eq:inversa-imagem-2}
    \frac{f(x) - f(\infty)}{\gamma_x} &=
    \frac{g(x) - g(\infty) }{1+\gamma_x} +
    \frac{L}{1+\gamma_x} \xrightarrow{x \to \infty} L.
  \end{align*}

  Usando a definição de $L$, podemos calcular:
  \begin{align*}
    \sum_{x \in \Nz} \left|f(x) - f(\infty)\right|
    &\leq \sum_{x\in\Nz} \frac{\gamma_x}{1+\gamma_x} |g(x) -
    g(\infty)|
    + \sum_{x\in\Nz} \frac{\gamma_x}{1+\gamma_x} |L| < \infty\\
    \sum_{x \in \Nz} \left[f(x) - f(\infty)\right]
    &= \sum_{x\in\Nz} \frac{\gamma_x}{1+\gamma_x} \left[g(x) -
    g(\infty)\right]
    + \sum_{x\in\Nz} \frac{\gamma_x}{1+\gamma_x} L\\
    &= \sum_{x\in\Nz} \frac{\gamma_x}{1+\gamma_x} \left[g(x) -
    g(\infty)\right]
  - \sum_{x\in\Nz} \frac{\gamma_x}{1+\gamma_x} \left[g(x) -
    g(\infty)\right] = 0.
  \end{align*}

  Observando $\eqref{eq:classe-gerador}$, concluímos que $f \in
  \DDD$.

  Para verificar que $f - \AAA f = g$, observe que para $x \in \Nz$:
  \begin{align*}
    f(\infty) - \AAA f (\infty) &=
    g(\infty) - L - (-L) = g(\infty) \\
    f(x) - \AAA f (x) &=
    \frac{1}{1+\gamma_x} \left[
      \gamma_x g(x) + g(\infty) - L
    \right] - \left(
      - \frac{g(x) - g(\infty) }{1+\gamma_x} -
      \frac{L}{1+\gamma_x}
    \right)\\
    &=
   \frac{\gamma_x g(x)}{1+\gamma_x}
    + \frac{g(x)}{1+\gamma_x} = g(x)
    \qedhere
  \end{align*}
\end{proof}



%% ------------------------------------------------------------------------- %%

\section{Tempo no infinito}
\label{sec:tempo-infinito}

Na demonstração da Proposição \ref{prop:continuidade} mostramos que,
no caso $c > 0$, o tempo que o processo passa no infinito tem medida
de Lebesgue positiva quase certamente.

Usando a Proposição \ref{prop:naocontinuidade}, podemos mostrar que a
medida de Lebesgue do tempo em que o processo passa no infinito vale
zero quase certamente no caso $c = 0$.

Nessa seção expandiremos um pouco o caso $c = 0$. Especificamente
apresentaremos uma maneira de calcular a dimensão de Hausdorff do
tempo em que o processo passa no infinito.  Para o restante dessa
seção, consideraremos sempre que $c = 0$.

Em vista da Proposição \ref{prop:reinicia-infinito}, vamos considerar
somente o Processo K iniciado no $\infty$, visto que o conjunto dos
instantes onde o processo está no infinito ao iniciar em um estado $y$
é igual ao mesmo conjunto do processo iniciado em $\infty$ mas
transladado.

\begin{definicao}
  Vamos denotar por $\RR$ a imagem de $\Gamma^\infty$. Isso é:
  \begin{displaymath}
    \RR := \left\{
      t \geq 0 : \exists s \geq 0 \textrm{ tal que } \Gamma^\infty(s) = t
    \right\}.
  \end{displaymath}
\end{definicao}

\begin{proposicao}
  Para quase toda realização do processo, vale que:
  \begin{displaymath}
    \RR = \left\{ t \geq 0: X(t) = \infty \right\}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  Vamos fixar uma realização do nosso processo onde $\Gamma$ é
  estritamente crescente, càdlàg, limitada em compactos e $\lim_{t \to
    \infty} \Gamma(t) = \infty$.

  Nessas realizações, tome um $t \in \RR$. Assim existe um (único por
  $\Gamma$ ser crescente) $s \geq 0$ tal que $\Gamma(s) = t$. Por
  absurdo, vamos supor que $X(t) = x < \infty$. Assim existe um $i
  \geq 1$ tal que $\Gamma(\sigma_i^x -) \leq t <
  \Gamma(\sigma_i^x)$. Portanto $\Gamma(s) < \Gamma(\sigma_i^x)$. Como
  $\Gamma$ é crescente, concluímos que $s < \sigma_i^x$, de onde
  concluímos que $\Gamma(\sigma_i^x-) > \Gamma(s) = t$, o que
  contraria a hipótese.

  Agora tomemos um $t \geq 0$ tal que $X(t) = \infty$. Definamos $s =
  \inf \{ r \geq 0 : \Gamma(r) > t \}$. Usando a definição do ínfimo e
  o fato de $\Gamma$ ser crescente e càdlàg, concluímos que
  $\Gamma(s-) \leq t$ e $\Gamma(s) \geq t$. Agora, por absurdo,
  suponhamos que $\Gamma(s) > t$. Assim teremos que $\Gamma(s-) \leq t
  < \Gamma(s)$. Como os pontos de descontinuidade de $\Gamma$
  correspondem às marcas dos processos de Poisson, teremos que existe
  um $\sigma_i^x = s$, de onde concluímos a contradição de que $X(t) =
  x \neq \infty$. Portanto, como não pode acontecer que $\Gamma(s) >
  t$, teremos que $\Gamma(s) = t$ e assim $t \in \RR$.
\end{proof}

\begin{proposicao}
  Se denotarmos por $\RRb$ o fecho de $\RR$, Teremos que:
  \begin{displaymath}
    \RRb = \RR \cup \{ \Gamma(s-) : s > 0 \}.
  \end{displaymath}
\end{proposicao}
\begin{proof}
  Novamente consideremos as realizações onde $\Gamma$ é càdlàg, crescente
  e limitada em compactos.

  Fixado um $s > 0$ vamos mostrar que $\Gamma(s-) \in \RRb$. Para isso
  tomemos uma sequência $(s_n)_{n \geq 1}$ tal que $s_n \nearrow s$. Por
  definição, temos que $\Gamma(s_n) \in \RR$ para todo $n$ e
  $\Gamma(s_n) \to \Gamma(s-)$. De onde concluímos que $\Gamma(s-) \in
  \RRb$.

  Agora tomemos um $t \in \RRb \setminus \RR$. Por definição existe uma
  sequência $(t_n)_n \in \RR$ tal que $t_n \to t$. Como $t_n \in \RR$,
  existe $s_n$ tal que $\Gamma(s_n) = t_n$.

  Notemos que $(s_n)$ é limitada, assim existe uma subsequência de
  $(s_n)$ que seja convergente. Assim, sem perda de generalidade, vamos
  supor que $(s_n)$ seja convergente e denotar seu limite por $s$.

  Como $s_n \to s$, $\Gamma(s_n) \to t$ e $\Gamma$ é càdlàg, então
  temos que $t \in \{ \Gamma(s), \Gamma(s-) \}$. Já que estamos
  supondo que $t \not\in \RR$, seque que $t = \Gamma(s-)$.
\end{proof}

\begin{proposicao}
  $\RRb \setminus \RR$ é quase certamente enumerável.
\end{proposicao}
\begin{proof}
  O conjunto dos pontos onde $\Gamma$ não é contínua é $\{ \sigma_i^x:
  x \in \Nz, \, i \geq 1\}$. Assim temos que $\RRb \setminus \RR
  \subseteq \{ \Gamma(\sigma_i^x-): x \in \Nz, \, i \geq 1\}$, que é
  um conjunto enumerável.
\end{proof}

\begin{proposicao}
  $\Gamma^\infty$, quando enxergada como um processo estocástico, é
  um subordinador, isso é, um processo que toma valores em $[0,
  \infty)$, iniciado em zero, crescente, contínuo à direita, que tem
  incrementos estacionários e independentes.
\end{proposicao}
\begin{proof}

  Já mostramos que $\Gamma$ é crescente e contínua à direita, e basta
  olhar para a definição para ver que $\Gamma^\infty(0) = 0$.

  Tomemos $\FF_t$ a $\sigma$-álgebra gerada por $\{ \Gamma(s): s \in [0,
  t]\}$ e repare que para $t, s > 0$:
  \begin{displaymath}
    \Gamma(t+s) - \Gamma(t) = \sum_{x \in \Nz} \sum_{i =
      N(t)+1}^{N(t+s)} \gamma_x T_i^x.
  \end{displaymath}

  Agora usando o fato das $\{ T_i^x: x \in \Nzb, \, i \geq 1\}$ serem
  independentes e identicamente distribuídas e os processos de Poisson
  terem incrementos estacionários e independentes, teremos que essa
  quantidade é independente de $\FF_t$ e tem a mesma distribuição que
  $\Gamma(s) - \Gamma(0)$.
\end{proof}

\begin{proposicao}
  O expoente de Laplace de $\Gamma^\infty_0$ é dado por:
  \begin{displaymath}
    \Phi(u) = u \sum_{x \in \Nz} \frac{\lambda_x \gamma_x}{1 + u\gamma_x}.
  \end{displaymath}
  Isso é, para $u \geq 0$, teremos que:
  \begin{displaymath}
    \E \left[
      \exp \left\{
        -u \Gamma^\infty_0 (t)
      \right\}
    \right] = 
    \exp\left\{
      -t \Phi(u)
    \right\}.
  \end{displaymath}
\end{proposicao}


Agora temos ferramentas para calcular a dimensão de Hausdorff de
$\RR$. Mas antes vamos introduzir brevemente o que é a dimensão de
Hausdorff de um conjunto.

Para um $\epsilon > 0$ e um boreliano $A$ vamos denotar por
$\II_\epsilon(A)$ a família de todas as coberturas de $A$ por um
número enumerável de intervalos de comprimento menor que $\epsilon$.


Para um $\rho > 0$, $\epsilon > 0$ e um boreliano $A$, definimos:
\begin{equation}
  \label{eq:dim-hausdorff-eps}
  m^\rho_\epsilon(A) = \inf_{\II \in \II_\epsilon(A)} 
  \sum_{I \in \II} (\mathrm{l}(I))^\rho, 
\end{equation}
onde  $\mathrm{l}(I)$ é o comprimento do intervalo $I$.

Quando diminuímos $\epsilon$, diminuímos o número de coberturas
possíveis, assim o ínfimo aumenta. Portanto o limite de
\eqref{eq:dim-hausdorff-eps} quando $\epsilon \searrow 0$ existe e
assim faz sentido definir:
\begin{displaymath}
  m^\rho(A) = \lim_{\epsilon \searrow 0} m^\rho_\epsilon (A).
\end{displaymath}

É possível mostrar que $m^\rho(\bullet)$ é uma medida sobre os
borelianos de $\R$, medida essa que chamados de medida de Hausdorff de
dimensão $\rho$.  A medida de Hausdorff de dimensão $1$ é a medida de
Lebesgue.

Também vale que se $m^\rho(A) = 0$, então $m^{\rho^\prime}(A) = 0$ para
todo $\rho^\prime > \rho$ e, se $m^\rho(A) > 0$, então $m^{\rho^\prime}(A)
= \infty$ para todo $\rho^\prime < \rho$.

\begin{definicao}
  Definimos a dimensão de Hausdorff de um boreliano $A$ como:
  \begin{displaymath}
    \dim_H(A) = \sup \left\{ \rho > 0 : m^\rho(A) < \infty \right\}
    = \inf \left\{ \rho > 0: m^\rho(A) = 0 \right\}.
  \end{displaymath}
\end{definicao}

\begin{teorema}
  \label{teo:dim-hausdorff}
  Para todo $t > 0$, vale que quase certamente:
  \begin{equation}
    \label{eq:dim-hausdorff}
    \dim_H(\RR^\infty \cap [0, t]) = 
    \dim_H(\RRb^\infty \cap [0, t]) =
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u}.
  \end{equation}
\end{teorema}
\begin{proof}
  A primeira igualdade vem do fato de um conjunto enumerável não
  alterar a dimensão de Hausdorff de um conjunto, enquanto que a
  segunda é o Corolário 5.3 de \cite{bertoin:97}, traduzido para a
  nossa notação.
\end{proof}

\begin{proposicao}
  \label{prop:dominar-dim-haus-sup}
  Supondo que $\sup_{x\in\Nz}\lambda_x < \infty$ e que existam um
  $\delta>0$ e um $n_0 \in \Nz$ tal que $\gamma_x \leq x^{-(1+\delta)}$
  para todo $x > n_0$, vai valer que:
  \begin{equation}
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u}  \leq
    \frac{1}{1+\delta} .
  \end{equation}
\end{proposicao}

\begin{proof}
  Tomemos $H > 0$ tal que $\lambda_x \leq H$ para todo $x \in
  \Nz$. Assim teremos que para alguma constante $C$:
  \begin{align*}
    \Phi(u) &= \sum_{x \in \Nz} \frac{\lambda_x}{\frac{1}{u\gamma_x} +
      1}\\
    &\leq C + \sum_{x > n_0} \frac{H}{\frac{x^{1+\delta}}{u} + 1}\\
    &=u^{\frac{1}{1+\delta}} \left[
      \frac{C}{u^{\frac{1}{1+\delta}}} +
      \frac{1}{u^{\frac{1}{1+\delta}}} \sum_{x > n_0} \frac{H}{
        \left( \frac{x}{u^{\frac{1}{1+\delta}}}  \right)^{1+\delta}
        + 1
      }
    \right].
  \end{align*}

  Agora vamos nos concentrar na quantidade dentro dos colchetes. Para
  isso vamos fazer a mudança de variáveis $v =
  u^{\frac{1}{1+\delta}}$. Se tomarmos $v$ inteiro maior que $n_0$ e
  agruparmos os termos da série entre múltiplos de $v$, obteremos:
  \begin{align*}
    \frac{C}{v} + \frac{1}{v} \sum_{x > n_0} \frac{H}{ \left(
        \frac{x}{v} \right)^{1+\delta} + 1 }
    &\leq
    \frac{C}{v} + \sum_{y=1}^{\infty} \frac{H}{y^{1+\delta} + 1}
    \xrightarrow{v \to \infty} C^\prime,
  \end{align*}
  onde $C^\prime$ é uma constante positiva.

  Como $u$ tender para o infinito é equivalente à $v$ ir para o
  infinito e como o limite seguindo $v$ inteiro é um limitante
  superior para $\liminf$, teremos que:

  \begin{align*}
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u} &\leq
    \frac{1}{1+\delta} + \liminf_{u \to \infty} 
    \frac{\log{C^\prime}}{\log u}\\
    &= \frac{1}{1+\delta} .
    \qedhere
  \end{align*}
\end{proof}

\begin{proposicao}
\label{prop:dominar-dim-haus-inf}
  Supondo que $\inf_{x\in\Nz}\lambda_x  > 0$ e que existam um
  $\delta>0$ e um $n_0 \in \Nz$ tal que $\gamma_x \geq x^{-(1+\delta)}$
  para todo $x > n_0$, vai valer que:
  \begin{equation}
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u}  \geq
    \frac{1}{1+\delta} .
  \end{equation}
\end{proposicao}

\begin{proof}
  Tomemos $H > 0$ tal que $\lambda_x \geq H$ para todo $x \in \Nz$. Dessa
  forma:
  \begin{align*}
    \Phi(u) &= \sum_{x \in \Nz} \frac{\lambda_x}{\frac{1}{u\gamma_x} +
      1}\\
    &\geq \sum_{x > n_0}
    \frac{H}{\left(\frac{x}{u^{\frac{1}{1+\delta}}}\right)^{1+\delta} +1}.
  \end{align*}

  Portanto, tomando $u > n_0$, e agrupando a série em somas sobre os inteiros
  que estão entre os múltiplos de $u$, teremos que:
  \begin{align*}
    \Phi(u^{1+\delta}) 
    &\geq \sum_{x > n_0}
    \frac{H}{\left(\frac{x}{u}\right)^{1+\delta} +1}\\
    &\geq \sum_{x \geq u}
    \frac{H}{\left(\frac{x}{u}\right)^{1+\delta} +1}\\
    &\geq \lfloor u \rfloor \sum_{y = 1}^{\infty}
    \frac{H}{y^{1+\delta} + 1}.
  \end{align*}

  Dessa forma:
  \begin{align*}
    \frac{\log \Phi(u)}{\log u} \geq \frac{\log\lfloor
      u^{\frac{1}{1+\delta}} \rfloor }{\log u} + 
    \frac{1}{\log u} \sum_{y = 1}^{\infty}
    \frac{H}{y^{1+\delta} + 1}.
  \end{align*}

  Como essa série é convergente, ao fazer $u \to \infty$ o segundo
  termo desaparecerá, enquanto que o primeiro irá convergir para
  $\frac{1}{1+\delta}$, de onde concluímos que:
  \begin{displaymath}
    \liminf_{u \to \infty} \frac{\log \Phi (u)}{\log u} \geq
    \frac{1}{1+\delta} .
    \qedhere
  \end{displaymath}
\end{proof}

\begin{corolario}
  \label{prop:igualar-dim-haus}
  Supondo que $\inf_{x \in \Nz} \lambda_x > 0$, $\sup_{x \in \Nz}
  \lambda_x < \infty$ e que existam um $\delta > 0$ e um $n_0 \in \Nz$
  tal que $\gamma_x = x^{-(1+\delta)}$ para todo $x > n_0$, vai valer que:
  \begin{displaymath}
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u} =
    \frac{1}{1+\delta} .
  \end{displaymath}
\end{corolario}
\begin{proof}
  Esse corolário é evidente ao observar as proposições
  \ref{prop:dominar-dim-haus-sup} e \ref{prop:dominar-dim-haus-inf}.
\end{proof}


%%% Local Variables: 
%%% TeX-master: "tese"
%%% End: 
