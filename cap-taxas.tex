%% ------------------------------------------------------------------------- %%
\chapter{Taxas de Transição}
\label{cap:taxas}

Vamos começar a escrever e depois penso no que escrever aqui.

%% ------------------------------------------------------------------------- %%

\section{Continuidade}
\label{sec:continuidade}

Para deixar a nossa notação menos carregada, vamos introduzir a
seguinte definição, para $x, y \in \Nzb$, $t \geq 0$:
\begin{equation}
  p_{xy} (t) = P(X^x(t) = y).
\end{equation}
Denotaremos ainda por $P(t)$ a ``matriz'' cujas entradas sejam $p_{xy}(t)$.

Nessa seção vamos estudar a continuidade ou não continuidade dessa
função na origem.

\begin{proposicao}
  Para $x \in \Nz$,
  \begin{equation}
    \lim_{t \searrow 0}p_{xx}(t) = 1.   
  \end{equation}
\end{proposicao}
\begin{proof}
  Como $x \in \Nz$, temos que:
  \begin{displaymath}
    p_{xx}(t) \geq P( \gamma_x T^x_0 > t) = e^{-\frac{t}{\gamma_x}}
    \to 1.
  \end{displaymath}
\end{proof}

\begin{lema}
  \label{lema:deriv_gamma}
  Vale que $ \frac{\Gamma^{\infty}(t)}{t} \xrightarrow{t \searrow 0}
  c$ em probabilidade.
\end{lema}
\begin{proof}
  Fixando um $t > 0$, teremos que:
  \begin{equation}
    \label{eq:gamma_tt}
    \frac{\Gamma^{\infty}(t)}{t} = 
    c + \frac{1}{t} \sum_{x \in \Nz} \sum_{i = 1}^{N^x(t)}
    \gamma_x T^x_i
  \end{equation}

  Agora podemos calcular a transformada de Laplace da segunda parte
  dessa expressão. Fixado um $u > 0$, teremos que:
  \begin{equation}
    \label{eq:lap_vaizero}
    \E \left[ \exp\left\{
      -u \frac{1}{t} \sum_{x \in \Nz} \sum_{i=1}^{N^x(t)} \gamma_x T^x_i
    \right\} \right] =
    \exp \left\{
      - u \sum_{x \in \Nz}  \frac{\lambda_x \gamma_x}{1 +
        \frac{u \gamma_x}{t}}
    \right\} %\xrightarrow{t \searrow 0} 1.
  \end{equation}

  Essa função é monótona em $t$ e cada termo da soma
  vai a zero quando $t \searrow 0$.  Assim o teorema da convergência
  monótona nos diz que \ref{eq:lap_vaizero} converge à $1$ quanto $t
  \searrow 0$, para qualquer $u \geq 0$ fixado.

  Como a função constante igual a $1$ é a transformada de Laplace de
  uma variável aleatória que vale $0$ sempre, temos que a expressão
  \ref{eq:gamma_tt} converge em probabilidade para $c$ quando $t
  \searrow 0$.
\end{proof}

\begin{lema}
  \label{lema:deriv_inv_gamma}
  Seja $\Gamma^{*}$  a função inversa de $\Gamma^\infty$.
  Vale que $ \frac{\Gamma^{*}(t)}{t} \xrightarrow{t \searrow 0}
  \frac{1}{c}$ em probabilidade.
\end{lema}
\begin{proof}

  Fixe $t > 0$ e $\epsilon > 0$. Com probabilidade $1$, temos que $t$
  é um ponto de continuidade de $\Gamma^\infty$. Assim existe $s > 0$
  tal que $\Gamma$

\end{proof}

\begin{proposicao}
  \label{prop:continuidade}
  Caso $c > 0$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0}p_{\infty \infty}(t) = 1.    
  \end{displaymath}
\end{proposicao}

\begin{proof}
  Considere a função:
  \begin{displaymath}
    \theta(t) = \int_0^t \ind \{ X^\infty (s) = \infty \} ds.
  \end{displaymath}.
  
  Pela construção do processo, temos que $\theta(t) \geq c
  \Gamma^*(t)$, onde $\Gamma^*(t)$ é a inversa de $\Gamma^\infty(t)$.

  O Lema \ref{lema:deriv_gamma} diz que $\frac{\Gamma^\infty(t)}{t}$
  converge à $1$ em probabilidade. Assim podemos reconstruir
  $\frac{\Gamma^\infty(t)}{t}$ em um novo espaço de probabilidades de
  forma que elas convirjam quase certamente.

  Agora vamos mostrar que essa expressão converge em probabilidade
  para $1$. Para isso vamos calcular a transformada de Laplace de
  $\frac{1}{ct} \sum_{x \in \Nz} \sum_{i = 1}^{N^x(t)} \gamma_x
  T^x_i$. Fixado um $u \geq 0$, podemos calcular:



  % Assim podemos reconstruir essas variáveis em um novo espaço de
  % probabilidade de forma que elas convirjam quase certamente. Como
  % $\frac{\theta(\Gamma^\infty(t))}{\Gamma^\infty(t)} \leq 1$, usando o
  % teorema da convergência dominada teremos que $$

  % Voltar para ca depois!!!!!!


  Agora, usando o Teorema de Fubbini, teremos que:
  \begin{align*}
    1 &= \lim_{t \searrow 0} \E \left[ \frac{\theta(t)}{t} \right] \\ 
    &= \lim_{t \searrow 0} \E\left[
      \frac{1}{t} \int_0^t \ind\{ X^\infty(s) = \infty \} ds
    \right] \\
    &= \lim_{t \searrow 0} 
      \frac{1}{t} \int_0^t \E\left[ \ind\{ X^\infty(s) = \infty \}\right] ds
    \\
    &= \lim_{t \searrow 0} \frac{1}{t} \int_0^t p_{\infty \infty} (s) ds
  \end{align*}

  Portanto, usando o teorema fundamental do calculo, teremos que
  $p_{\infty \infty} (t) \xrightarrow{t \searrow 0} 0$


\end{proof}

\begin{proposicao}
  \label{prop:naocontinuidade}
  Caso $c = 0$, para todo $x \in \Nzb$ e $t > 0$, temos que $p_{x
    \infty} (t) = 0$.
\end{proposicao}
\begin{proof}
  Essa demonstração é uma adaptação direta do Lema \emph{3.15} de
  \cite{fontes:08}.

  Tome $m \in \Nz$. Vamos definir:
  \begin{align*}
    \theta_m(t) := \int_0^t \ind \left\{ X^\infty_0(s) \geq m \right\}
  \end{align*}

  Estamos interessados em calcular $\theta(t) :=
  \theta_\infty(t)$. Para isso note que $\theta_m(t)$ é decrescente em
  $m$. Dessa forma, se $\Xi$ for a função inversa de $\Gamma$, teremos
  que:
  \begin{align*}
    \theta_\infty(t) \leq \theta_m(t) = \sum_{x \geq m}
    \sum_{i=1}^{\Xi(t)} \gamma_x T^x_i.
  \end{align*}


  Como $\theta_m(t) \leq t$, essa série (em $x$) converge. Assim temos
  que $\lim_{m\to\infty} \theta_m(t) = 0$ \qc. E portanto $\theta(t) =
  0$ \qc. Dessa forma também vale que $\E[\theta(t)] = 0$. Agora
  usando o teorema de Fubbini, teremos:
  \begin{align*}
    0 &= \E\left[ \int_0^t \ind \left\{ X^\infty_0(s) = \infty
      \right\} ds \right]\\
    &= \int_0^t P \left\{ X^\infty_0(s) = \infty
    \right\} ds\\
    &= \int_0^t p_{\infty \infty} (s) ds
  \end{align*}

  Dessa forma concluímos que $p_{\infty \infty} (t) = 0$ para Lebesgue
  quase todo $t$.

  Agora tome um $x \in \Nz$ arbitrário. Condicionando no valor de
  $T^x_0$, teremos que:
  \begin{align*}
    p_{x \infty} (t) = \int_0^t p_{\infty \infty} (t-s)
    \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds.\\
  \end{align*}

  Portanto para todo $t > 0$ vale que $p_{x \infty} (t) = 0$.

  Assim, usando a propriedade de Markov do processo, teremos que para
  todos $t> 0$ e $0 < s < t$:
  \begin{align*}
    p_{\infty \infty} (t) &= \sum_{x \in \Nzb} p_{\infty x}(s) p_{x
      \infty} (t-s)\\
    &= p_{\infty \infty} (s) p_{\infty \infty} (t-s).
  \end{align*}

  Agora suponha por absurdo que exista um $t \geq 0$ tal que
  $p_{\infty \infty} (t) > 0$. Então teríamos que $p_{\infty
    \infty}(s) > 0$ para todo $s \in (0, t)$, conjunto esse que tem
  medida de Lebesgue positiva.
\end{proof}


%% ------------------------------------------------------------------------- %%

\section{Matriz Q}
\label{sec:matrizq}

Nessa seção vamos calcular a matriz de taxas de transição do nosso
processo. Ela pode ser definida pelo limite:
\begin{displaymath}
  Q = \lim_{t \searrow 0} \frac{P(t) - I}{t} 
\end{displaymath}.

Vamos mostrar que, para o caso $c > 0$, essa matriz vale:
\begin{displaymath}
  Q = \left(
    \begin{array}{ccccc}
      -\frac{1}{\gamma_1} & 0 & 0 & \cdots & \frac{1}{\gamma_1}\\
      0 & -\frac{1}{\gamma_2} & 0 & \cdots & \frac{1}{\gamma_2}\\
      0 & 0 & -\frac{1}{\gamma_3} & \cdots & \frac{1}{\gamma_3}\\
      \vdots & \vdots & \vdots & \vdots & \ddots \\
      \frac{\lambda_1}{c} & \frac{\lambda_2}{c} &
      \frac{\lambda_3}{c} & \cdots & -\infty\\
    \end{array}
  \right).
\end{displaymath}

Equanto que no caso $c=0$, teremos:
\begin{displaymath}
  Q = \left(
    \begin{array}{ccccc}
      -\frac{1}{\gamma_1} & 0 & 0 & \cdots & 0\\
      0 & -\frac{1}{\gamma_2} & 0 & \cdots & 0\\
      0 & 0 & -\frac{1}{\gamma_3} & \cdots & 0\\
      \vdots & \vdots & \vdots & \vdots & \ddots \\
      \infty & \infty & \infty & \cdots & -\infty\\
    \end{array}
  \right).
\end{displaymath}

\begin{proposicao}
  \label{prop:taxa-x-y}
  Sejam $x, y \in \Nz$, $x \neq y$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{xy}(t)}{t} = 0.
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align*}
    \frac{p_{x y} (t)}{t}
    &= \frac{1}{t}\int_{0}^{t} P( X^x(t) = y |
    \gamma_x T_0^x = s) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{1}{t} \int_{0}^{t} P( X^\infty(t-s) = y ) \frac{1}{\gamma_x}
    e^{-\frac{s}{\gamma_x}} ds \\
    &\leq \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(t-s) ds \\
    &= \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(t-s) ds \\
    &= \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(s) ds
    \xrightarrow{t \searrow 0} \lim_{t \searrow 0} \frac{p_{\infty y}
      (t)}{\gamma_x} = 0.
  \end{align*}

\end{proof}

\begin{proposicao}
  \label{prop:taxa-x-x}
  Para $x \in \Nz$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{xx}(t) - 1}{t} = -\frac{1}{\gamma_x}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align*}
    p_{xx} (t)
    &= P( \gamma_x T_0^x > t) + 
    \int_{0}^{t} P( X^x(t) = y |
    \gamma_x T_0^x = s) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &= e^{-\frac{t}{\gamma_x}} + 
    \int_{0}^{t} P( X^\infty(t-s) = y) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
  \end{align*}
  Fazendo contas análogas às da proposição anterior, podemos
  mostrar que o segundo termo dessa soma, dividido por $t$ vai para
  zero quando $t \searrow 0$. Tratando o primeiro termo agora, temos
  que:
  \begin{displaymath}
    \frac{e^{-\frac{t}{\gamma_x}} - 1}{t} \xrightarrow{t \searrow 0}
    -\frac{1}{\gamma_x}.
  \end{displaymath}

  Dessa forma:
  \begin{displaymath}
     \lim_{t \searrow 0} \frac{p_{xx} (t) - 1}{t} = -\frac{1}{\gamma_x}
  \end{displaymath}
  
\end{proof}

\begin{proposicao}
  Para $x \in \Nz$, vale que:
  \label{prop:taxa-inf-x}
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{\infty x}(t)}{t} = \begin{cases}
      \frac{\lambda_x}{c} & \textrm{ se } c > 0 \\
      \infty & \textrm{ se } c = 0 \\
    \end{cases}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align}
    \frac{p_{\infty x}}{t} &= \frac{1}{t} P \left( \bigcup_{i =
        1}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right) \notag \\
    &= \frac{P \left( \Gamma^\infty (\sigma^x_1 -) \leq t <
      \Gamma^\infty(\sigma^x_1) \right)}{t} +
    \frac{1}{t} P \left( \bigcup_{i =
        2}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right) \notag \notag \\
    \label{erros_taxa_inf}
    &= \frac{P \left( \Gamma^\infty (\sigma^x_1 -) \leq t \right)}{t} -
    \frac{P \left( \Gamma^\infty (\sigma^x_1) \leq t \right)}{t} +
    \frac{1}{t} P \left( \bigcup_{i =
        2}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right)
  \end{align}

  Agora vamos mostrar que o segundo termo dessa soma vai a zero quando
  $t \searrow 0$.
  \begin{align*}
    \frac{P (\Gamma^\infty (\sigma^x_1) \leq t)}{t}
    &= \frac{1}{t} P \left(
      \gamma_x T^x_1 + 
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i +
      c\sigma^x_1
      \leq t
    \right) \\
    &\leq \frac{1}{t} P \left(
      \gamma_x T^x_1 + 
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t
    \right)\\
    &= \frac{1}{t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t - s
      \middle\vert \gamma_x T^x_1 = s
    \right) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &\leq \frac{1}{\gamma_x t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t - s
    \right) ds\\
    &\leq \frac{1}{\gamma_x t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq s
    \right) ds\\
    &\xrightarrow{t\searrow0} \frac{1}{\gamma_x} P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      = 0 \right) = 0
  \end{align*}

  Agora note que, como $\Gamma^\infty$ é não decrescente, o evento no
  terceiro termo de \eqref{erros_taxa_inf} está contido no evento do
  segundo termo. Assim como mostramos que o segundo termo vai a zero,
  teremos que o terceiro também irá.

  Agora resta calcular o limite do primeiro termo. Ao fazer isso
  estaremos também calculando o limite desejado.


  Faremos isso usando o Teorema Tauberiano, que relaciona o
  comportamente a função de distribuição de uma variável aleatória
  positiva perto do zero com o comportamento de sua transformada de
  Laplace no infinito.  Seguiremos o enunciado do
  Teorema\emph{XIII.5.1} de \cite{fellerv2}.

  O primeiro passo é calcular a transformada da Laplace de
  $\Gamma^\infty(\sigma^x_1-)$. Assim para um $u \geq 0$, podemos
  calcular:
  \begin{align*}
    \phi (u) := \E \left[ e^{-u \Gamma^\infty (\sigma^x_1-)}  \right] =
    \lambda_x \left( \lambda_x + uc + u \sum_{y \neq x}
      \frac{\lambda_x \gamma_x}{1 + u\gamma_x}  \right)^{-1}
  \end{align*}

  Por enquanto vamos nos concentrar no caso $c > 0$. Nesse caso
  teremos que para $u, v > 0$:
  \begin{align*}
    \frac{\phi(uv)}{\phi (u)} &= \frac{\lambda_x + uc + \sum_{y \neq
        x} \frac{u \lambda_x\gamma_x}{1 + u \gamma_x}} {\lambda_x + u
      v c + \sum_{y \neq x} \frac{u v
        \lambda_x\gamma_x}{1 + u v \gamma_x}} \\
    &= \frac{\frac{\lambda_x}{u} + c + \sum_{y \neq x}
      \frac{\lambda_x\gamma_x}{1 + u \gamma_x}} {\frac{\lambda_x}{u} +
      v c + \sum_{y \neq x} \frac{v
        \lambda_x\gamma_x}{1 + u v \gamma_x}}. \\
  \end{align*}

  Agora note que quando $u$ vai ao infinito, os termos de cada uma das
  somas vai a zero monotonamente com $u$, assim usando o teorema da
  convergência monótona, teremos que:
  \begin{align*}
      \lim_{u \to \infty} \frac{\phi(uv)}{\phi (u)} &= \frac{1}{v}.
  \end{align*}

  Assim verificamos a condição do teorema, e temos que:
  \begin{align*}
    \lim_{t \searrow 0} \frac{P( \Gamma^\infty(\sigma^x_1-) \leq
      t)}{\phi(\frac{1}{t})} = 1
  \end{align*}

  Assim se mostrarmos que $\frac{\phi(\frac{1}{t})}{t}$ converge para
  $\frac{\lambda_x}{c}$, teremos mostrado o nosso resultado.

  \begin{align*}
    \frac{\phi(\frac{1}{t})}{t} &= \frac{1}{t} \lambda_x \left(
      \lambda_x + \frac{c}{t} + \sum_{y \neq x} \frac{1}{t}
      \frac{\lambda_x \gamma_x}{1 + \frac{\gamma_x}{t}} \right)^{-1} \\
    &= \lambda_x \left( t\lambda_x + c + \sum_{y \neq x}
      \frac{\lambda_x \gamma_x}{1 + \frac{\gamma_x}{t}} \right)^{-1}.
  \end{align*}

  Novamente cada termo da soma vai a zero monotonamente quanto $t
  \searrow 0$, assim pelo teorema da convergêcia monótona, teremos que
  $\lim_{t \searrow 0} \frac{\phi(\frac{1}{t})}{t} =
  \frac{\lambda_x}{c}$.

  Agora vamos tratar o caso $c = 0$. Para isso note que nossa
  construção do processo K permite que acoplemos várias versões do
  processo, com $c$ diferentes em um mesmo espaço de
  probabilidade. Basta usar os mesmos processos e Poisson e variáveis
  exponenciais para todos eles. Assim vamos colocar um índice $c$ em
  $\Gamma^y_c$ para denotar qual valor de $c$ estamos trabalhando.

  Dessa forma teremos que para todo $y \in \Nzb$, $\Gamma^y_c$ é
  crescente com $c$, e portanto $P ( \Gamma^\infty_c(\sigma^x_1-) \leq
  t)$ é monótona em $c$.

  Dessa forma teremos que, para todo $c > 0$:
  \begin{align*}
    \liminf_{t \searrow 0} \frac{P ( \Gamma^\infty_0(\sigma^x_1-) \leq
      t)}{t} &\geq \liminf_{t \searrow 0} \frac{P (
      \Gamma^\infty_c(\sigma^x_1-) \leq t)}{t}
    = \frac{\lambda_x}{c}
  \end{align*}

  Assim tomando $c > 0$ cada vez menores, concluíremos que $\frac{P (
    \Gamma^\infty_0(\sigma^x_1-))}{t} \xrightarrow{t \searrow 0}
  \infty$
\end{proof}

\begin{proposicao}
  \label{prop:taxa-inf-inf}
  Vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{\infty \infty}(t) - 1}{t} = -\infty
  \end{displaymath}
\end{proposicao}
\begin{proof}
  O caso $c = 0$ é trivial pela Proposição
  \ref{prop:naocontinuidade}. Assim vamos supor que $c > 0$.
  \begin{align*}
    \frac{p_{\infty \infty} (t) - 1}{t} &= - \sum_{x \in \Nz}
      \frac{p_{\infty x} (t)}{t}.
  \end{align*}

  Cada termo dessa soma converge à $\frac{\lambda_x}{c}$, e estamos
  supondo que a soma dessa série diverge para $\infty$, de onde
  concluímos que:
  \begin{align*}
    \lim_{t \searrow 0}\frac{p_{\infty \infty} (t) - 1}{t} &= - \infty
  \end{align*}
\end{proof}

\begin{proposicao}
  \label{prop:taxa-x-inf}
  Para $x \in \Nz$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{x \infty}(t)}{t} = \begin{cases}
      \frac{1}{\gamma_x} & \textrm{ se } c > 0 \\
      0 & \textrm{ se } c = 0 \\
    \end{cases}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  Novamente o caso $c = 0$ é trivial em vista da Proposição
  \ref{prop:naocontinuidade}. Assim vamos supor que $c > 0$.
  \begin{align*}
    \frac{p_{x \infty}}{t} &= \frac{1}{t} \int_0^t P(X^x (t) = \infty | \gamma_x
    T^x_0 = s) \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{1}{t} \int_0^t p_{\infty \infty} (t-s)
    \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{e^{-t}}{\gamma_x} \frac{1}{t} \int_0^t p_{\infty \infty} (s)
    e^{\frac{s}{\gamma_x}} ds\\
  \end{align*}

  Como $c>0$, pela Proposição \ref{prop:continuidade}, $p_{\infty
    \infty} (t) \xrightarrow{t \searrow 0} 1$. De onde concluímos que:
   \begin{align*}
    \frac{p_{x \infty}(t)}{t} \xrightarrow{t \searrow 0} 
    \frac{1}{\gamma_x}
  \end{align*}
\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Medida invariante}
\label{sec:invariante}

O objetivo dessa seção é mostrar que a seguinte distrubuição de
probabilidade é uma medida invariante para o processo K.

\begin{equation}
  \label{eq:invariante}
  \pi(x) = \begin{cases}
    \frac{\lambda_x \gamma_x}{c + \sum_{y \in \Nz} \lambda_y \gamma_y}
    & \textrm{ se } x \in \Nz \\
    \frac{c}{c + \sum_{y \in \Nz} \lambda_y \gamma_y}
    & \textrm{ se } x = \infty \\
  \end{cases}
\end{equation}

\begin{lema}
  \label{lema:cont_direita_prob}
  Para todo $x, y \in \Nzb$ e $t > 0$, temos que:
  \begin{align}
    \lim_{s \searrow t} p_{x y} (s) = p_{x y} (t)
  \end{align}
\end{lema}
\begin{proof}
  O Corolário \ref{cor:proc_cadlag} nos garante que o processo K é
  Càdlàg na topologia definida na Seção \ref{sec:topologia}.

  Note que para $y \in \Nz$, a função $f_y: \Nzb \to \R$ definida por
  $f_y(x) = \ind\{ x = y\}$ é contínua nessa topologia. Assim temos
  que $\lim_{s \searrow t} f_y(X^x(s)) = f_y (X^x(t))$ \qc.

  Como $f_y$ é uma função limitada, o teorema da convergência dominada
  nos garante que:
  \begin{displaymath}
    p_{x y} (s) = \E[f_y(X^x(s))] \xrightarrow{s \searrow t} \E[f_y
    (X^x(t))] = p_{x y} (t).
  \end{displaymath}

  Para o caso $y = \infty$, vamos dividir em dois casos.  Caso $c =
  0$, temos que $p_{x \infty} (s) = 0$ para todo $s > 0$, assim essa
  função é obviamente contínua em todo ponto $t > 0$.

  Para o caso $c > 0$, pelo fato do processo K ser Markoviano, para um
  $s > t$ teremos que:

  \begin{align*}
    p_{x \infty} (s) &= \sum_{y \in \Nzb} p_{x y} (t) p_{y \infty}
    (s - t) \\
    &= p_{x \infty} (t) p_{\infty \infty} (s - t) + 
    \sum_{y \in \Nz} p_{x y} (t) p_{y \infty} (s - t). \\
  \end{align*}


  Ao fazer $s \searrow t$, pela Proposição \ref{prop:continuidade},
  temos que o primeiro termo converge para $p_{x \infty}(t)$, enquanto
  que cada termo da série do segundo termo converge à zero. Como
  podemos dominar cada termo dessa série por $p_{x y} (t)$ que é
  somável em $y$, usando o teorema da convergência dominada concluímos
  que:
  \begin{displaymath}
    \lim_{s \searrow t} p_{x \infty} (s) = p_{x \infty} (t)
  \end{displaymath} 
\end{proof}

\begin{proposicao}
  O processo K tem uma única medida invariante.
\end{proposicao}
\begin{proof}
  Fixe um número real $h > 0$ e considere uma cadeia de Markov em
  tempo discreto $(Y^{y, h}_n)_{n \geq 0}$ dada por $Y^{y, h}_n =
  X^y(n h)$.

  Fixado um estado $y \in \Nz$, defina $M = \min_{i \geq 1} \{ T_i^y >
  h \}$. $M$ é uma variável aleatória com distribuição geométrica com
  probabilidade de sucesso dada por $e^{-\frac{h}{\gamma_y}}$. Agora
  temos que $X_t^y = y$ para todo $t \in [\Gamma^y(\sigma_M^x-),
  \Gamma^y(\sigma_M^x-) + T_M^y)$, e como $T_M^y > h$, existe um
  múltimo de $h$ nesse intervalo. Assim isso vai corresponder à um
  ponto onde $Y_n^{y, h} = y$, assim a esperança de tempo de retorno à
  um estado em $(Y_n)$ é finita.


  Para $x, y \in \Nz$, vale que $P (Y^{x, h}_1 = y) > 0$. Temos ainda
  que para todo $x \in \Nzb$ $P (Y^{x, h}_1 = \infty ) > 0$ se $c >
  0$. Enquanto que $P (Y^{x, h}_1 = \infty) = 0$ para $c = 0$. Assim
  a cadeia é irredutível no caso $c > 0$ e no caso $c = 0$ temos que
  $\Nz$ é uma classe de comunicação fechada, enquanto que $\{\infty\}$
  é uma classe de comuniçação aberta.


  Em todo caso, para cada escolha de $h$, existe uma única medida de
  probabilidade $\mu_h$ tal que para todo $n \geq 1$:
  \begin{displaymath}
    \mu_h (y) = \sum_{x \in \Nzb} \mu_h(x) P ( Y_n^ {x, h} = y) =
    \sum_{x \in \Nzb} \mu_h(x) P ( X^x (nh) = y)
  \end{displaymath}

  Agora fixe um inteiro $m \geq 1$ e um número real $h > 0$, temos que:
  \begin{align*}
    \mu_h (y) &= \sum_{x \in \Nzb} \mu_h(x) P ( Y_{m}^ {x, h} = y)\\
    &= \sum_{x \in \Nzb} \mu_h(x) P ( X^x (m h) = y) \\
    &= \sum_{x \in \Nzb} \mu_h(x) P ( Y_{1}^ {x, m h} = y).
  \end{align*}

  Portanto $\mu_h$ é uma medidada invariante para $(Y^{\bullet, mh}_n)_{n
    \geq 0}$ e como ela é única, teremos que $\mu_h = \mu_{m h}$.

  Assim concluímos que $\mu_{h} = \mu_1$ para todo $h > 0$ racional.
  Agora fixado um $h > 0$ irracional, tome uma sequencia de números
  racionais $(h_n)$ que convirjam para $h$ pela direita. Teremos que:
  \begin{align*}
    \mu_1(y) &= \sum_{x \in \Nzb} \mu_1(x) P ( Y_{1}^ {x, h_n} = y)\\
    &= \sum_{x \in \Nzb} \mu_1(x) P ( X^x(h_n) = y)\\
    &= \sum_{x \in \Nzb} \mu_1(x) p_{x y} (h_n).    
  \end{align*}
  
  Cada termo dessa soma é dominado por $\mu_1(x)$, que tem soma $1$,
  assim usando o Lema \ref{lema:cont_direita_prob} e o teorema da
  convergência dominada, concluímos que:
  \begin{align*}
    \mu_1 (y) &= \sum_{x \in \Nzb} \mu_1(x) p_{x y} (h)\\
    &= \sum_{x \in \Nzb} \mu_1(x) P(Y_1^{x, h} = y)\\
  \end{align*}
  ou seja, $\mu_1$ é uma probabilidade invariante para $(Y_n^{\bullet,
    h})_n $ e como a probabilidade invariante é única, concluímos que
  $\mu_1 = \mu_h$ para todo $h > 0$.
\end{proof}


\begin{proposicao}
  A probabilidade $\pi$ definida em \eqref{eq:invariante} é a medida
  invariante do processo K, caso $c > 0$.
\end{proposicao}
\begin{proof}
  Seja $\mu$ a única probabilidade invariante do nosso processo que
  encontramos na última proposição. Para todo $t > 0$, vale que:
  \begin{gather*}
    \frac{\mu(y)}{t} = \sum_{x \in \Nzb} \mu(x) \frac{p_{x y}
      (t)}{t}\\
  \end{gather*}

  Rearranjando os termos, teremos que para todo $y \in \Nz$:
  \begin{displaymath}
    0 = \mu(\infty) \frac{p_{\infty y}(t)}{t} + 
    \mu(y) \frac{p_{y y}(t) - 1}{t} + 
    \sum_{\substack{x \in \Nz \\ x \neq y}} \mu(x) \frac{p_{x
        y}(t)}{t}.
  \end{displaymath}

  As Proposições \ref{prop:taxa-x-x} e \ref{prop:taxa-inf-x} nos
  fornecem os limites dos dois primeiros termos, enquanto que a
  Proposição \ref{prop:taxa-x-y} nos garante que $\frac{p_{x
      y}(t)}{t}$ converge à zero quando $t \searrow 0$ para cada $x
  \in \Nz \setminus \{y\}$.

  Agora note que $p_{x y} (t) \leq P(\Gamma^x (\sigma_1^y -) \leq t)
  \leq P(\Gamma^\infty (\sigma_1^y -) \leq t)$. Agora na demonstração
  da Proposição \ref{prop:taxa-inf-x} mostramos que o limite dessa
  última quantidade, sobre $t$ converge à $\frac{\lambda_y}{c}$. Assim
  concluímos existe uma constante $K$ tal que para $t$ pequeno o
  suficiente e para todo $x \in \Nz\setminus \{ y\}$, vale que
  $\mu(x)\frac{p_{x y}(t)}{t} \leq K \mu(x)$.

  Como $\sum_{x} K \mu(x) \leq K$, o teorema da convergência dominada
  nos garante que:
  \begin{displaymath}
    \sum_{\substack{x \in \Nz \\ x \neq y}} \mu(x)
    \frac{p_{x y}(t)}{t} \xrightarrow{t \searrow 0} 0.
  \end{displaymath}

  Assim obtemos que, para cada $y \in \Nz$:
  \begin{gather*}
    0 = \mu(\infty) \frac{\lambda_y}{c} - \mu(y) \frac{1}{\gamma_y}.
  \end{gather*}

  Juntando a esse sistema de equações a restrição de que
  $\sum_{x\in\Nzb}\mu(x) = 1$, teremos um sistema com uma única
  solução, que é a $\pi$ dada em \eqref{eq:invariante}.
\end{proof}

\begin{lema}
  \label{lema:c_continuo}
  Para todo $t > 0$ e $y \in \Nzb$, vale que:
  \begin{displaymath}
    \lim_{c\searrow 0} X_c^y(t) = X_0^y(t)
  \end{displaymath}
  quase certamente.
\end{lema}
\begin{proof}
  Observe que $\Gamma^y_c (t) - \Gamma^y_0(t) = ct \xrightarrow{c
    \searrow 0} 0$.

  Pela Proposição \ref{prop:naocontinuidade}, temos que $P(X^y_0(t) =
  \infty) = 0$.

  Ainda sabemos que $\Gamma^y_0(\sigma^x_i-)$ é uma variável aleatória
  contínua. Assim $P( \Gamma^y_0(\sigma^x_i-) = t) = 0$.

  Assim vamos fixar uma realização do processo onde
  $X^y_0(t) = x < \infty$ e $\Gamma^y_0(\sigma^x_i-) \neq t$ para todo
  $x \in \Nz$ e $i \geq 1$. Tais realizações tem probabilidade $1$.


  Agora há duas possibilidades. A primeira é que $t < \gamma_y
  T^y_0$. Nesse caso teremos que para qualquer $c \geq 0$, vale que
  $X^y_c = y$. Assim a convergência é trivial.

  A segunda possibilidade é que $t \geq \gamma_y T^y_0$. Nesse caso,
  existe um $i \geq 1$ tal que $\Gamma^y_0(\sigma^x_i-) < t <
  \Gamma^y_0(\sigma^x_i)$. Portanto para $c > 0$ pequeno o suficiente,
  teremos que $\Gamma^y_c(\sigma^x_i-) < t < \Gamma^y_c(\sigma^x_i)$ e
  assim $X_c^y(t) = x$.
\end{proof}

\begin{proposicao}
  A probabilidade $\pi$ definida em \eqref{eq:invariante} é a medida
  invariante do processo K, caso $c = 0$.
\end{proposicao}
\begin{proof}

  Vamos denotar a probabilidade definida em \eqref{eq:invariante} por
  $\pi_c$

  Mostramos na proposição \ref{prop:naocontinuidade} que para todo $y
  \in \Nzb$, $P(X^y(t) = \infty) = 0$. Assim é trivial verificar que
  $0 = \pi_0(\infty) = \sum_{y\in\Nzb} \pi_0(y) P(X^y(t) =
  \infty)$.

  Note que para todo $x \in \Nzb$, $\pi_c(x) \xrightarrow{c\searrow0}
  \pi_0(x)$.

  Pelo Lema \ref{lema:c_continuo}, e usando o fato que, para $x \in
  \Nz$ fixado, a função $f_x(y) = \ind\{y = x\}$ é contínua e
  limitada, teremos que $P(X^y_c (t) = x) = \E [f_x(X^y_c(t))]
  \xrightarrow{c\searrow0} \E [f_x(X^y_c(t))] = P(X^y_0(t) = x)$.

  Como $\pi_c$ é invariante para $(X^y_c)$, teremos que para todo $x
  \in \Nz$ e $t \geq 0$:

  \begin{align*}
    \pi_c(x) &= \sum_{y \in \Nzb} \pi_c (y) P(X^y_c (t) = x).\\
  \end{align*}

  Cada termo dessa soma pode ser dominado por $\pi_0(y)$ se $y <
  \infty$ e por $1$ se $y = \infty$, assim tomando limite $c\searrow0$
  e usando o teorema da convergência dominada, teremos que:

  \begin{align*}
    \pi_0(x) &= \sum_{y \in \Nzb} \pi_0 (y) P(X^y_0 (t) = x).\\
  \end{align*}

  De onde concluímos que $\pi_0$ é uma medida invariante para o o
  processo K no caso $c = 0$.
\end{proof}


%%% Local Variables: 
%%% TeX-master: "tese"
%%% End: 
