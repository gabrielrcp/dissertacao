%% ------------------------------------------------------------------------- %%
\chapter{Taxas de Transição}
\label{cap:taxas}

Vamos começar a escrever e depois penso no que escrever aqui.

%% ------------------------------------------------------------------------- %%

\section{Continuidade}
\label{sec:continuidade}

Para deixar a nossa notação menos carregada, vamos introduzir a
seguinte definição, para $x, y \in \Nzb$, $t \geq 0$:
\begin{equation}
  p_{xy} (t) = P(X^x(t) = y).
\end{equation}
Denotaremos ainda por $P(t)$ a ``matriz'' cujas entradas sejam $p_{xy}(t)$.

Nessa seção vamos estudar a continuidade ou não continuidade dessa
função na origem.

\begin{proposicao}
  Para $x \in \Nz$,
  \begin{equation}
    \lim_{t \searrow 0}p_{xx}(t) = 1.   
  \end{equation}
\end{proposicao}
\begin{proof}
  Como $x \in \Nz$, temos que:
  \begin{displaymath}
    p_{xx}(t) \geq P( \gamma_x T^x_0 > t) = e^{-\frac{t}{\gamma_x}}
    \xrightarrow{t \searrow 0} 1.
  \end{displaymath}
\end{proof}

\begin{lema}
  \label{lema:deriv_gamma}
  Vale que $ \frac{\Gamma^{\infty}(t)}{t} \xrightarrow{t \searrow 0}
  c$ em probabilidade.
\end{lema}
\begin{proof}
  Fixando um $t > 0$, teremos que:
  \begin{equation}
    \label{eq:gamma_tt}
    \frac{\Gamma^{\infty}(t)}{t} = 
    c + \frac{1}{t} \sum_{x \in \Nz} \sum_{i = 1}^{N^x(t)}
    \gamma_x T^x_i
  \end{equation}

  Agora podemos calcular a transformada de Laplace da segunda parte
  dessa expressão. Fixado um $u > 0$, teremos que:
  \begin{equation}
    \label{eq:lap_vaizero}
    \E \left[ \exp\left\{
      -u \frac{1}{t} \sum_{x \in \Nz} \sum_{i=1}^{N^x(t)} \gamma_x T^x_i
    \right\} \right] =
    \exp \left\{
      - u \sum_{x \in \Nz}  \frac{\lambda_x \gamma_x}{1 +
        \frac{u \gamma_x}{t}}
    \right\} %\xrightarrow{t \searrow 0} 1.
  \end{equation}

  Essa função é monótona em $t$ e cada termo da soma
  vai a zero quando $t \searrow 0$.  Assim o teorema da convergência
  monótona nos diz que \eqref{eq:lap_vaizero} converge à $1$ quanto $t
  \searrow 0$, para qualquer $u \geq 0$ fixado.

  Como a função constante igual a $1$ é a transformada de Laplace de
  uma variável aleatória que vale $0$ sempre, temos que a expressão
  \eqref{eq:gamma_tt} converge em probabilidade para $c$ quando $t
  \searrow 0$.
\end{proof}

\begin{lema}
  \label{lema:deriv_inv_gamma}
  Seja $\Xi$  a função inversa de $\Gamma^\infty$.
  Vale que $ \frac{\Xi(t)}{t} \xrightarrow{t \searrow 0}
  \frac{1}{c}$ em probabilidade.
\end{lema}
\begin{proof}

  Fixe $t > 0$ e $\epsilon > 0$. Com probabilidade $1$, temos que $t$
  é um ponto de continuidade de $\Gamma^\infty$. Assim existe $s > 0$
  tal que $\Gamma$

\end{proof}

\begin{proposicao}
  \label{prop:continuidade}
  Caso $c > 0$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0}p_{\infty \infty}(t) = 1.    
  \end{displaymath}
\end{proposicao}

\begin{proof}
  Considere a função:
  \begin{displaymath}
    \theta(t) = \int_0^t \ind \{ X^\infty (s) = \infty \} ds.
  \end{displaymath}.
  
  Pela construção do processo, temos que $\theta(t) \geq c
  \Gamma^*(t)$, onde $\Gamma^*(t)$ é a inversa de $\Gamma^\infty(t)$.

  O Lema \ref{lema:deriv_gamma} diz que $\frac{\Gamma^\infty(t)}{t}$
  converge à $1$ em probabilidade. Assim podemos reconstruir
  $\frac{\Gamma^\infty(t)}{t}$ em um novo espaço de probabilidades de
  forma que elas convirjam quase certamente.

  Agora vamos mostrar que essa expressão converge em probabilidade
  para $1$. Para isso vamos calcular a transformada de Laplace de
  $\frac{1}{ct} \sum_{x \in \Nz} \sum_{i = 1}^{N^x(t)} \gamma_x
  T^x_i$. Fixado um $u \geq 0$, podemos calcular:


  % Assim podemos reconstruir essas variáveis em um novo espaço de
  % probabilidade de forma que elas convirjam quase certamente. Como
  % $\frac{\theta(\Gamma^\infty(t))}{\Gamma^\infty(t)} \leq 1$, usando o
  % teorema da convergência dominada teremos que $$

  % Voltar para ca depois!!!!!!


  Agora, usando o Teorema de Fubini, teremos que:
  \begin{align*}
    1 &= \lim_{t \searrow 0} \E \left[ \frac{\theta(t)}{t} \right] \\ 
    &= \lim_{t \searrow 0} \E\left[
      \frac{1}{t} \int_0^t \ind\{ X^\infty(s) = \infty \} ds
    \right] \\
    &= \lim_{t \searrow 0} 
      \frac{1}{t} \int_0^t \E\left[ \ind\{ X^\infty(s) = \infty \}\right] ds
    \\
    &= \lim_{t \searrow 0} \frac{1}{t} \int_0^t p_{\infty \infty} (s) ds
  \end{align*}

  Portanto, usando o teorema fundamental do calculo, teremos que
  $p_{\infty \infty} (t) \xrightarrow{t \searrow 0} 0$


\end{proof}

\begin{proposicao}
  \label{prop:naocontinuidade}
  Caso $c = 0$, para todo $x \in \Nzb$ e $t > 0$, temos que $p_{x
    \infty} (t) = 0$.
\end{proposicao}
\begin{proof}
  Essa demonstração é uma adaptação direta do Lema \emph{3.15} de
  \cite{fontes:08}.

  Tome $m \in \Nz$. Vamos definir:
  \begin{align*}
    \theta_m(t) := \int_0^t \ind \left\{ X^\infty_0(s) \geq m \right\}
  \end{align*}

  Estamos interessados em calcular $\theta(t) :=
  \theta_\infty(t)$. Para isso note que $\theta_m(t)$ é decrescente em
  $m$. Dessa forma, se $\Xi$ for a função inversa de $\Gamma$, teremos
  que:
  \begin{align*}
    \theta_\infty(t) \leq \theta_m(t) = \sum_{x \geq m}
    \sum_{i=1}^{\Xi(t)} \gamma_x T^x_i.
  \end{align*}


  Como $\theta_m(t) \leq t$, essa série (em $x$) converge. Assim temos
  que $\lim_{m\to\infty} \theta_m(t) = 0$ \qc. E portanto $\theta(t) =
  0$ \qc. Dessa forma também vale que $\E[\theta(t)] = 0$. Agora
  usando o teorema de Fubini, teremos:
  \begin{align*}
    0 &= \E\left[ \int_0^t \ind \left\{ X^\infty_0(s) = \infty
      \right\} ds \right]\\
    &= \int_0^t P \left\{ X^\infty_0(s) = \infty
    \right\} ds\\
    &= \int_0^t p_{\infty \infty} (s) ds
  \end{align*}

  Dessa forma concluímos que $p_{\infty \infty} (t) = 0$ para Lebesgue
  quase todo $t$.

  Agora tome um $x \in \Nz$ arbitrário. Condicionando no valor de
  $T^x_0$, teremos que:
  \begin{align*}
    p_{x \infty} (t) = \int_0^t p_{\infty \infty} (t-s)
    \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds.\\
  \end{align*}

  Portanto para todo $t > 0$ vale que $p_{x \infty} (t) = 0$.

  Assim, usando a propriedade de Markov do processo, teremos que para
  todos $t> 0$ e $0 < s < t$:
  \begin{align*}
    p_{\infty \infty} (t) &= \sum_{x \in \Nzb} p_{\infty x}(s) p_{x
      \infty} (t-s)\\
    &= p_{\infty \infty} (s) p_{\infty \infty} (t-s).
  \end{align*}

  Agora suponha por absurdo que exista um $t \geq 0$ tal que
  $p_{\infty \infty} (t) > 0$. Então teríamos que $p_{\infty
    \infty}(s) > 0$ para todo $s \in (0, t)$, conjunto esse que tem
  medida de Lebesgue positiva.
\end{proof}


%% ------------------------------------------------------------------------- %%

\section{Matriz Q}
\label{sec:matrizq}

Nessa seção vamos calcular a matriz de taxas de transição do nosso
processo. Ela pode ser definida pelo limite:
\begin{displaymath}
  Q = \lim_{t \searrow 0} \frac{P(t) - I}{t} 
\end{displaymath}.

Vamos mostrar que, para o caso $c > 0$, essa matriz vale:
\begin{displaymath}
  Q = \left(
    \begin{array}{ccccc}
      -\frac{1}{\gamma_1} & 0 & 0 & \cdots & \frac{1}{\gamma_1}\\
      0 & -\frac{1}{\gamma_2} & 0 & \cdots & \frac{1}{\gamma_2}\\
      0 & 0 & -\frac{1}{\gamma_3} & \cdots & \frac{1}{\gamma_3}\\
      \vdots & \vdots & \vdots & \vdots & \ddots \\
      \frac{\lambda_1}{c} & \frac{\lambda_2}{c} &
      \frac{\lambda_3}{c} & \cdots & -\infty\\
    \end{array}
  \right).
\end{displaymath}

Enquanto que no caso $c=0$, teremos:
\begin{displaymath}
  Q = \left(
    \begin{array}{ccccc}
      -\frac{1}{\gamma_1} & 0 & 0 & \cdots & 0\\
      0 & -\frac{1}{\gamma_2} & 0 & \cdots & 0\\
      0 & 0 & -\frac{1}{\gamma_3} & \cdots & 0\\
      \vdots & \vdots & \vdots & \vdots & \ddots \\
      \infty & \infty & \infty & \cdots & -\infty\\
    \end{array}
  \right).
\end{displaymath}

\begin{proposicao}
  \label{prop:taxa-x-y}
  Sejam $x, y \in \Nz$, $x \neq y$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{xy}(t)}{t} = 0.
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align*}
    \frac{p_{x y} (t)}{t}
    &= \frac{1}{t}\int_{0}^{t} P( X^x(t) = y |
    \gamma_x T_0^x = s) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{1}{t} \int_{0}^{t} P( X^\infty(t-s) = y ) \frac{1}{\gamma_x}
    e^{-\frac{s}{\gamma_x}} ds \\
    &\leq \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(t-s) ds \\
    &= \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(t-s) ds \\
    &= \frac{1}{t \gamma_x} \int_{0}^{t} p_{\infty y}(s) ds
    \xrightarrow{t \searrow 0} \lim_{t \searrow 0} \frac{p_{\infty y}
      (t)}{\gamma_x} = 0.
  \end{align*}

\end{proof}

\begin{proposicao}
  \label{prop:taxa-x-x}
  Para $x \in \Nz$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{xx}(t) - 1}{t} = -\frac{1}{\gamma_x}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align*}
    p_{xx} (t)
    &= P( \gamma_x T_0^x > t) + 
    \int_{0}^{t} P( X^x(t) = y |
    \gamma_x T_0^x = s) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &= e^{-\frac{t}{\gamma_x}} + 
    \int_{0}^{t} P( X^\infty(t-s) = y) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
  \end{align*}
  Fazendo contas análogas às da proposição anterior, podemos
  mostrar que o segundo termo dessa soma, dividido por $t$, vai para
  zero quando $t \searrow 0$. Tratando o primeiro termo agora, temos
  que:
  \begin{displaymath}
    \frac{e^{-\frac{t}{\gamma_x}} - 1}{t} \xrightarrow{t \searrow 0}
    -\frac{1}{\gamma_x}.
  \end{displaymath}

  Dessa forma:
  \begin{displaymath}
     \lim_{t \searrow 0} \frac{p_{xx} (t) - 1}{t} = -\frac{1}{\gamma_x}.
  \end{displaymath}
  
\end{proof}

\begin{proposicao}
  Para $x \in \Nz$, vale que:
  \label{prop:taxa-inf-x}
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{\infty x}(t)}{t} = \begin{cases}
      \frac{\lambda_x}{c} & \textrm{ se } c > 0 \\
      \infty & \textrm{ se } c = 0 \\
    \end{cases}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  \begin{align}
    \frac{p_{\infty x}}{t} &= \frac{1}{t} P \left( \bigcup_{i =
        1}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right) \notag \\
    &= \frac{P \left( \Gamma^\infty (\sigma^x_1 -) \leq t <
      \Gamma^\infty(\sigma^x_1) \right)}{t} +
    \frac{1}{t} P \left( \bigcup_{i =
        2}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right) \notag \notag \\
    \label{erros_taxa_inf}
    &= \frac{P \left( \Gamma^\infty (\sigma^x_1 -) \leq t \right)}{t} -
    \frac{P \left( \Gamma^\infty (\sigma^x_1) \leq t \right)}{t} +
    \frac{1}{t} P \left( \bigcup_{i =
        2}^{\infty} \left\{ \Gamma^\infty (\sigma^x_i -) \leq t <
        \Gamma^\infty(\sigma^x_i) \right\} \right)
  \end{align}

  Agora vamos mostrar que o segundo termo dessa soma vai a zero quando
  $t \searrow 0$.
  \begin{align*}
    \frac{P (\Gamma^\infty (\sigma^x_1) \leq t)}{t}
    &= \frac{1}{t} P \left(
      \gamma_x T^x_1 + 
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i +
      c\sigma^x_1
      \leq t
    \right) \\
    &\leq \frac{1}{t} P \left(
      \gamma_x T^x_1 + 
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t
    \right)\\
    &= \frac{1}{t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t - s
      \middle\vert \gamma_x T^x_1 = s
    \right) \frac{1}{\gamma_x} e^{-\frac{s}{\gamma_x}} ds\\
    &\leq \frac{1}{\gamma_x t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq t - s
    \right) ds\\
    &\leq \frac{1}{\gamma_x t} \int_0^t P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      \leq s
    \right) ds\\
    &\xrightarrow{t\searrow0} \frac{1}{\gamma_x} P \left(
      \sum_{y \neq x} \sum_{i = 1}^{N_y (\sigma^x_1)} \gamma_y T^y_i
      = 0 \right) = 0
  \end{align*}

  Agora note que, como $\Gamma^\infty$ é não decrescente, o evento no
  terceiro termo de \eqref{erros_taxa_inf} está contido no evento do
  segundo termo. Assim como mostramos que o segundo termo vai a zero,
  teremos que o terceiro também irá.

  Agora resta calcular o limite do primeiro termo. Ao fazer isso
  estaremos também calculando o limite desejado.


  Faremos isso usando o Teorema Tauberiano, que relaciona o
  comportamento a função de distribuição de uma variável aleatória
  positiva perto do zero com o comportamento de sua transformada de
  Laplace no infinito.  Seguiremos o enunciado do
  Teorema\emph{XIII.5.1} de \cite{fellerv2}.

  O primeiro passo é calcular a transformada da Laplace de
  $\Gamma^\infty(\sigma^x_1-)$. Assim para um $u \geq 0$, podemos
  calcular:
  \begin{align*}
    \phi (u) := \E \left[ e^{-u \Gamma^\infty (\sigma^x_1-)}  \right] =
    \lambda_x \left( \lambda_x + uc + u \sum_{y \neq x}
      \frac{\lambda_x \gamma_x}{1 + u\gamma_x}  \right)^{-1}
  \end{align*}

  Por enquanto vamos nos concentrar no caso $c > 0$. Nesse caso
  teremos que para $u, v > 0$:
  \begin{align*}
    \frac{\phi(uv)}{\phi (u)} &= \frac{\lambda_x + uc + \sum_{y \neq
        x} \frac{u \lambda_x\gamma_x}{1 + u \gamma_x}} {\lambda_x + u
      v c + \sum_{y \neq x} \frac{u v
        \lambda_x\gamma_x}{1 + u v \gamma_x}} \\
    &= \frac{\frac{\lambda_x}{u} + c + \sum_{y \neq x}
      \frac{\lambda_x\gamma_x}{1 + u \gamma_x}} {\frac{\lambda_x}{u} +
      v c + \sum_{y \neq x} \frac{v
        \lambda_x\gamma_x}{1 + u v \gamma_x}}. \\
  \end{align*}

  Agora note que quando $u$ vai ao infinito, os termos de cada uma das
  somas vai a zero monotonamente com $u$, assim usando o teorema da
  convergência monótona, teremos que:
  \begin{align*}
      \lim_{u \to \infty} \frac{\phi(uv)}{\phi (u)} &= \frac{1}{v}.
  \end{align*}

  Assim verificamos a condição do teorema, e temos que:
  \begin{align*}
    \lim_{t \searrow 0} \frac{P( \Gamma^\infty(\sigma^x_1-) \leq
      t)}{\phi(\frac{1}{t})} = 1
  \end{align*}

  Assim se mostrarmos que $\frac{\phi(\frac{1}{t})}{t}$ converge para
  $\frac{\lambda_x}{c}$, teremos mostrado o nosso resultado.

  \begin{align*}
    \frac{\phi(\frac{1}{t})}{t} &= \frac{1}{t} \lambda_x \left(
      \lambda_x + \frac{c}{t} + \sum_{y \neq x} \frac{1}{t}
      \frac{\lambda_x \gamma_x}{1 + \frac{\gamma_x}{t}} \right)^{-1} \\
    &= \lambda_x \left( t\lambda_x + c + \sum_{y \neq x}
      \frac{\lambda_x \gamma_x}{1 + \frac{\gamma_x}{t}} \right)^{-1}.
  \end{align*}

  Novamente cada termo da soma vai a zero monotonamente quanto $t
  \searrow 0$, assim pelo teorema da convergência monótona, teremos que
  $\lim_{t \searrow 0} \frac{\phi(\frac{1}{t})}{t} =
  \frac{\lambda_x}{c}$.

  Agora vamos tratar o caso $c = 0$. Para isso note que nossa
  construção do processo K permite que acoplemos várias versões do
  processo, com $c$ diferentes em um mesmo espaço de
  probabilidade. Basta usar os mesmos processos e Poisson e variáveis
  exponenciais para todos eles. Assim vamos colocar um índice $c$ em
  $\Gamma^y_c$ para denotar qual valor de $c$ estamos trabalhando.

  Dessa forma teremos que para todo $y \in \Nzb$, $\Gamma^y_c$ é
  crescente com $c$, e portanto $P ( \Gamma^\infty_c(\sigma^x_1-) \leq
  t)$ é monótona em $c$.

  Dessa forma teremos que, para todo $c > 0$:
  \begin{align*}
    \liminf_{t \searrow 0} \frac{P ( \Gamma^\infty_0(\sigma^x_1-) \leq
      t)}{t} &\geq \liminf_{t \searrow 0} \frac{P (
      \Gamma^\infty_c(\sigma^x_1-) \leq t)}{t}
    = \frac{\lambda_x}{c}
  \end{align*}

  Assim tomando $c > 0$ cada vez menores, concluiremos que $\frac{P (
    \Gamma^\infty_0(\sigma^x_1-))}{t} \xrightarrow{t \searrow 0}
  \infty$
\end{proof}

\begin{proposicao}
  \label{prop:taxa-inf-inf}
  Vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{\infty \infty}(t) - 1}{t} = -\infty
  \end{displaymath}
\end{proposicao}
\begin{proof}
  O caso $c = 0$ é trivial pela Proposição
  \ref{prop:naocontinuidade}. Assim vamos supor que $c > 0$.
  \begin{align*}
    \frac{p_{\infty \infty} (t) - 1}{t} &= - \sum_{x \in \Nz}
      \frac{p_{\infty x} (t)}{t}.
  \end{align*}

  Cada termo dessa soma converge à $\frac{\lambda_x}{c}$, e estamos
  supondo que a soma dessa série diverge para $\infty$, de onde
  concluímos que:
  \begin{align*}
    \lim_{t \searrow 0}\frac{p_{\infty \infty} (t) - 1}{t} &= - \infty
  \end{align*}
\end{proof}

\begin{proposicao}
  \label{prop:taxa-x-inf}
  Para $x \in \Nz$, vale que:
  \begin{displaymath}
    \lim_{t \searrow 0} \frac{p_{x \infty}(t)}{t} = \begin{cases}
      \frac{1}{\gamma_x} & \textrm{ se } c > 0 \\
      0 & \textrm{ se } c = 0 \\
    \end{cases}
  \end{displaymath}
\end{proposicao}
\begin{proof}
  Novamente o caso $c = 0$ é trivial em vista da Proposição
  \ref{prop:naocontinuidade}. Assim vamos supor que $c > 0$.
  \begin{align*}
    \frac{p_{x \infty}}{t} &= \frac{1}{t} \int_0^t P(X^x (t) = \infty | \gamma_x
    T^x_0 = s) \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{1}{t} \int_0^t p_{\infty \infty} (t-s)
    \frac{1}{\gamma_x}e^{-\frac{s}{\gamma_x}} ds\\
    &= \frac{e^{-t}}{\gamma_x} \frac{1}{t} \int_0^t p_{\infty \infty} (s)
    e^{\frac{s}{\gamma_x}} ds\\
  \end{align*}

  Como $c>0$, pela Proposição \ref{prop:continuidade}, $p_{\infty
    \infty} (t) \xrightarrow{t \searrow 0} 1$. De onde concluímos que:
   \begin{align*}
    \frac{p_{x \infty}(t)}{t} \xrightarrow{t \searrow 0} 
    \frac{1}{\gamma_x}
  \end{align*}
\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Medida invariante}
\label{sec:invariante}

O objetivo dessa seção é mostrar que a seguinte distribuição de
probabilidade é uma medida invariante para o processo K.

\begin{equation}
  \label{eq:invariante}
  \pi(x) = \begin{cases}
    \frac{\lambda_x \gamma_x}{c + \sum_{y \in \Nz} \lambda_y \gamma_y}
    & \textrm{ se } x \in \Nz \\
    \frac{c}{c + \sum_{y \in \Nz} \lambda_y \gamma_y}
    & \textrm{ se } x = \infty \\
  \end{cases}
\end{equation}

\begin{lema}
  \label{lema:cont_direita_prob}
  Para todo $x, y \in \Nzb$ e $t > 0$, temos que:
  \begin{align}
    \lim_{s \searrow t} p_{x y} (s) = p_{x y} (t)
  \end{align}
\end{lema}
\begin{proof}
  O Corolário \ref{prop:proc-cadlag} nos garante que o processo K é
  Càdlàg na topologia definida na Seção \ref{sec:topologia}.

  Note que para $y \in \Nz$, a função $f_y: \Nzb \to \R$ definida por
  $f_y(x) = \ind\{ x = y\}$ é contínua nessa topologia. Assim temos
  que $\lim_{s \searrow t} f_y(X^x(s)) = f_y (X^x(t))$ \qc.

  Como $f_y$ é uma função limitada, o teorema da convergência dominada
  nos garante que:
  \begin{displaymath}
    p_{x y} (s) = \E[f_y(X^x(s))] \xrightarrow{s \searrow t} \E[f_y
    (X^x(t))] = p_{x y} (t).
  \end{displaymath}

  Para o caso $y = \infty$, vamos dividir em dois casos.  Caso $c =
  0$, temos que $p_{x \infty} (s) = 0$ para todo $s > 0$, assim essa
  função é obviamente contínua em todo ponto $t > 0$.

  Para o caso $c > 0$, usando a propriedade de Markov do processo,
  para um $s > t$ teremos que:

  \begin{align*}
    p_{x \infty} (s) &= \sum_{y \in \Nzb} p_{x y} (t) p_{y \infty}
    (s - t) \\
    &= p_{x \infty} (t) p_{\infty \infty} (s - t) + 
    \sum_{y \in \Nz} p_{x y} (t) p_{y \infty} (s - t). \\
  \end{align*}


  Ao fazer $s \searrow t$, pela Proposição \ref{prop:continuidade},
  temos que o primeiro termo converge para $p_{x \infty}(t)$, enquanto
  que cada termo da série do segundo termo converge à zero. Como
  podemos dominar cada termo dessa série por $p_{x y} (t)$ que é
  somável em $y$, usando o teorema da convergência dominada concluímos
  que:
  \begin{displaymath}
    \lim_{s \searrow t} p_{x \infty} (s) = p_{x \infty} (t)
  \end{displaymath} 
\end{proof}

\begin{proposicao}
  O processo K tem uma única medida invariante.
\end{proposicao}
\begin{proof}
  Fixe um número real $h > 0$ e considere uma cadeia de Markov em
  tempo discreto $(Y^{y, h}_n)_{n \geq 0}$ dada por $Y^{y, h}_n =
  X^y(n h)$.

  Fixado um estado $y \in \Nz$, defina $M = \min_{i \geq 1} \{ T_i^y >
  h \}$. $M$ é uma variável aleatória com distribuição geométrica com
  probabilidade de sucesso dada por $e^{-\frac{h}{\gamma_y}}$. Agora
  temos que $X_t^y = y$ para todo $t \in [\Gamma^y(\sigma_M^x-),
  \Gamma^y(\sigma_M^x-) + T_M^y)$, e como $T_M^y > h$, existe um
  múltiplo de $h$ nesse intervalo. Assim isso vai corresponder à um
  ponto onde $Y_n^{y, h} = y$, assim a esperança de tempo de retorno à
  um estado em $(Y_n)$ é finita.


  Para $x, y \in \Nz$, vale que $P (Y^{x, h}_1 = y) > 0$. Temos ainda
  que para todo $x \in \Nzb$ $P (Y^{x, h}_1 = \infty ) > 0$ se $c >
  0$. Enquanto que $P (Y^{x, h}_1 = \infty) = 0$ para $c = 0$. Assim
  a cadeia é irredutível no caso $c > 0$ e no caso $c = 0$ temos que
  $\Nz$ é uma classe de comunicação fechada, enquanto que $\{\infty\}$
  é uma classe de comunicação aberta.


  Em todo caso, para cada escolha de $h$, existe uma única medida de
  probabilidade $\mu_h$ tal que para todo $n \geq 1$:
  \begin{displaymath}
    \mu_h (y) = \sum_{x \in \Nzb} \mu_h(x) P ( Y_n^ {x, h} = y) =
    \sum_{x \in \Nzb} \mu_h(x) P ( X^x (nh) = y)
  \end{displaymath}

  Agora fixe um inteiro $m \geq 1$ e um número real $h > 0$, temos que:
  \begin{align*}
    \mu_h (y) &= \sum_{x \in \Nzb} \mu_h(x) P ( Y_{m}^ {x, h} = y)\\
    &= \sum_{x \in \Nzb} \mu_h(x) P ( X^x (m h) = y) \\
    &= \sum_{x \in \Nzb} \mu_h(x) P ( Y_{1}^ {x, m h} = y).
  \end{align*}

  Portanto $\mu_h$ é uma medida invariante para $(Y^{\bullet, mh}_n)_{n
    \geq 0}$ e como ela é única, teremos que $\mu_h = \mu_{m h}$.

  Assim concluímos que $\mu_{h} = \mu_1$ para todo $h > 0$ racional.
  Agora fixado um $h > 0$ irracional, tome uma sequencia de números
  racionais $(h_n)$ que convirjam para $h$ pela direita. Teremos que:
  \begin{align*}
    \mu_1(y) &= \sum_{x \in \Nzb} \mu_1(x) P ( Y_{1}^ {x, h_n} = y)\\
    &= \sum_{x \in \Nzb} \mu_1(x) P ( X^x(h_n) = y)\\
    &= \sum_{x \in \Nzb} \mu_1(x) p_{x y} (h_n).    
  \end{align*}
  
  Cada termo dessa soma é dominado por $\mu_1(x)$, que tem soma $1$,
  assim usando o Lema \ref{lema:cont_direita_prob} e o teorema da
  convergência dominada, concluímos que:
  \begin{align*}
    \mu_1 (y) &= \sum_{x \in \Nzb} \mu_1(x) p_{x y} (h)\\
    &= \sum_{x \in \Nzb} \mu_1(x) P(Y_1^{x, h} = y)\\
  \end{align*}
  ou seja, $\mu_1$ é uma probabilidade invariante para $(Y_n^{\bullet,
    h})_n $ e como a probabilidade invariante é única, concluímos que
  $\mu_1 = \mu_h$ para todo $h > 0$.
\end{proof}


\begin{proposicao}
  A probabilidade $\pi$ definida em \eqref{eq:invariante} é a medida
  invariante do processo K, caso $c > 0$.
\end{proposicao}
\begin{proof}
  Seja $\mu$ a única probabilidade invariante do nosso processo que
  encontramos na última proposição. Para todo $t > 0$, vale que:
  \begin{gather*}
    \frac{\mu(y)}{t} = \sum_{x \in \Nzb} \mu(x) \frac{p_{x y}
      (t)}{t}\\
  \end{gather*}

  Rearranjando os termos, teremos que para todo $y \in \Nz$:
  \begin{displaymath}
    0 = \mu(\infty) \frac{p_{\infty y}(t)}{t} + 
    \mu(y) \frac{p_{y y}(t) - 1}{t} + 
    \sum_{\substack{x \in \Nz \\ x \neq y}} \mu(x) \frac{p_{x
        y}(t)}{t}.
  \end{displaymath}

  As Proposições \ref{prop:taxa-x-x} e \ref{prop:taxa-inf-x} nos
  fornecem os limites dos dois primeiros termos, enquanto que a
  Proposição \ref{prop:taxa-x-y} nos garante que $\frac{p_{x
      y}(t)}{t}$ converge à zero quando $t \searrow 0$ para cada $x
  \in \Nz \setminus \{y\}$.

  Agora note que $p_{x y} (t) \leq P(\Gamma^x (\sigma_1^y -) \leq t)
  \leq P(\Gamma^\infty (\sigma_1^y -) \leq t)$. Agora na demonstração
  da Proposição \ref{prop:taxa-inf-x} mostramos que o limite dessa
  última quantidade sobre $t$ converge à $\frac{\lambda_y}{c}$. Assim
  concluímos existe uma constante $K$ tal que para $t$ pequeno o
  suficiente e para todo $x \in \Nz\setminus \{ y\}$, vale que
  $\mu(x)\frac{p_{x y}(t)}{t} \leq K \mu(x)$.

  Como $\sum_{x} K \mu(x) \leq K$, o teorema da convergência dominada
  nos garante que:
  \begin{displaymath}
    \sum_{\substack{x \in \Nz \\ x \neq y}} \mu(x)
    \frac{p_{x y}(t)}{t} \xrightarrow{t \searrow 0} 0.
  \end{displaymath}

  Assim obtemos que, para cada $y \in \Nz$:
  \begin{gather*}
    0 = \mu(\infty) \frac{\lambda_y}{c} - \mu(y) \frac{1}{\gamma_y}.
  \end{gather*}

  Juntando a esse sistema de equações a restrição de que
  $\sum_{x\in\Nzb}\mu(x) = 1$, teremos um sistema de equações cuja uma única
  solução, que é a $\pi$ dada em \eqref{eq:invariante}.
\end{proof}

\begin{lema}
  \label{lema:c_continuo}
  Para todo $t > 0$ e $y \in \Nzb$, vale que:
  \begin{displaymath}
    \lim_{c\searrow 0} X_c^y(t) = X_0^y(t)
  \end{displaymath}
  quase certamente.
\end{lema}
\begin{proof}
  Observe que $\Gamma^y_c (t) - \Gamma^y_0(t) = ct \xrightarrow{c
    \searrow 0} 0$.

  Pela Proposição \ref{prop:naocontinuidade}, temos que $P(X^y_0(t) =
  \infty) = 0$.

  Ainda sabemos que $\Gamma^y_0(\sigma^x_i-)$ é uma variável aleatória
  contínua. Assim $P( \Gamma^y_0(\sigma^x_i-) = t) = 0$.

  Assim vamos fixar uma realização do processo onde
  $X^y_0(t) = x < \infty$ e $\Gamma^y_0(\sigma^x_i-) \neq t$ para todo
  $x \in \Nz$ e $i \geq 1$. Tais realizações tem probabilidade $1$.


  Agora há duas possibilidades. A primeira é que $t < \gamma_y
  T^y_0$. Nesse caso teremos que para qualquer $c \geq 0$, vale que
  $X^y_c = y$. Assim a convergência é trivial.

  A segunda possibilidade é que $t \geq \gamma_y T^y_0$. Nesse caso,
  existe um $i \geq 1$ tal que $\Gamma^y_0(\sigma^x_i-) < t <
  \Gamma^y_0(\sigma^x_i)$. Portanto para $c > 0$ pequeno o suficiente,
  teremos que $\Gamma^y_c(\sigma^x_i-) < t < \Gamma^y_c(\sigma^x_i)$ e
  assim $X_c^y(t) = x$.
\end{proof}

\begin{proposicao}
  A probabilidade $\pi$ definida em \eqref{eq:invariante} é a medida
  invariante do processo K, caso $c = 0$.
\end{proposicao}
\begin{proof}

  Vamos denotar a probabilidade definida em \eqref{eq:invariante} por
  $\pi_c$

  Mostramos na proposição \ref{prop:naocontinuidade} que para todo $y
  \in \Nzb$, $P(X^y(t) = \infty) = 0$. Assim é trivial verificar que
  $0 = \pi_0(\infty) = \sum_{y\in\Nzb} \pi_0(y) P(X^y(t) =
  \infty)$.

  Note que para todo $x \in \Nzb$, $\pi_c(x) \xrightarrow{c\searrow0}
  \pi_0(x)$.

  Pelo Lema \ref{lema:c_continuo}, e usando o fato que, para $x \in
  \Nz$ fixado, a função $f_x(y) = \ind\{y = x\}$ é contínua e
  limitada, teremos que $P(X^y_c (t) = x) = \E [f_x(X^y_c(t))]
  \xrightarrow{c\searrow0} \E [f_x(X^y_c(t))] = P(X^y_0(t) = x)$.

  Como $\pi_c$ é invariante para $(X^y_c)$, teremos que para todo $x
  \in \Nz$ e $t \geq 0$:

  \begin{align*}
    \pi_c(x) &= \sum_{y \in \Nzb} \pi_c (y) P(X^y_c (t) = x).\\
  \end{align*}

  Cada termo dessa soma pode ser dominado por $\pi_0(y)$ se $y <
  \infty$ e por $1$ se $y = \infty$, assim tomando limite $c\searrow0$
  e usando o teorema da convergência dominada, teremos que:

  \begin{align*}
    \pi_0(x) &= \sum_{y \in \Nzb} \pi_0 (y) P(X^y_0 (t) = x).\\
  \end{align*}

  De onde concluímos que $\pi_0$ é uma medida invariante para o o
  processo K no caso $c = 0$.
\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Tempo no infinito}
\label{sec:tempo-infinito}

Na demonstração da Proposição \ref{prop:continuidade} mostramos que,
no caso $c > 0$, o tempo que o processo passa no infinito tem medida
de Lebesgue positiva quase certamente. Ainda usando argumentos como os
da Proposição \ref{prop:naocontinuidade}, a medida de Lebesgue do
tempo em que o processo passa no infinito vale zero quase certamente
no caso $c = 0$.

Nessa seção tentaremos expandir um pouco no caso $c = 0$,
especificamente iremos dar uma maneira de calcular a dimensão de
Hausdorf do tempo em que o processo passa no infinito.  Para o
restante dessa seção, considere sempre que $c = 0$.

\begin{definicao}
  Vamos denotar por $\RR^y$ a imagem de $\Gamma^y$. Isso é:
  \begin{displaymath}
    \RR^y = \left\{
      t \geq 0 : \exists s \geq 0 \textrm{ tal que } \Gamma^y(s) = t
    \right\}.
  \end{displaymath}
  Aqui também vamos omitir o índice $y$ quando não fizer diferença
  qual é a condição inicial.
\end{definicao}

\begin{proposicao}
  Quase certamente, temos que $X(t) = \infty$ se e somente se $t \in \RR$.
\end{proposicao}
\begin{proof}
  Vamos fixar uma realização do nosso processo onde $\Gamma$ é
  estritamente crescente, càdlàg, limitada em compactos e $\lim_{t \to
    \infty} \Gamma(t) = \infty$.

  Nessas realizações, tome um $t \in \RR$. Assim existe um (único por
  $\Gamma$ ser crescente) $s \geq 0$ tal que $\Gamma(s) = t$. Por
  absurdo, suponha que $X(t) = x < \infty$. Assim existe um $i \geq 1$
  tal que $\Gamma(\sigma_i^x -) \leq t < \Gamma(\sigma_i^x)$. Portanto
  $\Gamma(s) < \Gamma(\sigma_i^x)$. Como $\Gamma$ é crescente,
  concluímos que $s < \sigma_i^x$, de onde concluímos que
  $\Gamma(\sigma_i^x-) > \Gamma(s) = t$, o que contraria a hipótese.

  Agora tome um $t \geq 0$ tal que $X(t) = \infty$. Defina $s = \inf
  \{ r \geq 0 :  \Gamma(r) > t \}$. Usando a definição do ínfimo e o
  fato de $\Gamma$ ser crescente e càdlàg, concluímos que $\Gamma(s-)
  \leq t$ e $\Gamma(s) \geq t$. Agora, por absurdo, suponha que
  $\Gamma(s) > t$. Assim teremos que $\Gamma(s-) \leq t <
  \Gamma(s)$. Assim como os pontos de descontinuidade de $\Gamma$
  correspondem às marcas dos processos de Poisson, teremos que existe
  um $\sigma_i^x = s$, de onde concluímos a contradição de que $X(t) =
  x \neq \infty$. Portanto, como não pode acontecer que $\Gamma(s) >
  t$, teremos que $\Gamma(s) = t$ e assim $t \in \RR$.
\end{proof}

\begin{proposicao}
  Agora vamos denotar por $\RRb$ o fecho de $\RR$. Temos que:
  \begin{displaymath}
    \RRb = \RR \cup \{ \Gamma(s-) : s > 0 \}.
  \end{displaymath}
\end{proposicao}
\begin{proof}
  Novamente considere as realizações onde $\Gamma$ é càdlàg, crescente
  e limitada em compactos.

  Fixado um $s > 0$ vamos mostrar que $\Gamma(s-) \in \RR$. Para isso
  tome uma sequência $(s_n)_{n \geq 1}$ tal que $s_n \nearrow s$. Por
  definição, temos que $\Gamma(s_n) \in \RR$ para todo $n$ e
  $\Gamma(s_n) \to \Gamma(s-)$. De onde concluímos que $\Gamma(s-) \in
  \RRb$.

  Agora tome um $t \in \RRb \setminus \RR$. Por definição existe uma
  sequência $(t_n)_n \in \RR$ tal que $t_n \to t$. Como $t_n \in \RR$,
  existe $s_n$ tal que $\Gamma(s_n) = t_n$.

  Note que $(s_n)$ é limitada, assim existe uma subsequencia de $(s_n)$
  que seja convergente. Assim sem perda de generalidade, vamos supor
  que $(s_n)$ seja convergente e denotar seu limite por $s$.

  Como $s_n \to s$, $\Gamma(s_n) \to t$ e $\Gamma$ é càdlàg, então
  temos que $t \in \{ \Gamma(s), \Gamma(s-) \}$. Já que estamos
  supondo que $t \not\in \RR$, seque que $t = \Gamma(s-)$.
\end{proof}

\begin{proposicao}
  $\RRb \setminus \RR$ é quase certamente enumerável.
\end{proposicao}
\begin{proof}
  O conjunto dos pontos onde $\Gamma$ não é contínua é $\{ \sigma_i^x:
  x \in \Nz, \, i \geq 1\}$. Assim temos que $\RRb \setminus \RR
  \subseteq \{ \Gamma(\sigma_i^x-): x \in \Nz, \, i \geq 1\}$, que é
  um conjunto enumerável.
\end{proof}

\begin{proposicao}
  $\Gamma$, quando enxergada como um processo estocástico, é um
  subordinador. Isso é, um processo crescente, contínuo à direita, que
  tem incrementos estacionários e independentes.
\end{proposicao}
\begin{proof}

  Já mostramos que $\Gamma$ é crescente e contínua à direita.  Tome
  $\FF_t$ a $\sigma$-álgebra gerada por $\{ \Gamma(s): s \in [0,
  t]\}$ e repare que para $t, s > 0$:
  \begin{displaymath}
    \Gamma(t+s) - \Gamma(t) = \sum_{x \in \Nz} \sum_{i =
      N(t)+1}^{N(t+s)} \gamma_x T_i^x.
  \end{displaymath}

  Agora usando o fato das $\{ T_i^x: x \in \Nzb, \, i \geq 1\}$ serem
  independentes e identicamente distribuídas e os processos de Poisson
  terem incrementos estacionários e independentes, teremos que essa
  quantidade é independente de $\FF_t$ e tem a mesma distribuição que
  $\Gamma(s) - \Gamma(0)$.
\end{proof}

\begin{proposicao}
  O expoente de Laplace de $\Gamma^\infty_0$ é dado por:
  \begin{displaymath}
    \Phi(u) = u \sum_{x \in \Nz} \frac{\lambda_x \gamma_x}{1 + u\gamma_x}.
  \end{displaymath}
  Isso é para $u \geq 0$, teremos que:
  \begin{displaymath}
    \E \left[
      \exp \left\{
        -u \Gamma^\infty_0 (t)
      \right\}
    \right] = 
    \exp\left\{
      -t \Phi(u)
    \right\}.
  \end{displaymath}
\end{proposicao}


Agora temos ferramentas para calcular a dimensão de Hausdorff de
$\RR$. Mas antes introduzir brevemente o que é a dimensão de Hausdorff
de um conjunto. Para uma cobertura mais abrangente do assunto,
recomendamos (???).

Para um $\epsilon > 0$, e um boreliano $A$, vamos chamar de
$\II_\epsilon(A)$ a família de todas as coberturas de $A$ por um número
enumerável de intervalos de comprimento menor que $\epsilon$.


Para um $\rho > 0$, $\epsilon > 0$ e um boreliano $A$, definimos:
\begin{equation}
  \label{eq:dim-hausdorff-eps}
  m^\rho_\epsilon(A) = \inf_{\II \in \II_\epsilon(A)} 
  \sum_{I \in \II} (\mathrm{l}(I))^\rho, 
\end{equation}
onde  $\mathrm{l}(I)$ é o comprimento do intervalo $I$.

Quando diminuímos $\epsilon$, diminuímos o número de coberturas
possíveis, assim o ínfimo aumenta. Portando o limite $\epsilon
\searrow 0$ de \eqref{eq:dim-hausdorff-eps} existe, e assim faz sentido
definir:
\begin{equation}
  \label{eq:dim-hausdorf}
  m^\rho(A) = \lim_{\epsilon \searrow 0} m^\rho_\epsilon (A)
\end{equation}

É possível mostrar que $m^\rho(\bullet)$ é uma medida sobre os
borelianos de $\R$. Medida essa que chamados de medida de Hausdorff de
dimensão $\rho$.  Note que caso $\rho = 1$, a medida $m^\rho$ é a
medida de Lebesgue.

Também vale que se $m^\rho(A) = 0$, então $m^{\rho^\prime} = 0$ para
todo $\rho^\prime > \rho$ e se $m^\rho(A) > 0$, então $m^{\rho^\prime}
= \infty$ para todo $\rho^\prime < \rho$.

\begin{definicao}
  Definimos a dimensão de Hausdorff de um boreliano $A$ como:
  \begin{displaymath}
    \dim_H(A) = \sup \left\{ \rho > 0 : m^\rho(A) < \infty \right\}
    = \inf \left\{ \rho > 0: m^\rho(A) = 0 \right\}
  \end{displaymath}
\end{definicao}

\begin{teorema}
  Para todo $t > 0$, vale que quase certamente:
  \begin{displaymath}
    \dim_H(\RR^\infty \cap [0, t]) = 
    \dim_H(\RRb^\infty \cap [0, t]) =
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u}
  \end{displaymath}
\end{teorema}
\begin{proof}
  A primeira igualdade é vem do fato de um conjunto enumerável não
  alterar a dimensão de Hausdorf de um conjunto, enquanto que a
  segunda é o Corolário 5.3 de \cite{bertoin:97}, traduzido para a
  nossa notação.
\end{proof}

\begin{proposicao}
  Suponha que $\sup_{x\in\Nz}\lambda_x < \infty$ e que existam um
  $\delta>0$ e um $n_0 \in \Nz$ tal que $\gamma_x \leq x^{-(1+\delta)}$
  para todo $x > n_0$. Nessas condições:
  \begin{equation}
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u}  \leq \frac{1}{1+\delta}
  \end{equation}
\end{proposicao}

\begin{proof}
  Tome $K > 0$ tal que $\lambda_x \leq K$ para todo $x \in \Nz$. Assim
  teremos que para alguma constante $C$:
  \begin{align*}
    \Phi(u) &= \sum_{x \in \Nz} \frac{\lambda_x}{\frac{1}{u\gamma_x} +
      1}\\
    &\leq C + \sum_{x > n_0} \frac{K}{\frac{x^{1+\delta}}{u} + 1}\\
    &=u^{\frac{1}{1+\delta}} \left[
      \frac{C}{u^{\frac{1}{1+\delta}}} +
      \frac{1}{u^{\frac{1}{1+\delta}}} \sum_{x > n_0} \frac{K}{
        \left( \frac{x}{u^{\frac{1}{1+\delta}}}  \right)^{1+\delta}
        + 1
      }
    \right].
  \end{align*}

  Agora vamos nos concentrar na quantidade dentro dos
  colchetes. Fazendo a mudança de variáveis $v =
  u^{\frac{1}{1+\delta}}$, e supondo que $v$ seja um inteiro maior que
  $n_0$, agrupando os termos da soma para $x$ entre múltiplos de $v$,
  teremos que:
  \begin{align*}
    \frac{C}{v} + \frac{1}{v} \sum_{x > n_0} \frac{K}{ \left(
        \frac{x}{v} \right)^{1+\delta} + 1 }
    &\leq
    \frac{C}{v} + \sum_{y=1}^{\infty} \frac{K}{y^{1+\delta} + 1}
    \xrightarrow{v \to \infty} C^\prime,
  \end{align*}
  onde $C^\prime$ é uma constante positiva.

  Como jogar $u$ para o infinito é equivalente à jogar $v$ para o
  infinito, e que para obter um limite superior para o $\liminf$,
  podemos  nos limitar à uma sequência específica, teremos que:

  \begin{align*}
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u} &\leq
    \frac{1}{1+\delta} + \liminf_{u \to \infty} 
    \frac{\log{C^\prime}}{\log u}\\
    &= \frac{1}{1+\delta}.
  \end{align*}
\end{proof}

\begin{proposicao}
  Suponha que $\inf_{x\in\Nz}\lambda_x  > 0$ e que existam um
  $\delta>0$ e um $n_0 \in \Nz$ tal que $\gamma_x \geq x^{-(1+\delta)}$
  para todo $x > n_0$. Nessas condições:
  \begin{equation}
    \liminf_{u \to \infty} \frac{\log \Phi(u)}{\log u}  \geq \frac{1}{1+\delta}
  \end{equation}
\end{proposicao}

\begin{proof}
  Tome $K > 0$ tal que $\lambda_x \geq K$ para todo $x \in \Nz$. Dessa
  forma:
  \begin{align*}
    \Phi(u) &= \sum_{x \in \Nz} \frac{\lambda_x}{\frac{1}{u\gamma_x} +
      1}\\
    &\geq \sum_{x > n_0}
    \frac{K}{\left(\frac{x}{u^{\frac{1}{1+\delta}}}\right)^{1+\delta} +1}.
  \end{align*}

  Portanto, tomando $u > n_0$, e agrupando a série em somas sobre os inteiros
  que estão entre os múltiplos de $u$, teremos que:
  \begin{align*}
    \Phi(u^{1+\delta}) 
    &\geq \sum_{x > n_0}
    \frac{K}{\left(\frac{x}{u}\right)^{1+\delta} +1}\\
    &\geq \sum_{x \geq u}
    \frac{K}{\left(\frac{x}{u}\right)^{1+\delta} +1}\\
    &\geq \lfloor u \rfloor \sum_{y = 1}^{\infty}
    \frac{K}{y^{1+\delta} + 1}.
  \end{align*}

  Dessa forma:
  \begin{align*}
    \frac{\log \Phi(u)}{\log u} \geq \frac{\log\lfloor
      u^{\frac{1}{1+\delta}} \rfloor }{\log u} + 
    \frac{1}{\log u} \sum_{y = 1}^{\infty}
    \frac{K}{y^{1+\delta} + 1}.
  \end{align*}

  Como essa série é convergente, ao fazer $u \to \infty$ o segundo
  termo desaparecerá, enquanto que o primeiro irá convergir para
  $\frac{1}{1+\delta}$. De onde concluímos que:

  \begin{displaymath}
    \liminf_{u \to \infty} \frac{\log \Phi (u)}{\log u} \geq \frac{1}{1+\delta}
  \end{displaymath}
\end{proof}

%%% Local Variables: 
%%% TeX-master: "tese"
%%% End: 
