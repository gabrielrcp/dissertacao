%% ------------------------------------------------------------------------- %%
\chapter{Propriedades Básicas do Processo K}
\label{cap:propriedades}

Nesse capítulo vamos estabelecer resultados que serão ferramentas
básicas para trabalhar com o processo K durante o resto da
dissertação.

Apresentaremos uma maneira de aproximar o processo K por processos
Markovianos de salto, bem como provaremos que o processo K é
Markoviano.

%% ------------------------------------------------------------------------- %%

\section{Observações gerais}
\label{sec:observacoes}

Nessa seção vamos colocar várias observações sobre o processo K, que
serão usadas à exaustão em seções subsequentes da dissertação.

\begin{proposicao}
  \label{prop:gamma-crescente}
  $\Gamma$ é \qc estritamente crescente.
\end{proposicao}
\begin{proof}
  Como estamos supondo que $\sum_{x \in \Nz} \lambda_x = \infty$,
  então o conjunto das marcas dos processos de Poisson é \qc densa em
  $\R^+$. Em outras palavras, quase certamente, para todo $0 \leq s <
  t$, teremos que existe um $x \in \Nz$ e $i \geq 1$ tal que $s <
  \sigma^x_i < t$. Dessa forma usando a definição de $\Gamma$, teremos
  que:
  \begin{displaymath}
    \Gamma(t) - \Gamma(s) \geq \gamma_x T^x_i > 0
  \end{displaymath}
\end{proof}

\begin{proposicao}
  \label{prop:gamma-finita}
  Com probabilidade $1$, $\Gamma(t)$ é finito para todo $t \geq 0$.
\end{proposicao}
\begin{proof}
  Usando o teorema da convergência monótona, podemos calcular:
  \begin{displaymath}
    \E \left[\Gamma^y(t) \right] = \gamma_y + t \sum_{x \in \Nz}
    \lambda_x \gamma_x + ct < \infty
  \end{displaymath}
  Assim para cada $t \geq 0$ fixo, teremos que $\Gamma(t)$ é quase
  certamente finito. Tomando uma sequência $t_n \to \infty$ quando $n
  \to \infty$, teremos que $P(\cap_{n = 1}^{\infty} \{ \Gamma^y (t_n)
  < \infty \} \cap \{ \Gamma \textrm{ é crescente}\}) = 1$. Assim para
  realizações dentro desse evento, teremos que para todo $t \geq 0$,
  existe um $n$ tal que $t_n > t$. Portando $\Gamma(t) < \Gamma(t_n) <
  \infty$.
\end{proof}


\begin{proposicao}
  \label{prop:gamma-cadlag}
  A função $\Gamma$ é quase certamente càdlàg. Isso é, é contínua à
  direita e tem limites à esquerda.
\end{proposicao}
\begin{proof}

  Fixemos uma realização de $\Gamma$ onde ela seja crescente e
  limitada em compactos. Vamos pedir ainda que cada processo de
  Poisson $N_x$, $x \in \Nz$ seja localmente finito. Tais realizações
  tem probabilidade $1$.

  O fato de $\Gamma$ ser crescente e limitada em compactos já
  estabelece diretamente a existência dos limites à direita e à
  esquerda. Agora resta mostrar a continuidade à direita.

  Para isso fixe um $t \geq 0$ arbitrário. Para $s > 0$ teremos:

  \begin{equation*}
    \Gamma(t+s) - \Gamma(t) = 
    c s + 
    \sum_{x \in \N} \sum_{i = N_x(t)+1}^{N_x(t+s)} \gamma_x T^x_i.
  \end{equation*}

  Para cada $x$, como $N_x$ é localmente finito, temos que $N_x(t+s) =
  N_x(t)$ para $s$ pequeno o suficiente. Assim quando $s \searrow 0$,
  nós iremos calcular o limite da ``cauda'' de uma série convergente,
  de onde concluímos que:

  \begin{displaymath}
    \lim_{s \searrow 0} \Gamma(t+s) - \Gamma(t) = 0
  \end{displaymath}
\end{proof}

\begin{proposicao}
  \label{prop:gamma-dist-continua}
  Para todo $s, t > 0$, $i \geq 1$ e $x \in \Nz$ vale que:
  \begin{align*}
    P(\Gamma(s) = t) = P(\Gamma(\sigma^x_i) = t) =
    P(\Gamma(\sigma^x_i-) = t) = 0
  \end{align*}
\end{proposicao}
\begin{proof}
  Para qualquer $x \in \Nz$ e $n \in \N$, temos que:
  \begin{displaymath}
    P\left(\sum_{i = 1}^n \gamma_x T_i^x = t\right) = 0,
  \end{displaymath}
  já que as $T_i^x$ são variáveis aleatórias contínuas e
  independentes.

  Dessa forma vale que:
  \begin{align*}
    P\left(\sum_{i = 1}^{N^x(s)} \gamma_x T_i^x = t\right) 
    &= \sum_{n = 0}^{\infty}
    P\left(\sum_{i = 1}^n \gamma_x T_i^x = t\right)
    \frac{e^{-\lambda_x s} (\lambda_x s)^n}{n!} = 0
  \end{align*}

  Assim se $\nu$ for a distribuição de $\sum_{x \geq 2} (\sum_{i =
    1}^{N^x(s)} \gamma_x T_i^x$, teremos que:
  \begin{align*}
    P\left(\sum_{x \in \Nz}\sum_{i = 1}^{N^x(s)} \gamma_x T_i^x = t\right) 
    &= \int \nu(dr) P\left(\sum_{i = 1}^{N^1(s)} \gamma_1 T_i^1 = t - r\right) 
    = 0
  \end{align*}

  Agora, observando a definição de $\Gamma$, concluímos a primeira
  parte dessa proposição:
  \begin{displaymath}
    P\left( \Gamma(s) = t \right) = 0.
  \end{displaymath}


  Fixado $i \geq 1$ e $x \in \Nz$, se denotarmos por $g^x_i$ a
  densidade de uma Gamma de parâmetros $\lambda_x$ e $i$, que é a
  distribuição de $\sigma^x_i$, teremos que:
  \begin{align*}
    P \left( \Gamma^y_c(\sigma^x_i) = t \right) &=
    \int P\left( \left[ \gamma_y T_0 + \sum_{j = 1}^{i} \gamma_x
    T_i^x + \sum_{z \neq x} \sum_{j = 1}^{N^z(s)} \gamma_z
    T^z_j + c s\right]  = t \right)g^x_i(s) ds\\
    P \left( \Gamma^y_c(\sigma^x_i-) = t \right) &=
    \int P\left( \left[ \gamma_y T_0 + \sum_{j = 1}^{i-1} \gamma_x
    T_i^x + \sum_{z \neq x} \sum_{j = 1}^{N^z(s)} \gamma_z
    T^z_j + c s\right]  = t \right)g^x_i(s) ds\\
  \end{align*}

  Usando a parte anterior, temos que a probabilidade que estamos
  integrando vale zero, de onde concluímos que:
 \begin{align*}
    P \left( \Gamma^y_c(\sigma^x_i) = t \right) =
    P \left( \Gamma^y_c(\sigma^x_i-) = t \right) =
    0
  \end{align*}
\end{proof}


\begin{proposicao}
  \label{prop:reinicia-infinito}
  Para todo $y \in \Nz$ e $t \geq 0$, vale que
  $X^y(t + \gamma_y T_0^y) = X^\infty(t)$.
\end{proposicao}
\begin{proof}
  Fixe um $y \in \Nz$. Primeiramente observe que para todo $s \geq 0$,
  temos que $\Gamma^y(s) = \Gamma^\infty(s) + \gamma_yT^y_0$.

  Fixada uma realização do processo, suponha que $X^\infty(t) = x \in
  \Nz$, assim existe um $i \geq 1$ tal que $\Gamma^\infty(\sigma_i^x-)
  \leq t < \Gamma^\infty(\sigma_i^x)$. Dessa forma, somando $\gamma_y
  T^y_0$ nos termos dessa desigualdade, teremos que
  $\Gamma^y(\sigma_i^x-) \leq t+\gamma_yT_0^y <
  \Gamma^y(\sigma_i^x)$. E portanto $X^y(t+\gamma_y T_0^y) = x$.

  Com raciocínio análogo, chegamos que se $X^y(t+\gamma_y T_0^y) = x
  \in \Nz$, então $X^\infty(t) = x$. De onde concluímos a igualdade desejada.
\end{proof}


\begin{proposicao}
  \label{prop:proc-cadlag}
  O processo K é quase certamente càdlàg.
\end{proposicao}
\begin{proof}

  Iremos mostrar que o processo iniciado no $\infty$ é Càdlàg. Isso
  irá mostrar que o processo é Càdlàg para qualquer condição inicial,
  visto que iniciando em $y$, iremos continuar em $y$ até $\gamma_y
  T^y_0$, e depois continuaremos como uma cópia do processo iniciado no
  $\infty$.

  %%% O Billingsley só faz isso no caso de funcoes cadlag de [0,
  %%% \infty) em \R. Lembrar de achar outra referencia que faça isso
  %%% em um esp. metrico completo ou escrever eu mesmo.
  Fixe uma realização do processo e fixe $T > 0$ e $\epsilon > 0$
  arbitrários. Seguindo \cite{billingsley:99}, vamos mostrar que
  existem $0 = t_0 < t_1 < \ldots < t_N = T$ tais que $w[t_{i-1}, t_i)
  < \epsilon$, para todo $i = 1, \ldots, N$. Onde $w(A) = \sup_{t, s
    \in A} |X^\infty(t) - X^\infty(s)|$ para $A \subseteq \R$.

  Tome um $m \in \Nz$ tal que o $\diam\{x \in \Nzb: x > m \} =
  \frac{1}{m+1} < \epsilon$, onde $\diam(A)$ é o diâmetro do conjunto
  $A$ na métrica \eqref{eq:metrica}.

  Agora tome $S_1 < S_2 < \ldots < S_M$ uma ordenação dos conjunto $\{
  \sigma^x_i: x \leq m, \: i \geq 1, \: \Gamma(\sigma^x_i) \leq
  T\}$. Finalmente fixe $N = 2M+1$ se $\Gamma(S_M) < T$ e $N = 2M$
  caso contrário. Tome $t_0 = 0$, $t_N = T$ e para $i=1,\ldots, M$:
  \begin{align*}
    t_{2i-1} &= \Gamma(S_i-)\\
    t_{2i} &= \Gamma(S_i).\\
  \end{align*}

  Se $t \in [\Gamma(S_i-), \Gamma(S_i))$, teremos que
  $X(t)$ é constante, enquanto que se $t \in
  [\Gamma(S_{i-1}), \Gamma(S_{i}-))$, termos que
  $X(t) > m$, assim a variação nesse intervalo é menor ou igual
  à $\frac{1}{m} < \epsilon$. O mesmo ocorre nos intervalos $[t_0,
  t_1)$ e $[t_{N-1}, t_N)$.
\end{proof}

\begin{proposicao}
  \label{prop:proc-descontinuidades}
  $(X(t))_{t\geq 0}$ é \qc contínuo fora de $\{ \Gamma(0),
  \Gamma(\sigma_i^x-), \Gamma(\sigma_i^x): x \in \Nz\}$.
\end{proposicao}
\begin{proof}
  Tome $t$ um ponto de descontinuidade de $\Gamma$, então $\Gamma(t) -
  \Gamma(t-) > \epsilon > 0$. Fixe um $T > t$ e tome $0 = t_0 < t_1 <
  \ldots < t_N = T$ como na demonstração da Proposição
  \ref{prop:proc-cadlag}. Se $t \not\in \{t_0, \ldots, t_N\}$, então
  existe um $i$ tal que $t \in (t_i, t_{i+1})$. Assim teremos que
  $w[t_i, t_{i+1}) \geq \Gamma(t) - \Gamma(t-) > \epsilon$, que
  contraria a nossa escolha de $\{t_0, \ldots, t_N\}$.

  Assim temos que $t = t_i$ para algum $i$. Como os $t_i$'s sempre
  foram escolhidos do conjunto proposto, temos que eles são os únicos
  candidatos possíveis para pontos de descontinuidade.
\end{proof}

\begin{corolario}
  \label{cor:continuidades-processo}
  Qualquer $t > 0$ é quase certamente um ponto de continuidade do
  processo K.
\end{corolario}
\begin{proof}
  Mostramos que o conjunto dos pontos de pontos de descontinuidade do
  processo está contido em $\{ \Gamma(0), \Gamma(\sigma_i^x-),
  \Gamma(\sigma_i^x): x \in \Nz\}$.

  Observando a Proposição \ref{prop:gamma-dist-continua}, concluímos
  que todo $t > 0$ é um ponto de continuidade do processo.
\end{proof}


%% ------------------------------------------------------------------------- %%

\section{Aproximações}
\label{sec:aproximacoes}

Fixado um natural $n \in \Nz$, vamos ``truncar'' nosso processo em
$\{1, 2, \ldots, n, \infty\}$. Depois vamos mostrar mostrar que esses
processos truncados convergem ao processo original.

Primeiramente, para $n \in \Nz$ e $y \in \{1, \ldots, n, \infty\}$,
vamos definir:
\begin{equation}
  \Gamma^{(n)} (t) := \Gamma^{y,(n)}_c (t) = \gamma_y T_0
  + \sum_{x =1}^{n} \sum_{i = 1}^{N_x(t)}
  \gamma_x T_i^x
  + ct.
\end{equation}

O processo truncado em $n \in \Nz$, com estado inicial $y \in \{1,
\ldots, n, \infty\}$ será:
\begin{equation}
  X^{(n)}(t) = X^{y,(n)}_c(t) = \begin{cases}
    y, & \textrm{ se }  t < \gamma_y T_0^y\\
    x, & \textrm{ se } \Gamma^{y,(n)}_c(\sigma_i^x-) \leq t <
    \Gamma^{y,(n)}_c(\sigma^x_i)
    \textrm{ para algum } i \\
    \infty, & \textrm{ caso contrário.}
  \end{cases}
\end{equation}

\begin{proposicao}
  O processo $X^{(n)}$ é Càdlàd e Markoviano.
\end{proposicao}
\begin{proof}
  Quando truncamos o processo K, ele se comporta de maneira análoga ao
  caso analisado na Seção \ref{sec:visualizacao}, assim processo
  $X^{(n)}$ é um processo Markoviano de saltos, construído para ser
  contínuo a direita.
\end{proof}

Note que nós construímos $\{(X^{(n)}(t))_t\}_{n \in \Nz}$ e $(X(t))_t$
num mesmo espaço de probabilidade. Assim é natural perguntar se
$(X^{(n)}(t))_t$ converge para $(X(t))_t$ de alguma forma.

\begin{teorema}
  \label{teo:convergencia}
  $X^{\infty, (n)} (\bullet)$ converge para $X^\infty(\bullet)$ \qc na
  métrica de Skorohod quando $n \to \infty$.
\end{teorema}

A métrica de Skorohod é uma métrica sobre o espaço das trajetórias
Càdlàg que permite pequenas distorções temporais. Como
referência recomendamos \cite{billingsley:99} e \cite{ethier:86}.

\begin{proof}
  Essa demonstração foi adaptada diretamente do \emph{Lema 3.11} de
  \cite{fontes:08}.

  Para provar a proposição, vamos mostrar que o item (c) da Proposição
  5.3 do Capítulo 5 de \cite{ethier:86} vale quase certamente. Assim
  fixe uma realização qualquer do processo.

  Por simplicidade, consideraremos sempre que o processo iniciou no
  $\infty$ e parar de carregar esse índice na notação.

  Fixe um $T > 0$. Para cada natural $m$, considere $\delta_m :=
  \diam\{ x \in \Nzb: x > m \} = \frac{1}{m+1}$. Tome ainda $0 = S_0^m
  < S_1^m < S_2^m < \ldots $ uma ordenação de $\{0\}\cup\{ \sigma^x_i
  : x \leq m, i \geq 1\}$. Considere ainda, para $n \in \Nz$:
  \begin{displaymath}
    L^m_n := \min \left\{ i \geq 1: \Gamma^{(n)}(S^m_i) \geq T \right\}.
  \end{displaymath}

  O conjunto $\{\sigma_i^x: x > m, i\geq 1\}$ é denso, assim para $n$
  suficientemente grande $\Gamma^{(n)}(S^m_{i+1}-) >
  \Gamma^{(n)}(S^m_i)$ para todo $i < L^m_n$.

  Para esses valores de $n$, defina $\lambda_n^m: [0, L_n^m] \to \R^+$
  da seguinte forma:
  \begin{displaymath}
    \lambda_n^m(t) = \begin{cases}
      \Gamma(S_i^m) + \frac{\Gamma(S_{i+1}^m-) - \Gamma(S_i^m)}
      {\Gamma^{(n)}(S_{i+1}^{(m)} -) - \Gamma^{(n)}(S_i^m)}
      \left[t - \Gamma^{(n)}(S_i^m)\right]
      & \textrm{ se }
      \Gamma^{(n)}(S_i^m) \leq t \leq \Gamma^{(n)}(S_{i+1}^m-) \\
      \Gamma(S_{i+1}^m-) - \Gamma^{(m)}(S_{i+1}^m-) + t
      & \textrm{ se }
      \Gamma^{(n)}(S_{i+1}^m-) \leq t \leq \Gamma^{(n)}(S_{i+1}^m).
    \end{cases}
  \end{displaymath}

  Para entender o que motivou essa definição, observe que para $i = 0,
  \ldots L_n^m$, teremos que:
  \begin{align*}
    \lambda_n^m(\Gamma^{(n)}(S_i^m-)) &= \Gamma(S_i^m-)\\
    \lambda_n^m(\Gamma^{(n)}(S_i^m)) &= \Gamma(S_i^m),
  \end{align*}
  enquanto que nos pontos interiores, ``completamos'' $\lambda_n^m$ de
  maneira linear.

  Como $\Gamma(t) \geq \Gamma^{(n)}(t)$ para todo $n \in \Nz$, teremos
  que $\lambda(t) \geq t$. Usando a linearidade por partes, e o fato
  que $\Gamma(\sigma^x_i) = \Gamma(\sigma_i^x-) + \gamma_x T^x_i$,
  teremos que:
  \begin{displaymath}
    \sup_{0 \leq t \leq T} |\lambda_n^m(t) - t| =
    \max_{0 \leq i \leq L_n^m} \{ \Gamma(S_i^m) -
    \Gamma^{(n)}(S_i^m)\}.
  \end{displaymath}
  Essa quantidade converge quase certamente à zero se mantivermos o
  $m$ fixo e jogarmos o $n$ para infinito. Assim para cada $m$ existe
  um $n_m$ tal que para $n \geq n_m$ vale que:
  \begin{displaymath}
    \sup_{0 \leq t \leq T} |\lambda_n^m(t) - t| < \delta_m.
  \end{displaymath}
  Podemos tomar a sequência $(n_m)_{m \geq 1}$ de modo que ela seja
  crescente. Agora vamos ``invertê-la'', isso é, para cada n em $\Z
  \cap [n_{i-1}, n_i)$, defina $m_n = i$. Teremos que:
  \begin{displaymath}
    \sup_{0 \leq t \leq T} |\lambda_n^{m_n}(t) - t| < \delta_{m_n}.
  \end{displaymath}
  Ainda como, para cada $m$ fixado, $n_m$ é finito, vale que $m_n$ vai
  ao infinito quando $n \to \infty$.
  
  Agora note que $t \in [\Gamma^{(n)}(S_{i}^m-),
  \Gamma^{(n)}(S_{i}^m))$ se e somente se $\lambda_n^m(t) \in
  [\Gamma(S_{i}^m-), \Gamma(S_{i}^m))$. Assim para cada $x \in \{1,
  \ldots, m\}$ teremos que $X(\lambda_n^m(t)) = x$ se e somente se
  $X^{(n)}(t) = x$. De onde concluímos que:
  \begin{displaymath}
    \sup_{0 \leq t \leq T} d\left(X(\lambda_n^m(t)), X^{(n)}\right)
    \leq \delta_m
  \end{displaymath}

  Assim tomando $\tilde{\lambda}_n = \lambda_n^{m_n}$, concluímos que
  quase certamente:
  \begin{align*}
    \sup_{0 \leq t \leq T} |\tilde{\lambda}_n(t) - t|
    &\xrightarrow{n\to\infty} 0 \\
    \sup_{0 \leq t \leq T} d(X(\tilde{\lambda}_n(t)), X^{(n)}(t))
    &\xrightarrow{n\to\infty} 0 \\
  \end{align*}
\end{proof}

\begin{corolario}
  \label{cor:convergencia}
  Para todo $y \in \Nzb$ e todo $T > 0$, para quase toda trajetória do
  processo, existe uma sequência de funções $\lambda^y_n: [0, T) \to
  [0, \infty)$ crescentes e contínuas tal que:
  \begin{gather}
    \label{eq:convergencia-truncado-uniforme}
    \lambda^y_n(t) \geq t \\
    \sup_{t \in [0, T]} |\lambda^y_n(t) - t| \leq
    \sup_{t \in [0, T]} |\lambda^\infty_n(t) - t|
    \xrightarrow{n \to \infty} 0 \notag\\
    \sup_{0 \leq t \leq T} d(X^y(\lambda_n^y(t)), X^{y, (n)}(t)) \leq
    \sup_{0 \leq t \leq T} d(X^\infty(\lambda_n(t)), X^{\infty, (n)}(t))
    \xrightarrow{n\to\infty} 0. \notag
  \end{gather}
  
  Dessa forma, teremos que teremos que $X^{y, (n)}$ converge \qc para
  para $X^y$ na métrica de Skorohod, sendo essa convergência uniforme
  em $y$.
\end{corolario}
\begin{proof}
  Fixado um $T > 0$, tome $\lambda_n: [0, \infty) \to [0, \infty)$
  como na demonstração do Teorema \ref{teo:convergencia}. Elas são
  funções contínuas crescentes tais que quase certamente:
  \begin{gather*}
    \lambda_n(t) \geq t\\
    \sup_{0 \leq t \leq T} |\lambda_n(t) - t|
    \xrightarrow{n\to\infty} 0 \\
    \sup_{0 \leq t \leq T} d(X^\infty(\lambda_n(t)), X^{\infty, (n)}(t))
    \xrightarrow{n\to\infty} 0. \\
  \end{gather*}

  Tome $\lambda^\infty_n = \lambda_n$ e para $y \in \Nz$, defina:
  \begin{displaymath}
    \lambda_n^y(t) = \begin{cases}
      t & \textrm{ se } t < \gamma_y T_0\\
      \gamma_yT_0 + \lambda_n(t - \gamma_y T_0) & \textrm{ c.c.}
    \end{cases}
  \end{displaymath}


  Pela Proposição \ref{prop:reinicia-infinito}, depois de $\gamma_y
  T_0$, teremos que o processo iniciado em $y$ se comportará
  exatamente igual ao processo iniciado no $\infty$, e como antes
  disso teremos $\lambda^y_n(t) = t$ e $X^\infty(t) = X^{\infty,
    (n)}(t)$, então concluímos
  \eqref{eq:convergencia-truncado-uniforme}.

  % \begin{gather*}
  %   \sup_{0 \leq t \leq T} |\lambda_n^y(t) - t| \leq
  %   \sup_{0 \leq t \leq T} |\lambda_n(t) - t|
  %   \xrightarrow{n\to\infty} 0 \notag \\
  %   \sup_{0 \leq t \leq T} d(X^y(\lambda_n^y(t)), X^{y, (n)}(t)) \leq
  %   \sup_{0 \leq t \leq T} d(X^\infty(\lambda_n(t)), X^{\infty, (n)}(t))
  %   \xrightarrow{n\to\infty} 0
  % \end{gather*}
\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Propriedade de Feller}
\label{sec:prop-feller}


Uma observação interessante sobre esse processo que fizemos durante a
introdução é que ele pode não ser de Feller. Nosso objetivo nessa
seção é especificar quando que isso ocorre.

\begin{definicao}
  \label{def:semigrupo}
  Denotaremos por $\Psi$ o semigrupo de transição do Processo K, isso
  é, para $t > 0$ e $f: \Nzb \to \R$:
  \begin{gather*}
    \Psi_t f (x) = \E \left[ f(X^x(t)) \right]\\
  \end{gather*}
\end{definicao}

Dizemos que um processo é de Feller se para todo $t > 0$, e toda $f:
\Nzb \to \R$ contínua, então $\Psi_t f$ também é uma função contínua.

\begin{proposicao}
  São equivalentes:
  \begin{enumerate}
  \item $\lim_{x \to \infty} \gamma_x = 0$
  \item O processo K é um processo de Feller.
  \end{enumerate}
\end{proposicao}

\begin{proof}

  Primeiramente vamos mostrar que se $\gamma_x \to 0$ quando $x \to
  \infty$, então o processo é de Feller.

  Fixe um $t > 0$ e uma $f: \Nzb \to \R$ contínua. Queremos mostrar
  que $\Psi_t f$ é uma função contínua, isso é, que $\Psi_t f(y) \to
  \Psi f (\infty)$ quando $y \to \infty$.

  Note que:
  \begin{displaymath}
    \Psi_t f(\infty) - \Psi f (y) =
    \E \left[
      f(X^\infty(t)) - f(X^y(t))
    \right].
  \end{displaymath}

  Como a quantidade dentro da esperança é limitada por $2 \Vert f
  \Vert$, então se mostrarmos que ela vai quase certamente à zero,
  seguirá pelo teorema da convergência dominada que a esperança também
  vai à zero.

  Como $gamma_y \to 0$, então $\gamma_y T_0 \to 0$ \qc. Assim tomando
  $y$ suficientemente grande teremos que $\gamma_y T_0 < t$. Portanto
  usando a Proposição \ref{prop:reinicia-infinito}, teremos que:

  \begin{displaymath}
    f(X^\infty(t)) - f(X^y(t)) = 
    f(X^\infty(t)) - f(X^\infty(t-\gamma_y T_0)).
  \end{displaymath}

  O Corolário \ref{cor:continuidades-processo} nos garante que $t$ é
  quase certamente um ponto de contínuidade do processo, assim
  $X^\infty(t-\gamma_y T_0) \to X^\infty(t)$ \qc, e como $f$ é
  contínua, concluímos que a quantidade acima converge à zero \qc.

  Agora vamos mostrar que se $\gamma_x$ não converge para zero quando
  $x \to \infty$, então o processo K não é de Feller.

  Como $\gamma_x$ não vai à zero, existe um $\delta > 0$ e uma
  sequência $(x_n)$ com $x_n \to \infty$ e $\gamma_{x_n} >
  \delta$. Dessa forma temos que para todo $t > 0$:
  \begin{align*}
    P(\gamma_{x_n} T_0 > t) &=
    \exp\left\{ -\frac{t}{\gamma_{x_n}}\right\}
    \geq  \exp\left\{ -\frac{t}{\delta}\right\}.
  \end{align*}

  Agora tomemos um $\epsilon > 0 $ tal que $\epsilon < e^{-t/\delta}$,
  vamos dividir em dois casos:

\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Propriedade de Markov}
\label{sec:prop-markov}

O objetivo dessa seção é provar que o processo K é Markoviano. Nossa
estratégia será mostrar que a propriedade de Markov se mantém quando
tomamos os limites dos processos truncados introduzidos na Seção
\ref{sec:aproximacoes}.

Note que nem sempre o limite de processos Markovianos é um processo
Markoviano. Para demonstrar que a propriedade é mantida, vamos abusar
de diversas propriedades da nossa construção, em particular da
convergência dos processos truncados uniformemente na condição
inicial.

Uma dificuldade introduzida ao aceitar pesos não homogêneos é que o
Processo pode não ser de Feller. Isso acontece porque o processo
iniciado em um estado $y$ ``grande'' pode ser bem diferente do
processo iniciado no $\infty$. Esses são os estados $y$ onde
$\gamma_y$ é ``grande''.

Vamos contornar esse problema usando o fato que esses estados onde
$\gamma_y$ é grande não são de fato visitados, essa idéia será
formalizada pela Proposição \ref{prop:gamma-somavel}. Com essa
proposição mostraremos que nosso processo tem propriedades muito
parecidas com a de um processo de Feller.

\begin{proposicao}
  \label{prop:gamma-somavel}
  Seja $V_t := \cup_{s \in [0, t]} \{ X(s) \} \setminus \{\infty\}$ o
  conjunto de estados visitados pelo processo até o instante
  $t$. Então:
  \begin{displaymath}
    \sum_{x \in V_t} \gamma_x < \infty,
  \end{displaymath}
  quase certamente para todo $t > 0$.
\end{proposicao}
\begin{proof}

  Primeiramente fixe um $t > 0$ arbitrário.

  Denote por $V^\prime_t = \{ x \in \Nz: N^x(t) \geq 1 \}$. Quase
  certamente vale que:
  \begin{displaymath}
    \sum_{x \in V^\prime_t} \gamma_x T^x_1 \leq \Gamma(t) < \infty.
  \end{displaymath}

  Se denotarmos por $\nu$ a distribuição de $V^\prime_t$ sobre o conjunto
  das partes de $\Nz$, como $V^\prime_t$ só depende dos processos de
  Poisson $\{N^x: x \in \Nz\}$ e esses são independentes de $\{ T^x_1:
  x \in \Nz \}$, teremos que:
  \begin{align*}
    1 &= P\left(\sum_{x \in V^\prime_t} \gamma_x T^x_1 < \infty
    \right)\\
    &=\int P\left(\sum_{x \in V} \gamma_x T^x_1 < \infty
    \right) \nu(dV)
  \end{align*}

  Portanto, para $\nu$ quase todo $V \subseteq \Nz$, vale que
  $P(\sum_{x \in V} \gamma_x T^x_1 < \infty) = 1$. Fixado um
  $V$ desses, teremos que $\sum_{x \in V}\gamma_x < \infty$, já que
  uma soma de variáveis aleatórias exponenciais independentes é finita
  \qc se e somente se as a soma de suas médias converge.

  Com isso concluímos que para $\nu$ quase todo $V$ vale que $\sum_{x
    \in V}\gamma_x < \infty$. Em outras palavras, $\sum_{x \in
    V^\prime_t}\gamma_x < \infty$ \qc.

  Agora fixe uma sequência $(t_n)$, com $t_n\nearrow \infty$, teremos
  que com probabilidade $1$, $\sum_{x \in V^\prime_{t_n}}\gamma_x <
  \infty$ para todo $n$.

  Note que se $t < s$, então $V^\prime_t \subseteq V^\prime_s$. Assim
  para qualquer $t > 0$, existe um $n$ tal que $t_n > t$, e portando:
  \begin{displaymath}
    \sum_{x \in V^\prime_t}\gamma_x T^x_1 \leq
    \sum_{x \in V^\prime_{t_n}}\gamma_x T^x_1 < \infty
  \end{displaymath}

  Voltando a trabalhar com $V_t$, note que se $s < t$, então $V_s
  \subseteq V_t$, note ainda que por construção $V^\prime_t =
  V_{\Gamma(t)}$.

  Agora fixe tome uma realização do processo, onde $\Gamma(t) \to
  \infty$ quando $t \to \infty$ e $\sum_{x \in V^\prime_t} \gamma_x <
  \infty$ para todo $t$, pelo argumento anterior, tais realizações tem
  probabilidade $1$. Nelas, para um $t > 0$ arbitrário, vai existir
  um $s > 0$ tal que $\Gamma(s) > t$. Dessa forma:
  \begin{displaymath}
    \sum_{x \in V_t} \gamma_x \leq \sum_{x \in V_{\Gamma(s)}}
    \gamma_x =
    \sum_{x \in V^\prime_s} \gamma_x < \infty.
  \end{displaymath}

  De onde concluímos o que desejávamos.
\end{proof}


\begin{proposicao}
  \label{prop:semigrupo-quase-continuo}
  Para $f: \Nzb \to \R$ contínua e $t, s > 0$ fixados arbitrariamente,
  vale que \qc:
  \begin{equation}
    \label{eq:semigrupo-quase-continuo}
    \lim_{n \to \infty} \Psi_s f (X^{(n)}(t)) = \Psi_s f(X(t))
  \end{equation}
\end{proposicao}
\begin{proof}
  Usando o Corolário \ref{cor:continuidades-processo}, temos que $t$ é
  \qc um ponto de continuidade do processo original. Assim, como
  $X^{(n)}$ converge \qc para $X$ na métrica de Skorohod, ele também
  converge pontualmente nos pontos de continuidade de $X$. Portanto
  $X^{(n)}(t) \to X(t)$ \qc quando $n \to \infty$.

  Vamos fixar agora uma dessas realizações e denotar por $y_n =
  X^{(n)}(t)$ e $y = X(t)$. Vamos separa em dois casos.

  O primeiro é quando existe um $n_0$ tal que $y_n = y$ para todo $n >
  n_0$. Nesse caso \eqref{eq:semigrupo-quase-continuo} é evidente.

  O segundo caso é o contrário do primeiro, isso é, para todo $n_0$
  existe um $n > n_0$ tal que $y_n \neq y$. Como $y_n \to y$, então
  concluímos que $y$ não é um ponto isolado, como o único ponto que
  não é isolado em $\Nzb$ é o $\infty$, concluímos que $y = \infty$.
  Note ainda que a sequência $y_n$ assume um número infinito de
  valores diferentes.

  Vamos ignorar os índices $n$ onde $y_n = \infty$, visto que neles a
  igualdade em \eqref{eq:semigrupo-quase-continuo} é trivial. 

  Por construção, vemos que o processo truncado é mais ``acelerado''
  que o processo original, isso é, se o processo truncado passou por
  um estado $x$, então o processo original deve ter passado
  também. Portanto $y_n \in V_t$ para todo $n$.

  Dessa forma, usando a Proposição \ref{prop:gamma-somavel} e o fato
  de $y_n$ assumir um número infinito de valores distintos, concluímos
  que $\gamma_{y_n} \to 0$ quando $n \to \infty$.

  Se tomarmos um $n$ suficientemente grande teremos que $\gamma_{y_n}
  T_0 < s$. Dessa forma usando a Proposição
  \ref{prop:reinicia-infinito} teremos que:
  \begin{align*}
    \Psi_s f (\infty) - \Psi_s f (y_k) &=
    \E\left[ f(X^\infty(s)) - f(X^{y_k}(s)) \right] \\
    &= \E\left[ f(X^\infty(s)) -f (X^\infty(s - \gamma_{y_k}T_0)) \right] \\
  \end{align*}

  Agora $s$ é \qc um ponto de continuidade de $X^\infty$ (pela
  Corolário \ref{cor:continuidades-processo}), e como $\gamma_{y_n}
  T_0$ converge à zero, temos que a quantidade dentro da esperança
  acima converge quase certamente à zero. Como $f$ é contínua (e
  portanto limitada), o teorema da convergência dominada nos garante
  que a esperança também converge à zero.
\end{proof}


\begin{teorema}
  \label{teo:proc_markov}
  O processo K é um processo Markoviano.
\end{teorema}

\begin{proof}
  Fixado um $m \geq 1$, $t_1 < t_2 < \ldots < t_{m+1}$ e $f_1, \ldots,
  f_{m+1} : \Nzb \to \R$ funções contínuas.

  Como $X^{(n)}$ é Markoviano, se denotarmos por $\Psi^n$ o semigrupo
  de transição do processo truncado, teremos que:
  \begin{gather}
    \label{eq:markov-truncado}
    \E \left[
      f_{1}(X^{(n)}(t_{1})) 
      \ldots
      f_{m}(X^{(n)}(t_{m})) 
      f_{m+1}(X^{(n)}(t_{m+1})) 
    \right] \\
    = \E \left[
      f_{1}(X^{(n)}(t_{1})) 
      \ldots
      f_{m}(X^{(n)}(t_{m})) 
      \Psi^n_{t_{m+1} - t_{m}} f_{m+1} (X^{(n)}(t_{m})) 
    \right]\notag
  \end{gather}

  O Corolário \ref{cor:continuidades-processo} nos garante que \qc
  $t_1, \ldots, t_{m+1}$ são pontos de continuidade do processo, e
  como convergência na métrica de Skorohod garante convergência
  pontual nos pontos de continuidade, teremos que $f_i(X^{(n)}(t_i)) \to
  f_i(X(t_i))$ \qc já que $f_i$ é contínua, $i = 1, \ldots, m+1$.

  Temos ainda que $f_1, \ldots f_{m+1}$ são limitadas porque são
  funções contínuas num espaço compacto. Assim usando o teorema da
  convergência dominada, teremos que a parte da esquerda de
  \eqref{eq:markov-truncado} converge para:
  \begin{displaymath}
    \E \left[
      f_{1}(X(t_{1})) 
      \ldots
      f_{m}(X(t_{m})) 
      f_{m+1}(X(t_{m+1})) 
    \right]
  \end{displaymath}

  Agora vamos estimar a parte da direita por:
  \begin{equation}
    \label{eq:markov-est-eps}
    \E \left[
      f_{1}(X^{(n)}(t_{1})) 
      \ldots
      f_{m}(X^{(n)}(t_{m})) 
      \Psi_{t_{m+1} - t_{m}} f_{m+1} (X^{(n)}(t_{m})) 
    \right] + \epsilon_n
  \end{equation}

  Usando a Proposição \ref{prop:semigrupo-quase-continuo}, temos que a
  parte da esquerda de \eqref{eq:markov-est-eps} converge para:
  \begin{displaymath}
    \E \left[
      f_{1}(X(t_{1})) 
      \ldots
      f_{m}(X(t_{m})) 
      \Psi_{t_{m+1} - t_{m}} f_{m+1} (X(t_{m})) 
    \right].
  \end{displaymath}

  Resta mostrar que $\epsilon_n \to 0$. Para simplificar a notação,
  vamos denotar por $s = t_{m+1} - t_m$, $t = t_m$ e $g = f_{m+1}$.
  Usando o fato de $f_1, \ldots, f_m$ serem limitadas, teremos que:
  \begin{displaymath}
    |\epsilon_n| \leq  \textrm{cte}
    \left\lvert \E \left[
        \Psi_{s}^n g (X^{(n)}(t)) -
        \Psi_{s} g (X^{(n)}(t))
      \right]
    \right\rvert.
  \end{displaymath}

  Como a parte interna dessa esperança pode ser dominada por $2\lVert
  g \rVert$, se mostrarmos que ela converge quase certamente para
  zero, seguirá do teorema da convergência dominada que a esperança também
  converge à zero. Como $X^{(n)}(t) \in V_t$ para todo $n$, teremos
  que:
  \begin{align*}
    \left\lvert \Psi_{s}^n g (X^{(n)}(t)) - \Psi_{s} g (X^{(n)}(t))
    \right\rvert
    &\leq \sup_{y \in V_t}  \left\lvert \Psi_{s}^n g (y) - \Psi_{s} g (y)
    \right\rvert
  \end{align*}

  A Proposição \ref{prop:gamma-somavel} nos garante que para quase
  todo $V_t$, a condição da Proposição
  \ref{prop:convergencia-semigrupo} é satisfeita. Portanto a
  quantidade acima vai à zero \qc.

  Juntando tudo concluímos que:
  \begin{displaymath}
    \E \left[
      f_{1}(X(t_{1})) 
      \ldots
      f_{m}(X(t_{m})) 
      f_{m+1}(X(t_{m+1})) 
    \right] \notag\\
    = \E \left[
      f_{1}(X(t_{1})) 
      \ldots
      f_{m}(X(t_{m})) 
      \Psi_{t_{m+1} - t_{m}} f_{m+1} (X(t_{m})) 
    \right].
  \end{displaymath}
\end{proof}


\begin{proposicao}
  \label{prop:convergencia-semigrupo}
  Seja $K \subseteq \Nz$ tal que
  \begin{displaymath}
    \lim_{n \to \infty} \sup\{ \gamma_x: x \in  K \textrm{ e } x > n\} = 0,
  \end{displaymath}
  sob a convenção de que $\sup \emptyset = 0$. Tome ainda $f: \Nzb
  \to \R$ contínua e $t > 0$. Nessas condições:
  \begin{equation}
    \label{eq:convergencia-semigrupo}
    \sup_{y \in K} | \Psi^n_t f (y) - \Psi_t f(y) |
    \xrightarrow{n\to\infty} 0,
  \end{equation}
  onde $\Psi^n$ denota o semigrupo de transição de $X^{(n)}$.
\end{proposicao}

\begin{proof}

  Primeiro cada ``termo'' de \eqref{eq:convergencia-semigrupo}
  converge para zero, ou seja, mostremos que $\Psi^n_t f (y) \to
  \Psi_t f(y)$ quando $n \to \infty$ para qualquer $y \in K$ fixado.

  \begin{align}
    \label{eq:semigrupo-convergencia-pontual}
    \Psi^n_t f (y) -\Psi_t f(y) &=
    \E \left[
      f(X^{y, (n)}(t)) - f(X^y(t))
    \right]
  \end{align}

  O Corolário \ref{cor:continuidades-processo} nos garante que $t$ é
  \qc um ponto de continuidade de $X^y$, assim $X^{y, (n)}(t) \to
  X^y(t)$ \qc quando $n \to \infty$, e como $f$ é contínua, concluímos
  que $f(X^{y, (n)}(t)) \to f(X^y(t))$ \qc.

  Mas $f$ também é limitada, daí pelo Teorema da Convergência Monótona
  concluímos que \eqref{eq:semigrupo-convergencia-pontual} converge à
  zero quando $n \to \infty$.

  Com isso o caso em que $K$ é finito é trivial. Portanto vamos supor
  que $K$ é infinito.

  Tomando $\lambda_n^y$ como na demonstração do Corolário
  \ref{cor:convergencia} para um $T > t$ qualquer teremos que:
  \begin{align}
    \label{eq:esperancas-markov}
    \sup_{y \in K} | \Psi^n_t f (y) - \Psi_t f(y) | 
    &= \sup_{y \in K} \left\lvert \E \left[f(X^{y, (n)}(t)) -
        f(X^y(t)) \right]\right\rvert \\
    &\leq \sup_{y\in K}\left\lvert \E \left[f(X^{y, (n)}(t)) -
        f(X^y(\lambda_n^y(t))) \right]
    \right\rvert \notag \\
    &+ \sup_{y\in K}\left\lvert \E \left[f(X^y(\lambda_n^y(t))) -
        f(X^y(t)) \right] \right\rvert \notag
  \end{align}

  Vamos tratar os dois termos dessa soma separadamente.

  Por \eqref{eq:convergencia-truncado-uniforme}, temos que para todo
  $y$, quase certamente:
  \begin{equation}
    \label{eq:dist-uniforme}
    d(X^{y, (n)}(t), X^y(\lambda_n^y(t)) \leq
    \sup_{s \in [0, T]} d(X^{\infty, (n)}(s), X^\infty(\lambda_n^\infty(s))
    \xrightarrow{n\to\infty} 0.
  \end{equation}
  
  $f$ é uma função contínua em um espaço compacto, assim ela é
  uniformemente contínua. Isso é, para todo $\epsilon > 0$, existe um
  $\delta_\epsilon > 0$ tal que se $d(x, y) < \delta_\epsilon$ então
  $|f(x)-f(y)| < \epsilon$.

  Dessa forma, fixado um $\epsilon > 0$ arbitrário, teremos que:
  \begin{gather*}
    \sup_{y\in K} \left\lvert \E \left[ f(X^{y,(n)}(t)) -
        f(X^{y}(\lambda_n^y(t))) \right]
    \right\rvert\\
    \leq \sup_{y\in K} \left\lvert \E \left[ \left(f(X^{y,(n)}(t)) -
          f(X^{y}(\lambda_n^y(t))) \right) \ind\{ d(X^{y, (n)}(t),
        X^y(\lambda_n^y(t)) < \delta_\epsilon \} \right]
    \right\rvert \\
    + 2\Vert f \Vert \sup_{y \in K} P\left( d(X^{y, (n)}(t),
      X^y(\lambda_n^y(t)) \geq
      \delta_\epsilon \right) \\
    \leq \epsilon + 2 \Vert f \Vert P \left( \sup_{s \in [0,
        T]}d(X^{\infty, (n)}(s), X^\infty(\lambda_n^\infty(s)) \geq
      \delta_\epsilon \right).
  \end{gather*}
  Essa última quantidade vai para $\epsilon$ quando $n\to \infty$
  devido à \eqref{eq:dist-uniforme}. Como $\epsilon$ foi escolhido
  arbitrariamente, concluímos que o primeiro termo de
  \eqref{eq:esperancas-markov} converge à zero.

  Para calcular o segundo termo, veja que aquela quantidade, para cada
  $y \in K$ fixado, vai à zero, já que $t$ é \qc um ponto de
  continuidade do processo e $\lambda_n^y(t) \to t$ quando
  $n\to\infty$. Dessa forma existe uma sequência $(k_n)$ tal que $k_n
  \xrightarrow{n\to\infty} \infty$ onde:
  \begin{displaymath}
      \max_{\stackrel{y \leq k_n}{y\in K}} \left\lvert
      \E \left[f(X^y(\lambda_n^y(t))) - f(X^y(t)) \right]
    \right\rvert \xrightarrow{n\to \infty} 0.
  \end{displaymath}

  Agora vamos tomar $\delta_n = \sqrt{\sup\{ \gamma_x: x \in K
    \textrm{ e } x > k_n\}}$. Por hipótese, temos que $\delta_n \to
  0$. Ainda note que $\sup \{ P(\gamma_y T_0 > \delta_n) : y \in K, y
  > k_n \} = e^{-\frac{1}{\delta_n}} \to 0$ quanto $n \to \infty$.

  Como $\delta_n \to 0$, vamos apenas considerar $n$ suficientemente
  grande, de forma que $\delta_n < t \leq \lambda_n^y(t)$. A segunda
  igualdade vale de acordo com
  \eqref{eq:convergencia-truncado-uniforme}. Usando a Proposição
  \ref{prop:reinicia-infinito} e separando nos casos $\gamma_yT_0 <
  \delta_n$ e $\gamma_yT_0 \geq \delta_n$, teremos que:
  \begin{align}
    \label{eq:markov-quase-la}
    \sup_{\stackrel{y > k_n}{y \in K}}& \left\lvert \E \left[f(X^y(\lambda_n^y(t))) -
        f(X^y(t)) \right]
    \right\rvert \notag \\
    &\leq \sup_{\stackrel{y > k_n}{y \in K}} \E \left\lvert
      \left[f(X^\infty(\lambda_n^y(t)-\gamma_yT_0)) -
        f(X^\infty(t-\gamma_yT_0)) \right] \ind\{ \gamma_yT_0 <
      \delta_n \}
    \right\rvert\\
    &+ 2 \Vert f \Vert \sup_{\stackrel{y > k_n}{y \in K}} P(\gamma_y T_0 > \delta_n).
    \notag
  \end{align}

  Nós escolhemos $\delta_n$ de forma que o segundo termo dessa soma vá
  à zero quando $n \to \infty$. Agora repare que podemos dominar o
  primeiro termo por:
  \begin{displaymath}
     \E \left[ \sup_{\stackrel{y > k_n}{y\in K}}
      \sup_{0 \leq s \leq \delta_n} \left\lvert
        f(X^\infty(\lambda_n^y(t)-s)) -
        f(X^\infty(t-s))
    \right\rvert \right].
  \end{displaymath}

  Note que a variável sobre a qual estamos tomando esperança é
  dominada por $2\Vert f \Vert$. Portando se mostrarmos que ela
  converge \qc para zero, valerá que sua esperança também vai a zero.

  Fixado um $\epsilon > 0$, tome $\epsilon^\prime > 0$ tal que $d(x, y) <
  \epsilon^\prime$ implique que $|f(x) - f(y)| < \epsilon$, isso
  existe por causa da continuidade uniforme de $f$.
 
  Como $t$ é \qc um ponto de continuidade de $X^\infty$, existe um
  $\epsilon^{\prime\prime}$ tal que se $|s| <
  \epsilon^{\prime\prime}$, então $d(X^\infty_t, X^\infty_{t+s}) <
  \frac{\epsilon^{\prime}}{2}$.

  Assim se tomarmos $n_0$ tal que para todo $n > n_0$, $\delta_{n} <
  \frac{\epsilon^{\prime\prime}}{2}$ e $\sup_{0 \leq s \leq T}
  |\lambda_n^\infty(s) - s| < \frac{\epsilon^{\prime\prime}}{2}$, teremos
  que, para todo $y\in\Nzb$ e $s \leq \delta_n$:
  \begin{gather*}
    |\lambda_n^y(t) - s - t| \leq 
     |\lambda_n^y(t) - t | + |s| \leq
     \sup_{r \in [0, T]} |\lambda_n^\infty(r) - r | + |s|
     < \epsilon^{\prime\prime}\\
    |t - s - t |  = |s| < \epsilon^{\prime\prime}
  \end{gather*}
  
  Portanto temos que:
  \begin{displaymath}
    d(X^\infty(\lambda_n^y(t)-s), X^\infty(t-s)) \leq
     d(X^\infty(\lambda_n^y(t)-s), X^\infty(t))+
     d(X^\infty(t), X^\infty(t-s))
     < \epsilon^{\prime}
  \end{displaymath}

  De onde finalmente concluímos que para todo $n > n_0$:
  \begin{displaymath} 
    \sup_{\stackrel{y > k_n}{y \in K}}
    \sup_{0 \leq s \leq \delta_n} \left\lvert
      f(X^\infty(\lambda_n^y(t)-s)) -
      f(X^\infty(t-s))
    \right\rvert < \epsilon
  \end{displaymath}

  Como o $\epsilon$ foi tomado arbitrariamente, concluímos que essa
  quantidade vai a zero quase certamente.
\end{proof}

%% ------------------------------------------------------------------------- %%

\section{Propriedade Forte de Markov}
\label{sec:prop-forte-markov}

Um resultado clássico diz que um processo de Feller markoviano é
fortemente markoviano. Como já vimos, nosso processo não é de Feller,
mas ele goza de algumas propriedades que irão possibilitar demonstrar
a propriedade forte de Markov de forma análoga à maneira em que  a
demonstramos para processos de Feller.

Para cada $t > 0$, vamos denotar por $\FF_t$ a $\sigma$-algebra gerada
por $\{X(s): s \in [0, t]\}$ e por $\FF$ a $\sigma$-algebra do espaço
de probabilidades onde estamos trabalhando.

\begin{proposicao}
  \label{prop:markov-forte}
  O processo K é fortemente Markoviano.
\end{proposicao}
\begin{proof}
  Fixada uma $f: \Nzb \to \R$ contínua e $\tau$ um tempo de parada
  para $(\FF_t)_{t \geq 0}$ quase certamente finito, vamos mostrar que
  quase certamente:
  \begin{equation}
    \label{eq:markov-forte}
    \E \left[ f(X(\tau + t) \middle| \FF_\tau \right]
    = \Psi_t f (X(\tau)),
  \end{equation}
  onde $\FF_\tau$ é definida da seguinte forma:
  \begin{displaymath}
    \FF_\tau = \left\{
      A \in \FF:  A \cap \{\tau \leq t\} \in \FF_t \quad \forall t \geq 0 
    \right\}
  \end{displaymath}


  Para cada $h > 0$, denote por: 
  \begin{displaymath}
    \tau_h := \inf\left\{
      t \geq \tau: t = k h, k \in \N
    \right\}.
  \end{displaymath}

  Note que $\tau_h \leq t$ se e somente se $\tau \leq \lfloor t
  \rfloor_h$, onde $\lfloor t \rfloor_h := \max\{k h \leq t: k \in
  \N\}$ Assim $\{\tau_h \leq k h\} \in \FF_{\lfloor t \rfloor_h}
  \subseteq \FF_t$ e portanto $\tau_h$ é um tempo de parada para a
  filtração $(\FF_{t})_{t \geq 0}$. Note ainda que $\tau \leq \tau_h
  < \tau + h$. Assim $\tau_h$ é \qc finito e $\tau_h \to \tau$ \qc
  quando $h \to 0$.

  Como em tempo discreto a propriedade de markov é equivalente à
  propriedade forte de markov, então teremos que para todo $h > 0$ e
  $k \in \N$ vale que:
 \begin{displaymath}
    \E \left[ f(X(\tau_h + k h)) \middle| \FF_{\tau_h} \right]
    = \Psi_{k h} f (X(\tau_h)).
  \end{displaymath}
  
  Fixe um $t > 0$ e tome $h = t/k$, reescrevendo a expressão acima
  teremos que:
  \begin{displaymath}
    \label{eq:markov-forte-esp-cond}
    \E \left[ f(X(\tau_h + t)) \middle| \FF_{\tau_h} \right]
    = \Psi_t f (X(\tau_h)).
  \end{displaymath}


  Vamos mostrar agora que para todo $h > 0$, $\FF_{\tau} \subseteq
  \FF_{\tau_h}$. Para isso tome um $A \in \FF_\tau$ e um $t \geq 0$
  arbitrários. Como $\{\tau \leq t\} \subseteq \{\tau_h \leq t\}$,
  então:
  \begin{displaymath}
    A \cap \{ \tau_h \leq t \} =
    A \cap \{ \tau \leq t \} \cap \{ \tau_h \leq t \},
  \end{displaymath}
  agora $A \cap \{ \tau \leq t \} \in \FF_t$ porque $A \in \FF_\tau$,
  enquanto que $ \{ \tau_h \leq t \} \in \FF_t$ porque $\tau_h$ é
  tempo de parada. Assim concluímos que $A \cap \{ \tau_h \leq t \}
  \in \FF_t$ e como isso vale para todo $t$, então $A \in
  \FF_{\tau_h}$.

  Usando a definição de esperança condicional em
  \eqref{eq:markov-forte-esp-cond}, teremos que para todo evento $B
  \in \FF_{\tau} \subseteq \FF_{\tau_h}$:
  \begin{equation}
    \label{eq:markov-forte-discreto}
    \E \left[ f(X(\tau_h + t)) \ind\{B\} \right]
    = \E \left[ \Psi_t f (X(\tau_h)) \ind\{B\} \right].
  \end{equation}

  Agora iremos fazer $h \searrow 0$ através de ``divisores'' de $t$.

  Como $f$ é uma função contínua, $X$ é um processo contínuo à direita
  e $\tau_h \searrow \tau$ quando $h \to 0$, teremos que $f(X(\tau_h + t))
  \to f(X(\tau + t))$ \qc, e como $f$ é limitada, então a parte da
  esquerda de \eqref{eq:markov-forte-discreto} converge para:
  \begin{displaymath}
    \E \left[ f(X(\tau + t)) \ind\{B\} \right]
  \end{displaymath}

  Suponha que mostremos que $\Psi_t f (X(\tau_h)) \to \Psi_t
  f(X(\tau))$ \qc quando $h \searrow 0$. Com isso, já que $\Psi_t f$ é
  limitada, teremos que a parte da direita de
  \eqref{eq:markov-forte-discreto} convergirá para:
  \begin{displaymath}
    \E \left[ \Psi_t f (X(\tau)) \ind\{B\} \right].
  \end{displaymath}

  Dessa forma, para todo $B \in \FF_{\tau}$, valerá que:
  \begin{displaymath}
    \E \left[ f(X(\tau + t)) \ind\{B\} \right]
    = \E \left[ \Psi_t f (X(\tau)) \ind\{B\} \right].
  \end{displaymath}

  Como $\Psi_t f(X(\tau))$ é mensurável em $\FF_\tau$, concluímos que:
  \begin{displaymath}
    \E \left[ f(X(\tau + t)) \middle| \FF_\tau \right]
    = \Psi_t f (X(\tau)),
  \end{displaymath}
  que é exatamente \eqref{eq:markov-forte}.

  Dessa forma só resta mostrar que:
  \begin{equation}
    \label{eq:convergencia-discretizacao-1}
    \Psi_t f (X(\tau_h)) \xrightarrow[\qc]{h \searrow 0}
    \Psi_t f(X(\tau)).
  \end{equation}
  
  É exatamente nesse ponto que nossa demonstração vai divergir da
  demonstração clássica. Se nosso processo fosse de Feller, então
  $\Psi_t f$ seria uma função contínua, e essa convergência seria
  evidente. Para contornar esse problema, vamos usar a Proposição
  \ref{prop:gamma-somavel}, de maneira parecida à que fizemos quando
  queríamos mostrar a propriedade de Markov simples.

  Vamos fixar uma realização do processo onde $\sum_{x \in V_t}
  \gamma_x < \infty$ para todo $t$, $\tau < \infty$ e $X(\tau_{h}) \to
  X(\tau)$ quando $h \to 0$. Tais realizações tem probabilidade 1.


  Tome uma sequência $h_n \searrow 0$ arbitrária. Vamos mostrar que:
  \begin{equation}
    \label{eq:convergencia-discretizacao-2}
    \Psi_t f (X(\tau_{h_n})) \xrightarrow[\qc]{n \to \infty}
    \Psi_t f(X(\tau)).
  \end{equation}

  Se isso valer para toda sequência $h_n$, então concluíremos
  \ref{eq:convergencia-discretizacao-1}. Sem perda de generalidade
  podemos olhar apenas para as sequências onde $h_n \leq 1$ para todo
  $n$.

  Se existir um $n_0$ tal que $n > n_0$ implica que $X(\tau_{h_n}) =
  X(\tau)$, então \eqref{eq:convergencia-discretizacao-2} é evidente.

  Assim vamos supor o contrário, ou seja, que para todo $n_0$ existe
  um $n > n_0$ tal que $X(\tau_{h_n}) \neq X(\tau)$. Dessa forma
  concluímos que $X(\tau_{h_n})$ não é um ponto isolado e portanto
  $X(\tau) = \infty$.

  Vamos denotar por $y_n = X(\tau_{h_n})$. Temos que $y_n \to
  \infty$. Vamos desconsiderar os índices $n$ onde $y_n = \infty$, já
  que neles a igualdade em \eqref{eq:convergencia-discretizacao-2} é
  óbvia.

  Como $h_n \leq 1$ e $\tau_h < \tau + h$, vale que $y_n \in V_{\tau +
    1}$ para todo $n$. Já que $\sum_{x \in V_{\tau + 1}} \gamma_x <
  \infty$ e passamos por infinitos valores em $y_n$, vale que
  $\gamma_{y_n} \to 0$.
  \begin{align*}
    \Psi_t f(\infty) - \Psi_t f (y_n)
    = \E \left[
      f(X^{\infty}(t)) - f(X^{y_n}(t))
    \right]
  \end{align*}

  Agora vamos mostrar que a quantidade dentro da esperança acima
  converge à zero quase certamente. Como essa quantidade é limitada
  por $2 \Vert f \Vert$, o teorema da convergência dominada irá
  garantir que a esperança também irá à zero.

  Se tomarmos um $n$ grande o suficiente, de forma que
  $\gamma_{y_n} T_0 < t$, usando a Proposição \ref{prop:reinicia-infinito},
  teremos que:
  \begin{align*}
    f(X^{\infty}(t)) - f(X^{y_n}(t)) &=
    f(X^{\infty}(t)) - f(X^{\infty}(t-\gamma_{y_n} T_0)).
  \end{align*}

  Essa quantidade acima vai à zero porque $\gamma_{y_n} T_0 \to 0$
  quando $n \to \infty$, $t$ é quase certamente um ponto de
  continuidade do processo (Corolário
  \ref{cor:continuidades-processo}) e $f$ é uma função contínua.
\end{proof}



%%% Local Variables: 
%%% TeX-master: "tese"
%%% End: 
